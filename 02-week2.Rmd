# Week 2

## Tokenization and word frequencies

When approaching large-scale quantiative analyses of text, a key task is how we identify and capture the unit of analysis. One of the most commonly used approaches, across diverse analytical contexts, is text tokenization. Here, we are splitting the text into word units: unigrams, bigrams, trigrams etc. 

The chapters by @manning_introduction_2007, listed below, provide a technical introduction to the task of "querying" text according to different word-based queries. This is a task we will be studying in the hands-on assignment for this week. 

For the seminar discussion, we will be focusing on some widely-cited examples of research in the applied social sciences employing token-based, or word frequency, analyses of large corpora. The first, by @michel_quantitative_2011 uses the enormous Google books corpus to measure cultural and linguistic trends. The second, by @bollen_historical_2021 uses the same corpus to demonstrate a more specific change over time---so-called "cognitive distortion." In both examples, we should be attentive to questions of sampling covered in previous weeks. This question is central to the back-and-forths in the short responses and replies to the articles by @michel_quantitative_2011 and @bollen_historical_2021.   

Questions:

1. Tokenizing and counting: what does this capture?
2. Corpus-based sampling: what biases might threaten inference?
3. If you had to write a critique of either @michel_quantitative_2011 or @bollen_historical_2021, what would it focus on?

**Required reading**:

- @michel_quantitative_2011
  - @schwartz_culturomics_2011
  - @morse-gagne_culturomics_2011
  - @aiden_culturomicsresponse_2011
  
- @bollen_historical_2021
  - @schmidt_uncontrolled_2021
  - @bollen_reply_2021
  
- @manning_introduction_2007 (ch. 2): [https://nlp.stanford.edu/IR-book/information-retrieval-book.html](https://nlp.stanford.edu/IR-book/information-retrieval-book.html)]
- @krippendorff_content_2004 (ch. 5)

**Further reading**:

- @rozado_prevalence_2021
- @alshaabi_storywrangler_2021
- @campos_survey_2015
- @greenfield_changing_2013

**Slides**:

- Week 2 [Slides](https://docs.google.com/presentation/d/1EB8l2R3aDnfabpx23qKq-dH-6HehfgDqC9n1Pc90jN8/edit?usp=sharing)

### Tokenizing

In this section, we'll have a quick overview of how we're processing text data when conducting analyses of word frequency. We'll be using some randomly simulated text. 

First we load the packages that we'll be using:

```{r, warning = F, message = F}
library(stringi) #to generate random text
library(dplyr) #tidyverse package for wrangling data
library(tidytext) #package for 'tidy' manipulation of text data
library(ggplot2) #package for visualizing data
library(scales) #additional package for formatting plot axes
library(kableExtra) #package for displaying data in html format (relevant for formatting this worksheet mainly)
```

We'll first get some random text to see what it looks like when we're tokenizing text.

```{r}
lipsum_text <- data.frame(text = stri_rand_lipsum(1, start_lipsum = TRUE))

head(lipsum_text$text)
```

We can then tokenize with the `unnest_tokens()` function in `tidytext`. 

````{r}
tokens <- lipsum_text %>%
  unnest_tokens(word, text)

head(tokens)
```

Now we'll get some larger data, simulating 5000 observations (rows) of random Latin text strings. 

```{r}
## Varying total words example
lipsum_text <- data.frame(text = stri_rand_lipsum(5000, start_lipsum = TRUE))
```

We'll then add another column and call this "weeks." This will be our unit of analysis. 

```{r}
# make some weeks one to ten
lipsum_text$week <- as.integer(rep(seq.int(1:10), 5000/10))
```

Now we'll simulate a trend where we see an increasing number of words as weeks go by. Don't worry too much about this as the code is a little more complex, but I share it here in case of interest.

```{r}
for(i in 1:nrow(lipsum_text)) {
  week <- lipsum_text[i, 2]
  morewords <-
    paste(rep("more lipsum words", times = sample(1:100, 1) * week), collapse = " ")
  lipsum_words <- lipsum_text[i, 1]
  new_lipsum_text <- paste0(morewords, lipsum_words, collapse = " ")
  lipsum_text[i, 1] <- new_lipsum_text
}
```

And we can see that as each week goes by, we have more and more text.

```{r}
lipsum_text %>%
  unnest_tokens(word, text) %>%
  group_by(week) %>%
  count(word) %>%
  select(week, n) %>%
  distinct() %>%
  ggplot() +
  geom_bar(aes(week, n), stat = "identity") +
  labs(x = "Week", y = "n words") +
  scale_x_continuous(breaks= pretty_breaks())
```

We can then do the same but with a trend where each week sees a decreasing number of words.

```{r}
# simulate decreasing words trend
lipsum_text <- data.frame(text = stri_rand_lipsum(5000, start_lipsum = TRUE))

# make some weeks one to ten
lipsum_text$week <- as.integer(rep(seq.int(1:10), 5000/10))

for(i in 1:nrow(lipsum_text)) {
  week <- lipsum_text[i,2]
  morewords <- paste(rep("more lipsum words", times = sample(1:100, 1)* 1/week), collapse = " ")
  lipsum_words <- lipsum_text[i,1]
  new_lipsum_text <- paste0(morewords, lipsum_words, collapse = " ")
  lipsum_text[i,1] <- new_lipsum_text
}

lipsum_text %>%
  unnest_tokens(word, text) %>%
  group_by(week) %>%
  count(word) %>%
  select(week, n) %>%
  distinct() %>%
  ggplot() +
  geom_bar(aes(week, n), stat = "identity") +
  labs(x = "Week", y = "n words") +
  scale_x_continuous(breaks= pretty_breaks())
```

Now let's check out the top frequency words in this text.

```{r}
lipsum_text %>%
  unnest_tokens(word, text) %>%
  count(word, sort = T) %>%
  top_n(5) %>%
  knitr::kable(format="html")%>% 
  kable_styling("striped", full_width = F)
```

We're going to check out the frequencies for the word "sed" and then we're gonna normalize these by denominating by total word frequencies for each week.

First we need to get total word frequencies for each week.

```{r}
lipsum_totals <- lipsum_text %>%
  group_by(week) %>%
  unnest_tokens(word, text) %>%
  count(word) %>%
  mutate(total = sum(n)) %>%
  distinct(week, total)
```

```{r}
# let's look for "sed"
lipsum_sed <- lipsum_text %>%
  group_by(week) %>%
  unnest_tokens(word, text) %>%
  filter(word == "sed")  %>%
  count(word) %>%
  mutate(total_sed = sum(n)) %>%
  distinct(week, total_sed)

```

Then we can join these two dataframes together with the `left_join()` function where we're joining by the "week" column. We can then pipe the joined data into a plot.

```{r}
lipsum_sed %>%
  left_join(lipsum_totals, by = "week") %>%
  mutate(sed_prop = total_sed/total) %>%
  ggplot() +
  geom_line(aes(week, sed_prop)) +
  labs(x = "Week", y = "
       Proportion sed word") +
  scale_x_continuous(breaks= pretty_breaks())
```

### Regexing

You'll notice that in the worksheet on word frequencies that at one point there are a set of parentheses after `str_detect()` we have the string "[a-z]". This is called a __character class__ and these use square brackets like `[]`.

Other character classes include, as helpfully listed in this [vignette](https://cran.r-project.org/web/packages/stringr/vignettes/regular-expressions.html) for the <tt>stringr</tt> package. What follows is adapted from these materials on regular expressions. 

* `[abc]`: matches a, b, or c.
* `[a-z]`: matches every character between a and z 
   (in Unicode code point order).
* `[^abc]`: matches anything except a, b, or c.
* `[\^\-]`: matches `^` or `-`.

Several other patterns match multiple characters. These include:

*   `\d`: matches any digit; the opposite of this is `\D`, which matches any character that 
    is not a decimal digit.

```{r}
str_extract_all("1 + 2 = 3", "\\d+")
str_extract_all("1 + 2 = 3", "\\D+")
```
    
*   `\s`: matches any whitespace; its opposite is `\S`
    
```{r}
(text <- "Some  \t badly\n\t\tspaced \f text")
str_replace_all(text, "\\s+", " ")
```

*   `^`: matches start of the string
    
```{r}
x <- c("apple", "banana", "pear")
str_extract(x, "^a")
```
*   `$`: matches end of the string
    
```{r}
x <- c("apple", "banana", "pear")
str_extract(x, "^a$")
```

*   `^` then `$`: exact string match
    
```{r}
x <- c("apple", "banana", "pear")
str_extract(x, "^apple$")
```

Hold up: what do the plus signs etc. mean?

* `+`: 1 or more.
* `*`: 0 or more.
* `?`: 0 or 1.

So if you can tell me why this output makes sense, you're getting there!

```{r}
str_extract_all("1 + 2 = 3", "\\d+")[[1]]
str_extract_all("1 + 2 = 3", "\\D+")[[1]]

str_extract_all("1 + 2 = 3", "\\d*")[[1]]
str_extract_all("1 + 2 = 3", "\\D*")[[1]]

str_extract_all("1 + 2 = 3", "\\d?")[[1]]
str_extract_all("1 + 2 = 3", "\\D?")[[1]]
```

### Some more regex resources:

1. Regex crossword: [https://regexcrossword.com/](https://regexcrossword.com/).
2. Regexone: [https://regexone.com/](https://regexone.com/)
3. R4DS [chapter 14](https://r4ds.had.co.nz/strings.html#matching-patterns-with-regular-expressions)
