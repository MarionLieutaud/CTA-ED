[["index.html", "Computational Text Analysis Course overview Assessment Fortnightly worksheets Fortnightly flash talks", " Computational Text Analysis Christopher Barrie 2021-12-17 Course overview In recent years, the use of computational techniques for the quantitative analysis of text has exploded. The volume and quantity of text data to which we now have access in the digital age is enormous. This has led social scientists to seek out new means of analyzing text data at scale. We will see that text records, be they in the form of digital traces left on social media platforms, archived works of literature, parliamentary speeches, video transcripts, or print news, can help us answer a huge range of important questions in social science. This course will give students training in the use of computational text analysis techniques. The course will prepare students for dissertation work that uses textual data and will provide hands-on training in the use of the R programming language and (some) Python. The course will provide a venue for seminar discussion of examples using these methods in the empirical social sciences as well as lectures on the technical and/or statistical dimensions of their application. We will be using this online book for the ten-week course in “Computational Text Analysis” (PGSP11584). Each chapter contains the readings for that week. The book also includes worksheets with example code for how to conduct some of the text analysis techniques we discuss each week. Each week, we will alternately be discussing the substantive and technical dimensions of published research in the empirical social sciences. The readings for each week contain two “substantive” readings–that is, examples of the application of text analysis techniques with empirical data–and one “technical” reading that focuses mainly on the statistical and computational aspects of a given technique. We will study first the technical aspects of analytical approaches and, second, the substantive dimensions of these applications. This means that, when discussing the readings, we will be able to discuss how satisfactory a given approach is for illuminating the question or topic at hand. Lectures will primarily be focused on these technical dimensions. The seminar (Q&amp;A) that follows will give us the opportunity to study and discuss questions of social scientific interest, and how computational text analysis had been used to answer these. NOTE: Before the end of Week 1, students should complete two introductory R exercises. First, you should consult the worksheet here, which is an introduction to setting up and understanding the very basics of working in R. Second, Ugur Ozdemir has provided such a more comprehensive introductory R course for the Research Training Centre at the University of Edinburgh and you can follow the instructions here to access this. Assessment Assessment takes the form of one summative assessment. This will be a 4000 word essay on a set of subjects or a subject of your choosing (with prior approval by me). For this, you will be required to select from a range of data sources I will provide. You may also suggest your own data source. You will be asked to formulate a research question and use at least one computational text analysis technique that we have studied over the course of the ten weeks. You will then provide the code they used in reproducible (markdown) format and will be assessed on both the substantive content of your essay contribution (the social science part) as well as their demonstrated competency in coding and text analysis (the computational part). Fortnightly worksheets Each fortnight, I will provide you with one worksheet that walks you through how to implement a different text analysis technique. At the end of these worksheets you will find a set of questions. You should buddy up with someone else in your class and go through these together. This is called “pair programming” and there’s a reason we do this. Firstly, coding can be an isolating and difficult thing—it’s good to bring a friend along for the ride. Secondly, if there’s something you don’t know, maybe your buddy will. This saves you both time! Thirdly, your buddy can check your code as you write it, and vice versa. Again, this means both of you are working together to produce something and check it as you go along. At the next week’s lecture, I will pick on a pair at random to answer each one of that worksheet’s questions (i.e., there is ~1/3 chance you’re going to get picked each week). I will ask you to walk us through your code. And remember: it’s also fine if you struggled and didn’t get to the end! If you encountered an obstacle, we can work through that together. All that matters to me is that you try. Fortnightly flash talks On the weeks where you are not going to be tasked with a coding assignment, you’re not off the hook! I will again be selecting a pair at random (the same as your coding pair) to talk me through one of the readings. I will pick different pair for each reading (i.e., ~ 1/3 chance…). Don’t stress too much about this: I just want two or three minutes where you lay out for me: 1) the research question; 2) the data source; 3) the method; 4) the findings; 5) what you thought its limitations were. For this last one, you will want to think about 1)-3); i.e., you will want to think about whether it really answered the research question, whether the data was appropriate for answering that question, and whether the method was appropriate for answering that question. And even if all three of these are reasonably solid, all articles have limitations so I want to hear about them! Final assessment The final assessment will take the form of one major summative assessment. This will be a 4000 word essay on a subject of the studentsÂ¿ choosing (with prior approval by the instructor) using a dataset of their own choosing and applying at least one computational text analysis technique. The students will provide the code they used in reproducible format and will be assessed on both the substantive content of their essay contribution as well as their demonstrated competency in coding and text analysis. "],["introduction-to-r.html", "Introduction to R 0.1 Getting started with R at home 0.2 Some basic information 0.3 Getting Started in RStudio 0.4 A simple example 0.5 Loading packages 0.6 Saving your objects, plots and scripts 0.7 Knowing where R saves your documents 0.8 Practicing in R 0.9 One final note 0.10 References", " Introduction to R This section is designed to ensure you are familiar with the R environment. 0.1 Getting started with R at home Given that we’re all working from home these days, you’ll need to download R and RStudio onto your own devices. R is the name of the programming language that we’ll be using for coding exercises; RStudio is the IDE (“Integrated Development Environment”), i.e., the piece of software that almost everyone uses when working in R. You can download both of these on Windows and Mac easily and for free. This is one of the first reasons to use an “open-source” programming language: it’s free! And this means everyone has access to it. Install R for Mac from here: https://cran.r-project.org/bin/macosx/. Install R for Windows from here: https://cran.r-project.org/bin/windows/base/. Download RStudio for Windows or Mac from here: https://rstudio.com/products/rstudio/download/, choosing the Free version: this is what most people use and is more than enough for all of our needs. All programs are free. Make sure to load everything listed above for your operating system or R will not work properly! 0.2 Some basic information A script is a text file in which you write your commands (code) and comments. If you put the # character in front of a line of text this line will not be executed; this is useful to add comments to your script! R is case sensitive, so be careful when typing. To send code from the script to the console, highlight the relevant line of code in your script and click on Run, or select the line and hit ctrl+enter on PCR or cmd+enter on Mac Access help files for R functions by preceding the name of the function with ? (e.g., ?table) By pressing the up key, you can go back to the commands you have used before Press the tab key to auto-complete variable names and commands 0.3 Getting Started in RStudio Begin by opening RStudio (located on the desktop). Your first task is to create a new script (this is where we will write our commands). To do so, click: File --&gt; NewFile --&gt; RScript Your screen should now have four panes: the Script (top left) the Console (bottom left) the Environment/History (top right) Files/Plots/Packages/Help/Viewer (bottom right) 0.4 A simple example The Script (top left) is where we write our commands for R. You can try this out for a first time by writing a small snipped of code as follows: x &lt;- &quot;I can&#39;t wait to learn Quantitative Research Methods&quot; #Note the quotation marks! To tell R to run the command, highlight the relevant row in your script and click the Run button (top right of the Script) - or hold down ctrl+enter on Windows or cmd+enter on Mac - to send the command to the Console (bottom left), where the actual evaluation and calculations are taking place. These shortcut keys will become very familiar to you very quickly! Running the command above creates an object named ‘x,’ that contains the words of your message. You can now see ‘x’ in the Environment (top right). To view what is contained in x, type in the Console (bottom left): print(x) ## [1] &quot;I can&#39;t wait to learn Quantitative Research Methods&quot; # or alternatively you can just type: x ## [1] &quot;I can&#39;t wait to learn Quantitative Research Methods&quot; 0.5 Loading packages The ‘base’ version of R is very powerful but it will not be able to do everything on its own, at least not with ease. For more technical or specialized forms of analysis, we will need to load new packages. This is when we will need to install a so-called ‘package’—a program that includes new tools (i.e., functions) to carry out specific tasks. You can think of them as ‘extensions’ enhancing R’s capacities. To take one example, we might want to do something a little more exciting than print how excited we are about this course. Let’s make a map instead. This might sound technical. But the beauty of the packaged extensions of R is that they contain functions to perform specialized types of analysis with ease. We’ll first need to install one of these packages, which you can do as below: install.packages(&quot;tidyverse&quot;) You can load the most recent version of packages using RStudio (by clicking Install under the packages tab of the bottom right pane and typing the name of your desired package) and ensuring the top drop down menu says install from Repository (CRAN). Keep Install dependencies ticked. After the package is installed, we then need to load it into our environment by typing library(). Note that, here, you don’t need to wrap the name of the package in quotation marks. So this will do the trick: library(tidyverse) What now? Well, let’s see just how easy it is to visualize some data using ggplot which is a package that comes bundled into the larger tidyverse package. ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y = hwy)) If we wanted to save where we’d got to with making our plots, we would want to save our scripts, and maybe the data we used as well, so that we could return to it at a later stage. 0.6 Saving your objects, plots and scripts Saving scripts: To save your script in RStudio (i.e. the top left panel), all you need to do is click File –&gt; Save As (and choose a name for your script). Your script will be something like: myfilename.R. Saving plots: If you have made any plots you would like to save, click Export (in the plotting pane) and choose a relevant file extension (e.g. .png, .pdf, etc.) and size. To save individual objects (for example x from above) from your environment, run the following command (choosing a suitable filename): save(x,file=&quot;myobject.RData&quot;) load(file=&quot;myobject.RData&quot;) To save all of your objects (i.e. everything in the top right panel) at once, run the following command (choosing a suitable filename): save.image(file=&quot;myfilname.RData&quot;) Your objects can be re-loaded into R during your next session by running: load(file=&quot;myfilename.RData&quot;) 0.7 Knowing where R saves your documents If you are at home, when you open a new script make sure to check and set your working directory (i.e. the folder where the files you create will be saved). To check your working directory use the getwd() command (type it into the Console or write it in your script in the Source Editor): getwd() To set your working directory, run the following command, substituting the file directory of your choice. Remember that anything following the `#’ symbol is simply a clarifying comment and R will not process it. ## Example for Mac setwd(&quot;/Users/Documents/mydir/&quot;) ## Example for PC setwd(&quot;c:/docs/mydir&quot;) 0.8 Practicing in R The best way to learn R is to use it. These workshops on text analysis will not be the place to become fully proficient in R. They will, however, be a chance to conduct some hands-on analysis with applied examples in a fast-expanding field. And the best way to learn is through doing. So give it a shot! For some further practice in the R programming language, look no further than Wickham and Grolemund (2017) and, for tidy text analysis, Silge and Robinson (2017). The free online book by Hadley Wickham “R for Data Science” is available here The free online book by Julia Silge and David Robinson “Text Mining with R” is available here For more practice with R, you may want to consult a set of interactive tutorials, available through the package “learnr.” Once you’ve installed this package, you can go through the tutorials yourselves by calling: library(learnr) available_tutorials() # this will tell you the names of the tutorials available run_tutorial(name = &quot;ex-data-basics&quot;, package = &quot;learnr&quot;) #this will launch the interactive tutorial in a new Internet browser window 0.9 One final note Once you’ve dipped into the “R for Data Science” book you’ll hear a lot about the so-called tidyverse in R. This is essentially a set of packages that use an alternative, and more intuitive, way of interacting with data. The main difference you’ll notice here is that, instead of having separate lines for each function we want to run, or wrapping functions inside functions, sets of functions are “piped” into each other using “pipe” functions, which look have the appearance: %&gt;%. I will be using “tidy” syntax in the weekly exercises for these computational text analysis workshops. If anything is unclear, I can provide the equivalents in “base” R too. but a lot of the useful text analysis packages are composed with tidy syntax. 0.10 References References "],["week-1.html", "Chapter 1 Week 1 1.1 Information retrieval and transformation", " Chapter 1 Week 1 1.1 Information retrieval and transformation Required reading: Grimmer and Stewart (2013) Gentzkow, Kelly, and Taddy (2019) Manning, Raghavan, and Schtze (2007, chs.1 and 10): https://nlp.stanford.edu/IR-book/information-retrieval-book.html] Further reading: Wilkerson and Casas (2017) DiMaggio (2015) References "],["week-2.html", "Chapter 2 Week 2 2.1 Word frequency and dictionary techniques", " Chapter 2 Week 2 2.1 Word frequency and dictionary techniques Required reading: Michel et al. (2011) Martins and Baumard (2020) Manning, Raghavan, and Schtze (2007, chs.2, 3, and 4): https://nlp.stanford.edu/IR-book/information-retrieval-book.html] Further reading: Rozado, Al-Gharbi, and Halberstadt (2021) References "],["week-3.html", "Chapter 3 Week 3 3.1 Information retrieval and transformation", " Chapter 3 Week 3 3.1 Information retrieval and transformation Required reading: Michel et al. (2011) Rozado, Al-Gharbi, and Halberstadt (2021) Manning, Raghavan, and Schtze (2007, ch.2: https://nlp.stanford.edu/IR-book/information-retrieval-book.html) Further reading: References "],["week-4.html", "Chapter 4 Week 4 4.1 Information retrieval and transformation", " Chapter 4 Week 4 4.1 Information retrieval and transformation Required reading: Michel et al. (2011) Rozado, Al-Gharbi, and Halberstadt (2021) Manning, Raghavan, and Schtze (2007, ch.2: https://nlp.stanford.edu/IR-book/information-retrieval-book.html) Further reading: References "],["week-5.html", "Chapter 5 Week 5 5.1 Information retrieval and transformation", " Chapter 5 Week 5 5.1 Information retrieval and transformation Required reading: Michel et al. (2011) Rozado, Al-Gharbi, and Halberstadt (2021) Manning, Raghavan, and Schtze (2007, ch.2: https://nlp.stanford.edu/IR-book/information-retrieval-book.html) Further reading: References "],["week-6.html", "Chapter 6 Week 6 6.1 Information retrieval and transformation", " Chapter 6 Week 6 6.1 Information retrieval and transformation Required reading: Michel et al. (2011) Rozado, Al-Gharbi, and Halberstadt (2021) Manning, Raghavan, and Schtze (2007, ch.2: https://nlp.stanford.edu/IR-book/information-retrieval-book.html) Further reading: References "],["week-7.html", "Chapter 7 Week 7 7.1 Information retrieval and transformation", " Chapter 7 Week 7 7.1 Information retrieval and transformation Required reading: Michel et al. (2011) Rozado, Al-Gharbi, and Halberstadt (2021) Manning, Raghavan, and Schtze (2007, ch.2: https://nlp.stanford.edu/IR-book/information-retrieval-book.html) Further reading: References "],["week-8.html", "Chapter 8 Week 8 8.1 Information retrieval and transformation", " Chapter 8 Week 8 8.1 Information retrieval and transformation Required reading: Michel et al. (2011) Rozado, Al-Gharbi, and Halberstadt (2021) Manning, Raghavan, and Schtze (2007, ch.2: https://nlp.stanford.edu/IR-book/information-retrieval-book.html) Further reading: References "],["week-9.html", "Chapter 9 Week 9 9.1 Information retrieval and transformation", " Chapter 9 Week 9 9.1 Information retrieval and transformation Required reading: Michel et al. (2011) Rozado, Al-Gharbi, and Halberstadt (2021) Manning, Raghavan, and Schtze (2007, ch.2: https://nlp.stanford.edu/IR-book/information-retrieval-book.html) Further reading: References "],["week-10.html", "Chapter 10 Week 10 10.1 Information retrieval and transformation", " Chapter 10 Week 10 10.1 Information retrieval and transformation Required reading: Michel et al. (2011) Rozado, Al-Gharbi, and Halberstadt (2021) Manning, Raghavan, and Schtze (2007, ch.2: https://nlp.stanford.edu/IR-book/information-retrieval-book.html) Further reading: References "],["exercise-1-word-frequency-analysis.html", "Chapter 11 Exercise 1: Word frequency analysis 11.1 Introduction 11.2 Setup 11.3 Load data and packages 11.4 Inspect and filter data 11.5 Tidy the text 11.6 Regex sidebar 11.7 Back to the Fringe 11.8 Analyze keywords 11.9 Compute aggregate statistics 11.10 Plot time trends 11.11 Bonus: gender prediction 11.12 Exercises 11.13 References", " Chapter 11 Exercise 1: Word frequency analysis 11.1 Introduction In this tutorial, you will learn how to summarise, aggregate, and analyze text in R: How to tokenize and filter text How to clean and preprocess text How to visualize results with ggplot How to perform automated gender assignment from name data (and think about possible biases these methods may enclose) 11.2 Setup To practice these skills, we will use a dataset that I have already collected from the Edinburgh Fringe Festival website. You can try this out yourself too: to obtain these data, you must first obtain an API key. Instructions on how to do this are available at the Edinburgh Fringe API page: Alt Text 11.3 Load data and packages Before proceeding, we’ll load the remaining packages we will need for this tutorial. library(tidyverse) # loads dplyr, ggplot2, and others library(tidytext) # includes set of functions useful for manipulating text library(ggthemes) # includes a set of themes to make your visualizations look nice! library(readr) # more informative and easy way to import data library(babynames) #for gender predictions For this tutorial, we will be using data that I have pre-cleaned and provided in .csv format. The data come from the Edinburgh Book Festival API, and provide data for every event that has taken place at the Edinburgh Book Festival, which runs every year in the month of August, for nine years: 2012-2020. There are many questions we might ask of these data. In this tutorial, we will investigate the contents of each event, and the speakers at each event, to determine if there are any trends in gender representation over time. The first task, then, is to read in these data. We can do this with the read_csv() function. The read_csv() function takes the .csv file and loads it into the working environment as a data frame object called “edbfdata.” We can call this object anything though. Try changing the name of the object before the &lt;- arrow. Note that R does not allow names with spaces in, however. It is also not a good idea to name the object something beginning with numbers, as this means you have to call the object within ` marks. edbfdata &lt;- read_csv(&quot;data/wordfreq/edbookfestall.csv&quot;) ## Warning: Missing column names filled in: &#39;X1&#39; [1] ## ## ── Column specification ─────────────────────────────────────────────────────────────────────── ## cols( ## X1 = col_double(), ## festival_id = col_character(), ## title = col_character(), ## sub_title = col_character(), ## artist = col_character(), ## year = col_double(), ## description = col_character(), ## genre = col_character(), ## latitude = col_double(), ## longitude = col_double(), ## age_category = col_character(), ## ID = col_character() ## ) If you’re working on this document from your own computer (“locally”) you can download the Edinburgh Fringe data in the following way: edbfdata &lt;- read_csv(&quot;https://raw.githubusercontent.com/cjbarrie/RDL-Ed/main/02-text-as-data/data/edbookfestall.csv&quot;) 11.4 Inspect and filter data Our next job is to cut down this dataset to size, including only those columns that we need. But first we can inspect it to see what the existing column names are, and how each variable is coded. To do this we can first call: colnames(edbfdata) ## [1] &quot;X1&quot; &quot;festival_id&quot; &quot;title&quot; &quot;sub_title&quot; &quot;artist&quot; &quot;year&quot; ## [7] &quot;description&quot; &quot;genre&quot; &quot;latitude&quot; &quot;longitude&quot; &quot;age_category&quot; &quot;ID&quot; And then: glimpse(edbfdata) ## Rows: 5,938 ## Columns: 12 ## $ X1 &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 2… ## $ festival_id &lt;chr&gt; &quot;book&quot;, &quot;book&quot;, &quot;book&quot;, &quot;book&quot;, &quot;book&quot;, &quot;book&quot;, &quot;book&quot;, &quot;book&quot;, &quot;book&quot;, … ## $ title &lt;chr&gt; &quot;Denise Mina&quot;, &quot;Alex T Smith&quot;, &quot;Challenging Expectations with Peter Cock… ## $ sub_title &lt;chr&gt; &quot;HARD MEN AND CARDBOARD GANGSTERS&quot;, NA, NA, &quot;WHAT CAUSED THE 2011 REVOLU… ## $ artist &lt;chr&gt; &quot;Denise Mina&quot;, &quot;Alex T Smith&quot;, &quot;Peter Cocks&quot;, &quot;Paul Mason&quot;, &quot;Chris d&#39;Lac… ## $ year &lt;dbl&gt; 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, … ## $ description &lt;chr&gt; &quot;&lt;p&gt;\\n\\tAs the grande dame of Scottish crime fiction, Denise Mina places… ## $ genre &lt;chr&gt; &quot;Literature&quot;, &quot;Children&quot;, &quot;Children&quot;, &quot;Literature&quot;, &quot;Children&quot;, &quot;Childre… ## $ latitude &lt;dbl&gt; 55.9519, 55.9519, 55.9519, 55.9519, 55.9519, 55.9519, 55.9519, 55.9519, … ## $ longitude &lt;dbl&gt; -3.206913, -3.206913, -3.206913, -3.206913, -3.206913, -3.206913, -3.206… ## $ age_category &lt;chr&gt; NA, &quot;AGE 4 - 7&quot;, &quot;AGE 11 - 14&quot;, NA, &quot;AGE 10 - 14&quot;, &quot;AGE 6 - 10&quot;, &quot;AGE 6 … ## $ ID &lt;chr&gt; &quot;Denise Mina2012&quot;, &quot;Alex T Smith2012&quot;, &quot;Peter Cocks2012&quot;, &quot;Paul Mason201… We can see that the description of each event is included in a column named “description” and the year of that event as “year.” So for now we’ll just keep these two. Remember: we’re interested in this tutorial firstly in the representation of gender and feminism in forms of cultural production given a platform at the Edinburgh International Book Festival. Given this, we are first and foremost interested in the reported content of each artist’s event. We use pipe %&gt;% functions in the tidyverse package to quickly and efficiently select the columns we want from the edbfdata data.frame object. We pass this data to a new data.frame object, which we call “evdes.” # get simplified dataset with only event contents and year evdes &lt;- edbfdata %&gt;% select(description, year) head(evdes) ## # A tibble: 6 x 2 ## description year ## &lt;chr&gt; &lt;dbl&gt; ## 1 &quot;&lt;p&gt;\\n\\tAs the grande dame of Scottish crime fiction, Denise Mina places a deep unders… 2012 ## 2 &quot;&lt;p&gt;\\n\\tWhen Alex T Smith was a little boy he wanted to be a chef, a rabbit or a child… 2012 ## 3 &quot;&lt;p&gt;\\n\\tPeter Cocks is known for his fantasy series Triskellion written with Mark Bill… 2012 ## 4 &quot;&lt;p&gt;\\n\\tTwo books by influential journalists are among the first to look at the factor… 2012 ## 5 &quot;&lt;p&gt;\\n\\tChris d&amp;rsquo;Lacey tells you all about The Fire Ascending, the next exciting … 2012 ## 6 &quot;&lt;p&gt;\\n\\tIt&amp;rsquo;s time for the honourable, feisty and courageous young hero, Sir Char… 2012 And let’s take a quick look at how many events there were over time at the festival. To do this, we first calculate the number of individual events (row observations) by year (column variable). evtsperyr &lt;- evdes %&gt;% mutate(obs=1) %&gt;% group_by(year) %&gt;% summarise(sum_events = sum(obs)) And then we can plot this using ggplot! ggplot(evtsperyr) + geom_line(aes(year, sum_events)) + theme_tufte(base_family = &quot;Helvetica&quot;) + scale_y_continuous(expand = c(0, 0), limits = c(0, NA)) Perhaps unsurprisingly, in the context of the pandemic, the number of recorded bookings for the 2020 Festival is drastically reduced. 11.5 Tidy the text Given that these data were obtained from an API that outputs data originally in HTML format, some of the text still contains some HTML and PHP encodings for e.g. bold font or paragraphs. We’ll need to get rid of this, as well as other punctuation before analyzing these data. The below set of commands takes the event descriptions, extracts individual words, and counts the number of times they appear in each of the years covered by our book festival data. #get year and word for every word and date pair in the dataset tidy_des &lt;- evdes %&gt;% mutate(desc = tolower(description)) %&gt;% unnest_tokens(word, desc) %&gt;% filter(str_detect(word, &quot;[a-z]&quot;)) 11.6 Regex sidebar You’ll notice that in the parentheses after str_detect() we have the string “[a-z].” This is called a character class and these use square brackets like []. Other character classes include, as helpfully listed in this vignette for the stringr package. What follows is adapted from these materials. [abc]: matches a, b, or c. [a-z]: matches every character between a and z (in Unicode code point order). [^abc]: matches anything except a, b, or c. [\\^\\-]: matches ^ or -. Several other patterns match multiple characters. These include: \\d: matches any digit; the opposite of this is \\D, which matches any character that is not a decimal digit. str_extract_all(&quot;1 + 2 = 3&quot;, &quot;\\\\d+&quot;) ## [[1]] ## [1] &quot;1&quot; &quot;2&quot; &quot;3&quot; str_extract_all(&quot;1 + 2 = 3&quot;, &quot;\\\\D+&quot;) ## [[1]] ## [1] &quot; + &quot; &quot; = &quot; \\s: matches any whitespace; its opposite is \\S (text &lt;- &quot;Some \\t badly\\n\\t\\tspaced \\f text&quot;) ## [1] &quot;Some \\t badly\\n\\t\\tspaced \\f text&quot; str_replace_all(text, &quot;\\\\s+&quot;, &quot; &quot;) ## [1] &quot;Some badly spaced text&quot; ^: matches start of the string x &lt;- c(&quot;apple&quot;, &quot;banana&quot;, &quot;pear&quot;) str_extract(x, &quot;^a&quot;) ## [1] &quot;a&quot; NA NA $: matches end of the string x &lt;- c(&quot;apple&quot;, &quot;banana&quot;, &quot;pear&quot;) str_extract(x, &quot;^a$&quot;) ## [1] NA NA NA ^ then $: exact string match x &lt;- c(&quot;apple&quot;, &quot;banana&quot;, &quot;pear&quot;) str_extract(x, &quot;^apple$&quot;) ## [1] &quot;apple&quot; NA NA Hold up: what do the plus signs etc. mean? +: 1 or more. *: 0 or more. ?: 0 or 1. So if you can tell me why this output makes sense, you’re getting there! str_extract_all(&quot;1 + 2 = 3&quot;, &quot;\\\\d+&quot;)[[1]] ## [1] &quot;1&quot; &quot;2&quot; &quot;3&quot; str_extract_all(&quot;1 + 2 = 3&quot;, &quot;\\\\D+&quot;)[[1]] ## [1] &quot; + &quot; &quot; = &quot; str_extract_all(&quot;1 + 2 = 3&quot;, &quot;\\\\d*&quot;)[[1]] ## [1] &quot;1&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;2&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;3&quot; &quot;&quot; str_extract_all(&quot;1 + 2 = 3&quot;, &quot;\\\\D*&quot;)[[1]] ## [1] &quot;&quot; &quot; + &quot; &quot;&quot; &quot; = &quot; &quot;&quot; &quot;&quot; str_extract_all(&quot;1 + 2 = 3&quot;, &quot;\\\\d?&quot;)[[1]] ## [1] &quot;1&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;2&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;3&quot; &quot;&quot; str_extract_all(&quot;1 + 2 = 3&quot;, &quot;\\\\D?&quot;)[[1]] ## [1] &quot;&quot; &quot; &quot; &quot;+&quot; &quot; &quot; &quot;&quot; &quot; &quot; &quot;=&quot; &quot; &quot; &quot;&quot; &quot;&quot; 11.6.1 Some more regex resources: Regex crossword: https://regexcrossword.com/. Regexone: https://regexone.com/ R4DS chapter 14 11.7 Back to the Fringe We see that the resulting dataset is large (~293k rows). This is because the above commands have first taken the pamphlet text, and has “mutated” it into a set of lower case character string. With the “unnest_tokens” function it has then taken each individual string and create a new column called “word” that contains each individual word contained in the pamphlet texts. Some terminology is also appropriate here. When we tidy our text into this format, we often refer to these data structures as consisting of “documents” and “terms.” This is because by “tokenizing” our text with the “unnest_tokens” functions we are generating a dataset with one term per row. Here, our “documents” are the collection of descriptions for all events in each year at the Edinburgh Book Festival. The way in which we sort our text into “documents” depends on the choice of the individual researcher. Instead of by year, we might have wanted to sort our text into “genre.” Here, we have two genres: “Literature” and “Children.” Had we done so, we would then have only two “documents,” which contained all of the words included in the event descriptions for each genre. Alternatively, we might be interested in the contributions of individual authors over time. Were this the case, we could have sorted our text into documents by author. In this case, each “document” would represent all the words included in event descriptions for events by the given author (many of whom do have multiple appearances over time or in the same festival for a given year). We can yet tidy this further, though. First we’ll remove all stop words and then we’ll remove all apostrophes: tidy_des &lt;- tidy_des %&gt;% filter(!word %in% stop_words$word) We see that the number of rows in our dataset reduces by about half to ~223k rows. This is natural since a large proportion of any string will contain many so-called “stop words.” We can see what these stop words are by typing: stop_words ## # A tibble: 1,149 x 2 ## word lexicon ## &lt;chr&gt; &lt;chr&gt; ## 1 a SMART ## 2 a&#39;s SMART ## 3 able SMART ## 4 about SMART ## 5 above SMART ## 6 according SMART ## 7 accordingly SMART ## 8 across SMART ## 9 actually SMART ## 10 after SMART ## # … with 1,139 more rows This is a lexicon (list of words) included in the tidytext package produced by Julia Silge and David Robinson (see here). We see it contains over 1000 such words. We remove them here because they are not very informative if we are interested in the substantive content of text (rather than, say, its grammatical content). Now let’s have a look at the most common words in these data: tidy_des %&gt;% count(word, sort = TRUE) ## # A tibble: 24,995 x 2 ## word n ## &lt;chr&gt; &lt;int&gt; ## 1 rsquo 5638 ## 2 book 2088 ## 3 event 1356 ## 4 author 1332 ## 5 world 1240 ## 6 story 1159 ## 7 join 1095 ## 8 em 1064 ## 9 life 879 ## 10 strong 864 ## # … with 24,985 more rows We can see that one of the most common words is “rsquo,” which is an HTML encoding for an apostrophe. Clearly we need to clean the data a bit more. This is a common issue in large-n text analysis and is a key step if you want to conduct reliably robust forms of text analysis. We’ll have another go using the the filter command, specifying that we only keep the words that are not included in the string of words rsquo, em, ndash, nbsp, lsquo. remove_reg &lt;- c(&quot;&amp;amp;&quot;,&quot;&amp;lt;&quot;,&quot;&amp;gt;&quot;,&quot;&lt;p&gt;&quot;, &quot;&lt;/p&gt;&quot;,&quot;&amp;rsquo&quot;, &quot;&amp;lsquo;&quot;, &quot;&amp;#39;&quot;, &quot;&lt;strong&gt;&quot;, &quot;&lt;/strong&gt;&quot;, &quot;rsquo&quot;, &quot;em&quot;, &quot;ndash&quot;, &quot;nbsp&quot;, &quot;lsquo&quot;, &quot;strong&quot;) reg_match &lt;- paste0(remove_reg, collapse = &quot;|&quot;) tidy_des &lt;- tidy_des %&gt;% filter(!word %in% remove_reg) tidy_des %&gt;% count(word, sort = TRUE) ## # A tibble: 24,989 x 2 ## word n ## &lt;chr&gt; &lt;int&gt; ## 1 book 2088 ## 2 event 1356 ## 3 author 1332 ## 4 world 1240 ## 5 story 1159 ## 6 join 1095 ## 7 life 879 ## 8 stories 860 ## 9 chaired 815 ## 10 books 767 ## # … with 24,979 more rows That’s more like it! The words that feature most seem to make sense now (and are actual words rather than random HTML and UTF-8 encodings). Let’s now collect these words into a data.frame object, which we’ll call edbf_term_counts: edbf_term_counts &lt;- tidy_des %&gt;% group_by(year) %&gt;% count(word, sort = TRUE) head(edbf_term_counts) ## # A tibble: 6 x 3 ## # Groups: year [6] ## year word n ## &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; ## 1 2016 book 295 ## 2 2018 book 283 ## 3 2019 book 265 ## 4 2012 book 254 ## 5 2013 book 241 ## 6 2015 book 239 For each year, we see that “book” is the most common word… perhaps no surprises here. But this is some evidence that we’re properly pre-processing and cleaning the data. Cleaning text data is an important element of preparing for any text analysis. It is often a process of trial and error as not all text data looks alike, may come from e.g. webpages with HTML encoding, unrecognized fonts or unicode, and all of these have the potential to cause issues! But finding these errors is also a chance to get to know your data… 11.8 Analyze keywords Okay, now we have our list of words, and the number of times they appear, we can tag those words we think might be related to issues of gender inequality and sexism. You may decide that this list is imprecise or inexhaustive. If so, then feel free to change the terms we are including after the grepl() function. edbf_term_counts$womword &lt;- as.integer(grepl(&quot;women|feminist|feminism|gender|harassment|sexism|sexist&quot;, x = edbf_term_counts$word)) head(edbf_term_counts) ## # A tibble: 6 x 4 ## # Groups: year [6] ## year word n womword ## &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 2016 book 295 0 ## 2 2018 book 283 0 ## 3 2019 book 265 0 ## 4 2012 book 254 0 ## 5 2013 book 241 0 ## 6 2015 book 239 0 11.9 Compute aggregate statistics Now that we have tagged individual words relating to gender inequality and feminism, we can sum up the number of times these words appear each year and then denominate them by the total number of words in the event descriptions. The intuition here is that any increase or decrease in the percentage of words relating to these issues is capturing a substantive change in the representation of issues related to sex and gender. What do we think of this measure? Is this an adequate measure of representation for such issues in the cultural sphere? Are the keywords we used precise enough? If not, what would you change? #get counts by year and word edbf_counts &lt;- edbf_term_counts %&gt;% complete(year, word, fill = list(n = 0)) %&gt;% group_by(year) %&gt;% mutate(year_total = sum(n)) %&gt;% filter(womword==1) %&gt;% summarise(sum_wom = sum(n), year_total= min(year_total)) head(edbf_counts) ## # A tibble: 6 x 3 ## year sum_wom year_total ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2012 22 23146 ## 2 2013 40 23277 ## 3 2014 30 25366 ## 4 2015 24 22158 ## 5 2016 34 24356 ## 6 2017 55 27602 11.10 Plot time trends So what do we see? Let’s take the count of words relating to gender in this dataset, and denominate them by the total number of words in these data per year. ggplot(edbf_counts, aes(year, sum_wom / year_total, group=1)) + geom_line() + xlab(&quot;Year&quot;) + ylab(&quot;% gender-related words&quot;) + scale_y_continuous(labels = scales::percent_format(), expand = c(0, 0), limits = c(0, NA)) + theme_tufte(base_family = &quot;Helvetica&quot;) We can add visual guides to draw attention to apparent changes in these data. Here, we might wish to signal the year of the #MeToo movement in 2017. ggplot(edbf_counts, aes(year, sum_wom / year_total, group=1)) + geom_line() + geom_vline(xintercept = 2017, col=&quot;red&quot;) + xlab(&quot;Year&quot;) + ylab(&quot;% gender-related words&quot;) + scale_y_continuous(labels = scales::percent_format(), expand = c(0, 0), limits = c(0, NA)) + theme_tufte(base_family = &quot;Helvetica&quot;) And we could label why we are highlighting the year of 2017 by including a text label along the vertical line. ggplot(edbf_counts, aes(year, sum_wom / year_total, group=1)) + geom_line() + geom_vline(xintercept = 2017, col=&quot;red&quot;) + geom_text(aes(x=2017.1, label=&quot;#metoo year&quot;, y=.0015), colour=&quot;black&quot;, angle=90, text=element_text(size=8)) + xlab(&quot;Year&quot;) + ylab(&quot;% gender-related words&quot;) + scale_y_continuous(labels = scales::percent_format(), expand = c(0, 0), limits = c(0, NA)) + theme_tufte(base_family = &quot;Helvetica&quot;) 11.11 Bonus: gender prediction We might decide that this measure is inadequate or too expansive to answer the question at hand. Another way of measuring representation in cultural production is to measure the gender of the authors who spoke at these events. Of course, this would take quite some time if we were to individually code each of the approximately 6000 events included in this dataset. But there do exist alternative techniques for imputing gender based on the name of an individual. We first create a new data.frame object, selecting just the columns for artist name and year. Then we generate a new column containing just the artist’s (author’s) first name: # get columns for artist name and year, omitting NAs gendes &lt;- edbfdata %&gt;% select(artist, year) %&gt;% na.omit() # generate new column with just the artist&#39;s (author&#39;s) first name gendes$name &lt;- sub(&quot; .*&quot;, &quot;&quot;, gendes$artist) A set of packages called gender and genderdata used to make the process of predicting gender based on a given individual’s name pretty straightforward. This technique worked with reference to U.S. Social Security Administration baby name data. Given that the most common gender associated with a given name changes over time, the function also allows us to specify the range of years for the cohort in question whose gender we are inferring. Given that we don’t know how wide the cohort of artists is that we have here, we could specify a broad range of 1920-2000. genpred &lt;- gender(gendes$name, years = c(1920, 2000)) Unfortunately, this package no longer works with newer versions of R; fortunately, I have recreated it using the original “babynames” data, which comes bundled in the babynames package. You don’t necessarily have to follow each step of how I have done this—I include this information for the sake of completeness. The babynames package. contains, for each year, the number of children born with a given name, as well as their sex. With this information, we can then calculate the total number of individuals with a given name born for each sex in a given year. Given we also have the total number of babies born in total cross these records, we can denominate (divide) the sums for each name by the total number of births for each sex in each year. We can take this proportion as representing the probability that a given individual in our Edinburgh Fringe dataset is male or female. More information on the babynames package can be found here. We first load the babynames package into the R environment as a data.frame object. Because the data.frame “babynames” is contained in the babynames package we can just call it as an object and store it with . This dataset contains names for all years over a period 1800–2019. The variable “n” represents the number of babies born with the given name and sex in that year, and the “prop” represents, according to the package materials accessible here, “n divided by total number of applicants in that year, which means proportions are of people of that gender with that name born in that year.” babynames &lt;- babynames head(babynames) ## # A tibble: 6 x 5 ## year sex name n prop ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1880 F Mary 7065 0.0724 ## 2 1880 F Anna 2604 0.0267 ## 3 1880 F Emma 2003 0.0205 ## 4 1880 F Elizabeth 1939 0.0199 ## 5 1880 F Minnie 1746 0.0179 ## 6 1880 F Margaret 1578 0.0162 We then calculate the total number of babies of female and male sex born in each year. Then we merge these to get a combined dataset of male and female baby names by year. We then merge this information back into the original babynames data.frame object. totals_female &lt;- babynames %&gt;% filter(sex==&quot;F&quot;) %&gt;% group_by(year) %&gt;% summarise(total_female = sum(n)) totals_male &lt;- babynames %&gt;% filter(sex==&quot;M&quot;) %&gt;% group_by(year) %&gt;% summarise(total_male = sum(n)) totals &lt;- merge(totals_female, totals_male) totsm &lt;- merge(babynames, totals, by = &quot;year&quot;) head(totsm) ## year sex name n prop total_female total_male ## 1 1880 F Mary 7065 0.07238359 90993 110491 ## 2 1880 F Anna 2604 0.02667896 90993 110491 ## 3 1880 F Emma 2003 0.02052149 90993 110491 ## 4 1880 F Elizabeth 1939 0.01986579 90993 110491 ## 5 1880 F Minnie 1746 0.01788843 90993 110491 ## 6 1880 F Margaret 1578 0.01616720 90993 110491 We can then calculate, for all babies born on or after 1920, the number of babies born with that name and sex. With this information, we can then get the proportion of all babies with a given name that were of a particular sex. For example, if 92% of babies born with the name “Mary” were female, this would give us a .92 probability that an individual with the name “Mary” is female. We do this for every name in the dataset, excluding names for which the proportion is equal to .5; i.e., for names that we cannot adjudicate between whether they are more or less likely male or female. totprops &lt;- totsm %&gt;% filter(year &gt;= 1920) %&gt;% group_by(name, year) %&gt;% mutate(sumname = sum(n), prop = ifelse(sumname==n, 1, n/sumname)) %&gt;% filter(prop!=.5) %&gt;% group_by(name) %&gt;% slice(which.max(prop)) %&gt;% summarise(prop = max(prop), totaln = sum(n), name = max(name), sex = unique(sex)) head(totprops) ## # A tibble: 6 x 4 ## name prop totaln sex ## &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;chr&gt; ## 1 Aaban 1 5 M ## 2 Aabha 1 7 F ## 3 Aabid 1 5 M ## 4 Aabir 1 5 M ## 5 Aabriella 1 5 F ## 6 Aada 1 5 F Once we have our proportions for all names, we can then merge these back with the names of our artists from the Edinburgh Fringe Book Festival. We can then easily plot the proportion of artists at the Festival who are male versus female in each year of the Festival. ednameprops &lt;- merge(totprops, gendes, by = &quot;name&quot;) ggplot(ednameprops, aes(x=year, fill = factor(sex))) + geom_bar(position = &quot;fill&quot;) + xlab(&quot;Year&quot;) + ylab(&quot;% women authors&quot;) + labs(fill=&quot;&quot;) + scale_y_continuous(labels = scales::percent_format(accuracy = 1)) + theme_tufte(base_family = &quot;Helvetica&quot;) + geom_abline(slope=0, intercept=0.5, col = &quot;black&quot;,lty=2) What can we conclude form this graph? Note that when we merged the proportions from th “babynames” data with our Edinburgh Fringe data we lost some observations. This is because some names in the Edinburgh Fringe data had no match in the “babynames” data. Let’s look at the names that had no match: names1 &lt;- ednameprops$name names2 &lt;- gendes$name diffs &lt;- setdiff(names2, names1) diffs ## [1] &quot;L&quot; &quot;Kapka&quot; &quot;Menzies&quot; &quot;Ros&quot; &quot;G&quot; ## [6] &quot;Pankaj&quot; &quot;Uzodinma&quot; &quot;Rodge&quot; &quot;A&quot; &quot;Zoë&quot; ## [11] &quot;László&quot; &quot;Sadakat&quot; &quot;Michèle&quot; &quot;Maajid&quot; &quot;Yrsa&quot; ## [16] &quot;Ahdaf&quot; &quot;Noo&quot; &quot;Dilip&quot; &quot;Sjón&quot; &quot;François&quot; ## [21] &quot;J&quot; &quot;K&quot; &quot;Aonghas&quot; &quot;S&quot; &quot;Bashabi&quot; ## [26] &quot;Kjartan&quot; &quot;Romesh&quot; &quot;T&quot; &quot;Chibundu&quot; &quot;Yiyun&quot; ## [31] &quot;Fiammetta&quot; &quot;W&quot; &quot;Sindiwe&quot; &quot;Cat&quot; &quot;Jez&quot; ## [36] &quot;Fi&quot; &quot;Sunder&quot; &quot;Saci&quot; &quot;C.J&quot; &quot;Halik&quot; ## [41] &quot;Niccolò&quot; &quot;Sifiso&quot; &quot;C.S.&quot; &quot;DBC&quot; &quot;Phyllida&quot; ## [46] &quot;R&quot; &quot;Struan&quot; &quot;C.J.&quot; &quot;SF&quot; &quot;Nadifa&quot; ## [51] &quot;Jérome&quot; &quot;D&quot; &quot;Xiaolu&quot; &quot;Ramita&quot; &quot;John-Paul&quot; ## [56] &quot;Ha-Joon&quot; &quot;Niq&quot; &quot;Andrés&quot; &quot;Sasenarine&quot; &quot;Frane&quot; ## [61] &quot;Alev&quot; &quot;Gruff&quot; &quot;Line&quot; &quot;Zakes&quot; &quot;Pip&quot; ## [66] &quot;Witi&quot; &quot;Halsted&quot; &quot;Ziauddin&quot; &quot;J.&quot; &quot;Åsne&quot; ## [71] &quot;Alecos&quot; &quot;.&quot; &quot;Julián&quot; &quot;Sunjeev&quot; &quot;A.C.S&quot; ## [76] &quot;Etgar&quot; &quot;Hyeonseo&quot; &quot;Jaume&quot; &quot;A.&quot; &quot;Jesús&quot; ## [81] &quot;Jón&quot; &quot;Helle&quot; &quot;M&quot; &quot;Jussi&quot; &quot;Aarathi&quot; ## [86] &quot;Shappi&quot; &quot;Macastory&quot; &quot;Odafe&quot; &quot;Chimwemwe&quot; &quot;Hrefna&quot; ## [91] &quot;Bidisha&quot; &quot;Packie&quot; &quot;Tahmima&quot; &quot;Sara-Jane&quot; &quot;Tahar&quot; ## [96] &quot;Lemn&quot; &quot;Neu!&quot; &quot;Jürgen&quot; &quot;Barroux&quot; &quot;Jan-Philipp&quot; ## [101] &quot;Non&quot; &quot;Metaphrog&quot; &quot;Wilko&quot; &quot;Álvaro&quot; &quot;Stef&quot; ## [106] &quot;Erlend&quot; &quot;Grinagog&quot; &quot;Norma-Ann&quot; &quot;Fuchsia&quot; &quot;Giddy&quot; ## [111] &quot;Joudie&quot; &quot;Sav&quot; &quot;Liu&quot; &quot;Jayne-Anne&quot; &quot;Wioletta&quot; ## [116] &quot;Sinéad&quot; &quot;Katherena&quot; &quot;Siân&quot; &quot;Dervla&quot; &quot;Teju&quot; ## [121] &quot;Iosi&quot; &quot;Daša&quot; &quot;Cosey&quot; &quot;Bettany&quot; &quot;Thordis&quot; ## [126] &quot;Uršuľa&quot; &quot;Limmy&quot; &quot;Meik&quot; &quot;Zindzi&quot; &quot;Dougie&quot; ## [131] &quot;Ngugi&quot; &quot;Inua&quot; &quot;Ottessa&quot; &quot;Bjørn&quot; &quot;Novuyo&quot; ## [136] &quot;Rhidian&quot; &quot;Sibéal&quot; &quot;Hsiao-Hung&quot; &quot;Audur&quot; &quot;Sadek&quot; ## [141] &quot;Özlem&quot; &quot;Zaffar&quot; &quot;Jean-Pierre&quot; &quot;Lalage&quot; &quot;Yaba&quot; ## [146] &quot;H&quot; &quot;DJ&quot; &quot;Sigitas&quot; &quot;Clémentine&quot; &quot;Celeste-Marie&quot; ## [151] &quot;Marawa&quot; &quot;Ghillie&quot; &quot;Ahdam&quot; &quot;Suketu&quot; &quot;Goenawan&quot; ## [156] &quot;Niviaq&quot; &quot;Steinunn&quot; &quot;Shoo&quot; &quot;Ibram&quot; &quot;Venki&quot; ## [161] &quot;DeRay&quot; &quot;Diarmaid&quot; &quot;Serhii&quot; &quot;Harkaitz&quot; &quot;Adélaïde&quot; ## [166] &quot;Agustín&quot; &quot;Jérôme&quot; &quot;Siobhán&quot; &quot;Nesrine&quot; &quot;Jokha&quot; ## [171] &quot;Gulnar&quot; &quot;Uxue&quot; &quot;Taqralik&quot; &quot;Tayi&quot; &quot;E&quot; ## [176] &quot;Dapo&quot; &quot;Dunja&quot; &quot;Maaza&quot; &quot;Wayétu&quot; &quot;Shokoofeh&quot; Do we notice anything about these names? What does this tell us about the potential biases of using such sources as US baby names data as a foundation for gender prediction? What are alternative ways we might go about this task? 11.12 Exercises Filter the books by genre (selecting e.g., “Literature” or “Children”) and plot frequency of women-related words over time. Choose another set of terms by which to filter (e.g., race-related words) and plot their frequency over time. 11.13 References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
