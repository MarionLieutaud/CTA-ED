---
title: "Computational Text Analysis"
author: "Christopher Barrie"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
output: bookdown::bs4_book
documentclass: book
bibliography: [book.bib, packages.bib, CTA.bib]
url: https://cjbarrie.github.io/CTA-ED/
cover-image: coverb.png
description: |
  Online book for ten-week course in "Computational Text Analysis" (PGSP11584)
link-citations: yes
github-repo: cjbarrie/CTA-ED
---

# "Computational Text Analysis" (PGSP11584) {.unnumbered}

![cover](coverb.png){.cover width="250"} This is the dedicated webpage for the course Computational Text Analysis" [(PGSP11584)](http://www.drps.ed.ac.uk/21-22/dpt/cxpgsp11584.htm) at the University of Edinburgh, taught by Christopher Barrie. Go to the Course Overview and Introduction tabs for a course overview and introduction to R.

We will be using this online book throughout the course. Each week has a set of essential and recommended readings. The essential readings must be consulted in full prior to the Lecture and Seminar for that week. In addition, you will find online worksheets and examples written in R. This is a "live" book and will be amended and updated during the course itself.

## Structure

The course is structured of alternating weeks of substantive and technical instruction.

| Week               | Focus                                                                                                              | Coding assignment(s)                                                                                                                                                              | Class activity                                                                                                              |
|----------------|----------------|------------------------|----------------|
| 1                  | Retrieving and analyzing text information                                                                          | [Introductory exercises](https://cjbarrie.github.io/CTA-ED/introduction-to-r.html) + [RTC Workshop by Ugur Ozdemir](https://research-training-centre.sps.ed.ac.uk/micro-methods/) | Seminar discussion                                                                                                          |
| 2                  | Tokenization and word frequencies                                                                                  | [Demo](https://cjbarrie.github.io/CTA-ED/week-2-demo.html)                                                                                                                        | Seminar discussion                                                                                                          |
| 3                  | Dictionary-based techniques                                                                                        | [Demo](https://cjbarrie.github.io/CTA-ED/week-3-demo.html) + [Worksheet 2](https://cjbarrie.github.io/CTA-ED/exercise-2-dictionary-based-methods.html)                            | Flash talk + [Worksheet 1](https://cjbarrie.github.io/CTA-ED/exercise-1-word-frequency-analysis.html) group work            |
| 4                  | Natural language, complexity, and similarity                                                                       | [Demo](https://cjbarrie.github.io/CTA-ED/week-4-demo.html)                                                                                                                        | Coding demo of Worksheet 2 + Seminar discussion                                                                             |
| 5                  | Scaling techniques                                                                                                 | [Demo](https://cjbarrie.github.io/CTA-ED/week-5-demo.html) + [Worksheet 4](https://cjbarrie.github.io/CTA-ED/exercise-4-scaling-techniques.html)                                  | Flash talk + [Worksheet 3](https://cjbarrie.github.io/CTA-ED/exercise-3-comparison-and-complexity.html) group work          |
| 6                  | Unsupervised learning (topic models)                                                                               | [Demo](https://cjbarrie.github.io/CTA-ED/week-6-demo.html)                                                                                                                        | Coding demo of Worksheet 4 + Seminar discussion                                                                             |
| 7                  | Unsupervised learning (word embedding)                                                                             | [Demo](https://cjbarrie.github.io/CTA-ED/week-7-demo.html) + [Worksheet 6](https://cjbarrie.github.io/CTA-ED/exercise-6-unsupervised-learning-word-embedding.html)                | Flash talk + [Worksheet 5](https://cjbarrie.github.io/CTA-ED/exercise-5-unsupervised-learning-topic-models.html) group work |
| 8                  | Sampling text information                                                                                          | [Demo](https://cjbarrie.github.io/CTA-ED/week-8-demo.html)                                                                                                                        | Coding demo of Worksheet 6 + Seminar discussion                                                                             |
| 9                  | Supervised learning                                                                                                |                                                                                                                                                                                   |                                                                                                                             |
| Demo + Worksheet 8 | Flash talk + [Worksheet 7](https://cjbarrie.github.io/CTA-ED/exercise-7-sampling-text-information.html) group work |                                                                                                                                                                                   |                                                                                                                             |
| 10                 | Validation                                                                                                         | Demo + Worksheet 9                                                                                                                                                                | Coding demo of Worksheet 8 + Seminar discussion                                                                             |

## Acknowledgments {.unnumbered}

When compiling this course, I benefited from syllabus materials shared online by Margaret Roberts, Alexandra Siegel, and Arthur Spirling. Thanks also to Justin Grimmer, Margaret Roberts, and Brandon Stewart for providing early view access to their forthcoming [*Text as Data*](https://press.princeton.edu/books/hardcover/9780691207544/text-as-data) book.

<!--chapter:end:index.Rmd-->

# Course Overview {.unnumbered}

In recent years, the use of computational techniques for the quantitative analysis of text has exploded. The volume and quantity of text data to which we now have access in the digital age is enormous. This has led social scientists to seek out new means of analyzing text data at scale.

We will see that text records, be they in the form of digital traces left on social media platforms, archived works of literature, parliamentary speeches, video transcripts, or print news, can help us answer a huge range of important questions.

## Learning outcomes {.unnumbered}

This course will give students training in the use of computational text analysis techniques. The course will prepare students for dissertation work that uses textual data and will provide hands-on training in the use of the R programming language and (some) Python.

The course will provide a venue for seminar discussion of examples using these methods in the empirical social sciences as well as lectures on the technical and/or statistical dimensions of their application.

## Course structure {.unnumbered}

We will be using this online book for the ten-week course in "Computational Text Analysis" (PGSP11584). Each chapter contains the readings for that week. The book also includes worksheets with example code for how to conduct some of the text analysis techniques we discuss each week.

Each week (with the partial exception of week 1), we will be discussing, alternately, the substantive and technical dimensions of published research in the empirical social sciences. The readings for each week generally contain two "substantive" readings---that is, examples of the application of text analysis techniques with empirical data---and one "technical" reading that focuses mainly on the statistical and computational aspects of a given technique.

We will study first the technical aspects of analytical approaches and, second, the substantive dimensions of these applications. This means that, when discussing the readings, we will be able to discuss how satisfactory a given approach is for illuminating the question or topic at hand.

Lectures will primarily be focused on the technical dimensions of a given technique. The seminar (Q&A) that follows will give us the opportunity to study and discuss questions of social scientific interest, and how computational text analysis had been used to answer these.

## Course pre-preparation {.unnumbered}

**NOTE**: Before the lecture in Week 2, students should complete two introductory R exercises. Those students who have already done this for my courses in Semester 1 do not need to do this.

For those who haven't done any of the pre-preparation tasks already, you should, first, consult the worksheet [here](https://cjbarrie.github.io/CTA-ED/introduction-to-r.html), which is an introduction to setting up and understanding the very basics of working in R. Second, Ugur Ozdemir has provided such a more comprehensive introductory R course for the Research Training Centre at the University of Edinburgh and you can follow the instructions [here](https://research-training-centre.sps.ed.ac.uk/micro-methods/) to access this.

## Reference sources {.unnumbered}

There are several other reference texts that will be of use during this course:

-   Wickham, Hadley and Garrett Grolemund. R for Data Science: <https://r4ds.had.co.nz/>
-   Silge, Julia and David Robinson. Text Mining with R: <https://www.tidytextmining.com/>
    -   For learning <tt>tidytext</tt>, this online tutorial will be used: <https://juliasilge.shinyapps.io/learntidytext/>
-   (later in the course) Hvitfelft, Emil and Julia Silge. Supervised Machine Learning for Text Analysis in R: <https://smltar.com/>

In several weeks, we will also be referring to two other textbooks, available online, on information retrieval and text processing. These are:

-   Jurafsky, Dan and James H. Martin. Speech and Language Processing (3rd ed. draft): <https://nlp.stanford.edu/IR-book/information-retrieval-book.html>
-   Manning, Christopher D.,Prabhakar Raghavan, and Hinrich Schütze. Introduction to Information Retrieval: <https://nlp.stanford.edu/IR-book/information-retrieval-book.html>

## Assessment {.unnumbered}

### Fortnightly worksheets {.unnumbered}

Each fortnight, I will provide you with one worksheet that walks you through how to implement a different text analysis technique. At the end of these worksheets you will find a set of questions. **You should buddy up with someone else in your class and go through these together.**

This is called "pair programming" and there's a reason we do this. Firstly, coding can be an isolating and difficult thing---it's good to bring a friend along for the ride! Secondly, if there's something you don't know, maybe your buddy will. This saves you both time. Thirdly, your buddy can check your code as you write it, and vice versa. Again, this means both of you are working together to produce and check something as you go along.

At the subsequent week's lecture, I will pick on a pair at random to answer each one of that worksheet's questions (i.e., there is \~1/3 chance you're going to get picked each week). I will ask you to walk us through your code. And remember: it's also fine if you struggled and didn't get to the end! If you encountered an obstacle, we can work through that together. All that matters to me is that you **try**.

*The remainder of the seminar on worksheet weeks will be dedicated to seminar discussion where we discuss the readings together.*

### Fortnightly flash talks {.unnumbered}

On the weeks where you are not going to be tasked with a coding assignment, you're not off the hook... I will again be selecting a pair at random (the same as your coding pair) to talk me through one of the readings. I will pick a different pair for each reading (i.e., \~ 1/3 chance again).

Don't let this be cause of great anguish: I just want **thirty seconds to a few minutes** where you lay out for me at least one---but preferably two or three---criticisms you had of any of the articles that are required reading for that week,

Here, you will want to think about whether the article really answered the research question, whether the data was appropriate for answering that question, whether the method was appropriate for answering that question, and whether the results show what the author claims they show.

*The remainder of the seminar on flash talk weeks will be dedicated to group work where we go through the coding Worksheet together.*

### Final assessment {.unnumbered}

Assessment takes the form of **one** summative assessment. This will be a 4000 word essay on a subject of your choosing (with prior approval by me). For this, you will be required to select from a range of data sources I will provide. You may also suggest your own data source.

You will be asked to: a) formulate a research question; b) use at least one computational text analysis technique that we have studied; c) conduct an analysis of the data source you have provided; d) write up the initial findings; and e) outline potential extensions of your analysis.

You will then provide the code you used in reproducible (markdown) format and will be assessed on both the substantive content of your essay contribution (the social science part) as well as your demonstrated competency in coding and text analysis (the computational part).

<!--chapter:end:00-course-overview.Rmd-->

# Introduction to R {.unnumbered}

This section is designed to ensure you are familiar with the <tt>R</tt> environment.

## Getting started with R at home 

Given that we're all working from home these days, you'll need to download R and RStudio onto your own devices. R is the name of the programming language that we'll be using for coding exercises; RStudio is the IDE ("Integrated Development Environment"), i.e., the piece of software that almost everyone uses when working in R. 

You can download both of these on Windows and Mac easily and for free. This is one of the first reasons to use an "open-source" programming language: it's free and everyone can contribute!

IT Services at the University of Edinburgh have provided a [walkthrough](https://uoe.sharepoint.com/sites/hss/sps/itservices/SPSShareSpaceManagement/Moblabdoc/SitePages/Computational-Text-Analysis.aspx?cid=59b29656-3df8-4c19-9423-24765076e742) of what is needed for you to get started. I also break this down below:

1. Install R for Mac from here: https://cran.r-project.org/bin/macosx/. Install R for Windows from here: https://cran.r-project.org/bin/windows/base/.

2. Download RStudio for Windows or Mac from here: https://rstudio.com/products/rstudio/download/, choosing the Free version: this is what most people use and is more than enough for all of our needs.

**All programs are free. Make sure to load everything listed above for your operating system or R will not work properly!**

## Some basic information 

+ A script is a text file in which you write your commands (code) and comments.

+ If you put the <tt>#</tt> character in front of a line of text this line will not be executed; this is useful to add comments to your script!

+ <tt>R</tt> is case sensitive, so be careful when typing. 

+ To send code from the script to the console, highlight the relevant line of code in your script and click on <tt>Run</tt>, or select the line and hit <tt>ctrl+enter</tt> on PCR or <tt>cmd+enter</tt> on Mac

+ Access help files for <tt>R</tt> functions by preceding the name of the function with <tt>?</tt> (e.g., <tt>?table</tt>)

+ By pressing the <tt>up</tt> key, you can go back to the commands you have used before

+ Press the <tt>tab</tt> key to auto-complete variable names and commands

## Getting Started in RStudio 

Begin by opening RStudio (located on the desktop). Your first task is to create a new script (this is where we will write our commands). To do so, click: 

```{r eval=FALSE}
File --> NewFile --> RScript
```

Your screen should now have four panes:

+ the Script (top left)

+ the Console (bottom left)

+ the Environment/History (top right)

+ Files/Plots/Packages/Help/Viewer (bottom right)

## A simple example 

The Script (top left) is where we write our commands for R. You can try this out for a first time by writing a small snipped of code as follows:

```{r, eval=TRUE}

x <- "I can't wait to learn Computational Text Analysis" #Note the quotation marks!

```

To tell R to run the command, highlight the relevant row in your script and click the <tt>Run</tt> button (top right of the Script) - or hold down <tt>ctrl+enter</tt> on Windows or <tt>cmd+enter</tt> on Mac - to send the command to the Console (bottom left), where the actual evaluation and calculations are taking place. These shortcut keys will become very familiar to you very quickly!

Running the command above creates an object named ‘x’, that contains the words of your message.

You can now see ‘x’ in the Environment (top right). To view what is contained in x, type in the Console (bottom left):

```{r eval=TRUE}

print(x)

# or alternatively you can just type:

x

```

## Loading packages 

The 'base' version of <tt>R</tt> is very powerful but it will not be able to do everything on its own, at least not with ease. For more technical or specialized forms of analysis, we will need to load new packages. 

This is when we will need to install a so-called ‘package’---a program that includes new tools (i.e., functions) to carry out specific tasks. You can think of them as 'extensions' enhancing <tt>R</tt>'s capacities. 

To take one example, we might want to do something a little more exciting than print how excited we are about this course. Let's make a map instead.

This might sound technical. But the beauty of the packaged extensions of <tt>R</tt> is that they contain functions to perform specialized types of analysis with ease. 

We'll first need to install one of these packages, which you can do as below:

```{r, eval=FALSE}
install.packages("tidyverse")
```

After the package is installed, we then need to load it into our environment by typing <tt>library(<insert name of package here>)</tt>. Note that, here, you don't need to wrap the name of the package in quotation marks. So this will do the trick:

```{r, eval=T}
library(tidyverse)
```

What now? Well, let's see just how easy it is to visualize some data using <tt>ggplot</tt> which is a package that comes bundled into the larger <tt>tidyverse</tt> package.

```{r, eval=T}
ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy))
```

If we wanted to save where we'd got to with making our plots, we would want to save our scripts, and maybe the data we used as well, so that we could return to it at a later stage.

## Saving your objects, plots and scripts 

* Saving scripts: To save your script in RStudio (i.e. the top left panel), all you need to do is click File –> Save As (and choose a name for your script). Your script will be something like: myfilename.R.

* Saving plots: If you have made any plots you would like to save, click Export (in the plotting pane) and choose a relevant file extension (e.g. .png, .pdf, etc.) and size.

+ To save **individual** objects (for example <tt>x</tt> from above) from your environment, run the following command (choosing a suitable filename): 

```{r, eval=FALSE}
save(x,file="myobject.RData")
load(file="myobject.RData")
```
      
+ To save **all** of your objects (i.e. everything in the top right panel) at once, run the following command (choosing a suitable filename):

```{r, eval=FALSE}
save.image(file="myfilname.RData")
```

+ Your objects can be re-loaded into R during your next session by running:

```{r, eval=FALSE}
load(file="myfilename.RData")
```

There are many other file formats you might use to save any output. We will encounter these as the course progresses.

## Knowing where R saves your documents 

If you are at home, when you open a new script make sure to check and set your working directory (i.e. the folder where the files you create will be saved). To check your working directory use the getwd() command (type it into the Console or write it in your script in the Source Editor):

```{r, eval=FALSE}
getwd()
```

To set your working directory, run the following command, substituting the file directory of your choice. Remember that anything following the `#’ symbol is simply a clarifying comment and R will not process it.

```{r, eval=FALSE}
## Example for Mac 
setwd("/Users/Documents/mydir/") 
## Example for PC 
setwd("c:/docs/mydir") 
```

## Practicing in R 

The best way to learn <tt>R</tt> is to use it. These workshops on text analysis will not be the place to become fully proficient in <tt>R</tt>. They will, however, be a chance to conduct some hands-on analysis with applied examples in a fast-expanding field. And the best way to learn is through doing. So give it a shot! 

For some further practice in the R programming language, look no further than @wickham_r_2017 and, for tidy text analysis, @silge_text_2017.

- The free online book by Hadley Wickham "R for Data Science" is available [here](https://r4ds.had.co.nz/)

- The free online book by Julia Silge and David Robinson "Text Mining with R" is available [here](https://www.tidytextmining.com/)

- For more practice with R, you may want to consult a set of interactive tutorials, available through the package "learnr." Once you've installed this package, you can go through the tutorials yourselves by calling:

```{r, eval = F}
library(learnr)

available_tutorials() # this will tell you the names of the tutorials available

run_tutorial(name = "ex-data-basics", package = "learnr") #this will launch the interactive tutorial in a new Internet browser window

```

## One final note 

Once you've dipped into the "R for Data Science" book you'll hear a lot about the so-called <tt>tidyverse</tt> in R. This is essentially a set of packages that use an alternative, and more intuitive, way of interacting with data. 

The main difference you'll notice here is that, instead of having separate lines for each function we want to run, or wrapping functions inside functions, sets of functions are "piped" into each other using "pipe" functions, which look have the appearance: `%>%`. 

I will be using "tidy" syntax in the weekly exercises for these computational text analysis workshops. If anything is unclear, I can provide the equivalents in "base" R too. But a lot of the useful text analysis packages are now composed with 'tidy' syntax.

<!--chapter:end:00-intro.Rmd-->

# Week 1

## Retrieving and analyzing text information

Our first task when conducting large-scale text analyses is gathering and curating the text information itself. This is the focus of the chapters by @manning_introduction_2007 listed below. Here, you'll find an introduction to different ways in which we can reformat and 'query' text data in order to begin asking questions of it. This is often referred to in computer science and natural language processing contexts as "information retrieval" and is the foundation of many search, including web search, processes.

The articles by @tatman_gender_2017 and @pechenick_characterizing_2015 will be the focus of our seminar (Q&A). These articles will get us thinking about the fundamentals of text discovery and sampling. When reading the articles we should think about where we are locating our texts, how we are sampling them, what biases might inhere in this sampling process, and what these texts *represent*; i.e., about what population or phenomenon of interest they might provide inferences. 

Questions for seminar:

1. Where do we access text? What do we need to consider when doing so?
2. How do we sample texts?
3. What biases do we need to keep in mind?

**Required reading**:

- @tatman_gender_2017
- @pechenick_characterizing_2015

- @manning_introduction_2007 (chs.1 and 10): [https://nlp.stanford.edu/IR-book/information-retrieval-book.html](https://nlp.stanford.edu/IR-book/information-retrieval-book.html)
- @krippendorff_content_2004 (ch. 6)

**Further reading**:

- @olteanu_social_2019
- @biber_using_1993
- @barbera_understanding_2015

**Slides**:

- Week 1 [Slides](https://docs.google.com/presentation/d/1TljlFQwyY8xoa5qr5R3EBasc0U-ixoUJx_D0RieJs_o/edit?usp=sharing)

<!--chapter:end:01-week1.Rmd-->

# Week 2

## Tokenization and word frequencies

When approaching large-scale quantiative analyses of text, a key task is how we identify and capture the unit of analysis. One of the most commonly used approaches, across diverse analytical contexts, is text tokenization. Here, we are splitting the text into word units: unigrams, bigrams, trigrams etc. 

The chapters by @manning_introduction_2007, listed below, provide a technical introduction to the task of "querying" text according to different word-based queries. This is a task we will be studying in the hands-on assignment for this week. 

For the seminar discussion, we will be focusing on some widely-cited examples of research in the applied social sciences employing token-based, or word frequency, analyses of large corpora. The first, by @michel_quantitative_2011 uses the enormous Google books corpus to measure cultural and linguistic trends. The second, by @bollen_historical_2021 uses the same corpus to demonstrate a more specific change over time---so-called "cognitive distortion." In both examples, we should be attentive to questions of sampling covered in previous weeks. This question is central to the back-and-forths in the short responses and replies to the articles by @michel_quantitative_2011 and @bollen_historical_2021.   

Questions:

1. Tokenizing and counting: what does this capture?
2. Corpus-based sampling: what biases might threaten inference?
3. If you had to write a critique of either @michel_quantitative_2011 or @bollen_historical_2021, what would it focus on?

**Required reading**:

- @michel_quantitative_2011
  - @schwartz_culturomics_2011
  - @morse-gagne_culturomics_2011
  - @aiden_culturomicsresponse_2011
  
- @bollen_historical_2021
  - @schmidt_uncontrolled_2021
  - @bollen_reply_2021
  
- @manning_introduction_2007 (ch. 2): [https://nlp.stanford.edu/IR-book/information-retrieval-book.html](https://nlp.stanford.edu/IR-book/information-retrieval-book.html)]
- @krippendorff_content_2004 (ch. 5)

**Further reading**:

- @rozado_prevalence_2021
- @alshaabi_storywrangler_2021
- @campos_survey_2015
- @greenfield_changing_2013

**Slides**:

- Week 2 [Slides](https://docs.google.com/presentation/d/1EB8l2R3aDnfabpx23qKq-dH-6HehfgDqC9n1Pc90jN8/edit?usp=sharing)

<!--chapter:end:02-week2.Rmd-->

# Week 2 Demo

## Setup

In this section, we'll have a quick overview of how we're processing text data when conducting analyses of word frequency. We'll be using some randomly simulated text. 

First we load the packages that we'll be using:

```{r, warning = F, message = F}
library(stringi) #to generate random text
library(dplyr) #tidyverse package for wrangling data
library(tidytext) #package for 'tidy' manipulation of text data
library(ggplot2) #package for visualizing data
library(scales) #additional package for formatting plot axes
library(kableExtra) #package for displaying data in html format (relevant for formatting this worksheet mainly)
```

## Tokenizing

We'll first get some random text to see what it looks like when we're tokenizing text.

```{r}
lipsum_text <- data.frame(text = stri_rand_lipsum(1, start_lipsum = TRUE))

head(lipsum_text$text)
```

We can then tokenize with the `unnest_tokens()` function in `tidytext`. 

````{r}
tokens <- lipsum_text %>%
  unnest_tokens(word, text)

head(tokens)
```

Now we'll get some larger data, simulating 5000 observations (rows) of random Latin text strings. 

```{r}
## Varying total words example
lipsum_text <- data.frame(text = stri_rand_lipsum(5000, start_lipsum = TRUE))
```

We'll then add another column and call this "weeks." This will be our unit of analysis. 

```{r}
# make some weeks one to ten
lipsum_text$week <- as.integer(rep(seq.int(1:10), 5000/10))
```

Now we'll simulate a trend where we see an increasing number of words as weeks go by. Don't worry too much about this as the code is a little more complex, but I share it here in case of interest.

```{r}
for(i in 1:nrow(lipsum_text)) {
  week <- lipsum_text[i, 2]
  morewords <-
    paste(rep("more lipsum words", times = sample(1:100, 1) * week), collapse = " ")
  lipsum_words <- lipsum_text[i, 1]
  new_lipsum_text <- paste0(morewords, lipsum_words, collapse = " ")
  lipsum_text[i, 1] <- new_lipsum_text
}
```

And we can see that as each week goes by, we have more and more text.

```{r}
lipsum_text %>%
  unnest_tokens(word, text) %>%
  group_by(week) %>%
  dplyr::count(word) %>%
  select(week, n) %>%
  distinct() %>%
  ggplot() +
  geom_bar(aes(week, n), stat = "identity") +
  labs(x = "Week", y = "n words") +
  scale_x_continuous(breaks= pretty_breaks())
```

We can then do the same but with a trend where each week sees a decreasing number of words.

```{r}
# simulate decreasing words trend
lipsum_text <- data.frame(text = stri_rand_lipsum(5000, start_lipsum = TRUE))

# make some weeks one to ten
lipsum_text$week <- as.integer(rep(seq.int(1:10), 5000/10))

for(i in 1:nrow(lipsum_text)) {
  week <- lipsum_text[i,2]
  morewords <- paste(rep("more lipsum words", times = sample(1:100, 1)* 1/week), collapse = " ")
  lipsum_words <- lipsum_text[i,1]
  new_lipsum_text <- paste0(morewords, lipsum_words, collapse = " ")
  lipsum_text[i,1] <- new_lipsum_text
}

lipsum_text %>%
  unnest_tokens(word, text) %>%
  group_by(week) %>%
  dplyr::count(word) %>%
  select(week, n) %>%
  distinct() %>%
  ggplot() +
  geom_bar(aes(week, n), stat = "identity") +
  labs(x = "Week", y = "n words") +
  scale_x_continuous(breaks= pretty_breaks())
```

Now let's check out the top frequency words in this text.

```{r}
lipsum_text %>%
  unnest_tokens(word, text) %>%
  dplyr::count(word, sort = T) %>%
  top_n(5) %>%
  knitr::kable(format="html")%>% 
  kable_styling("striped", full_width = F)
```

We're going to check out the frequencies for the word "sed" and then we're gonna normalize these by denominating by total word frequencies for each week.

First we need to get total word frequencies for each week.

```{r}
lipsum_totals <- lipsum_text %>%
  group_by(week) %>%
  unnest_tokens(word, text) %>%
  dplyr::count(word) %>%
  mutate(total = sum(n)) %>%
  distinct(week, total)
```

```{r}
# let's look for "sed"
lipsum_sed <- lipsum_text %>%
  group_by(week) %>%
  unnest_tokens(word, text) %>%
  filter(word == "sed")  %>%
  dplyr::count(word) %>%
  mutate(total_sed = sum(n)) %>%
  distinct(week, total_sed)

```

Then we can join these two dataframes together with the `left_join()` function where we're joining by the "week" column. We can then pipe the joined data into a plot.

```{r}
lipsum_sed %>%
  left_join(lipsum_totals, by = "week") %>%
  mutate(sed_prop = total_sed/total) %>%
  ggplot() +
  geom_line(aes(week, sed_prop)) +
  labs(x = "Week", y = "
       Proportion sed word") +
  scale_x_continuous(breaks= pretty_breaks())
```

## Regexing

You'll notice that in the worksheet on word frequencies that at one point there are a set of parentheses after `str_detect()` we have the string "[a-z]". This is called a __character class__ and these use square brackets like `[]`.

Other character classes include, as helpfully listed in this [vignette](https://cran.r-project.org/web/packages/stringr/vignettes/regular-expressions.html) for the <tt>stringr</tt> package. What follows is adapted from these materials on regular expressions. 

* `[abc]`: matches a, b, or c.
* `[a-z]`: matches every character between a and z 
   (in Unicode code point order).
* `[^abc]`: matches anything except a, b, or c.
* `[\^\-]`: matches `^` or `-`.

Several other patterns match multiple characters. These include:

*   `\d`: matches any digit; the opposite of this is `\D`, which matches any character that 
    is not a decimal digit.

```{r}
str_extract_all("1 + 2 = 3", "\\d+")
str_extract_all("1 + 2 = 3", "\\D+")
```
    
*   `\s`: matches any whitespace; its opposite is `\S`
    
```{r}
(text <- "Some  \t badly\n\t\tspaced \f text")
str_replace_all(text, "\\s+", " ")
```

*   `^`: matches start of the string
    
```{r}
x <- c("apple", "banana", "pear")
str_extract(x, "^a")
```
*   `$`: matches end of the string
    
```{r}
x <- c("apple", "banana", "pear")
str_extract(x, "^a$")
```

*   `^` then `$`: exact string match
    
```{r}
x <- c("apple", "banana", "pear")
str_extract(x, "^apple$")
```

Hold up: what do the plus signs etc. mean?

* `+`: 1 or more.
* `*`: 0 or more.
* `?`: 0 or 1.

So if you can tell me why this output makes sense, you're getting there!

```{r}
str_extract_all("1 + 2 = 3", "\\d+")[[1]]
str_extract_all("1 + 2 = 3", "\\D+")[[1]]

str_extract_all("1 + 2 = 3", "\\d*")[[1]]
str_extract_all("1 + 2 = 3", "\\D*")[[1]]

str_extract_all("1 + 2 = 3", "\\d?")[[1]]
str_extract_all("1 + 2 = 3", "\\D?")[[1]]
```

### Some more regex resources:

1. Regex crossword: [https://regexcrossword.com/](https://regexcrossword.com/).
2. Regexone: [https://regexone.com/](https://regexone.com/)
3. R4DS [chapter 14](https://r4ds.had.co.nz/strings.html#matching-patterns-with-regular-expressions)

<!--chapter:end:02-week2demo.Rmd-->

# Week 3

## Dictionary-based techniques

An extension of word frequency analyses, which we covered last week, are so-called "dictionary-based" techniques. In their most basic form, these analyses use an index of target terms and classify the corpus of interest based on their presence or absence. The technical dimensions of this type of analysis are covered in the chapter section by @krippendorff_content_2004, and some of the issues attending them in the article by - @loughran_when_2011.

We will also be reading two examples of the application of these techniques by @martins_rise_2020 and @young_affective_2012. Here, we will be discussing how successful the authors are in measuring the phenomenon of interest ("prosociality" and "tone" respectively). Questions about sampling and representativeness will again be relevant here, and will naturally inform our assessments of this work.

Questions:

1. Are *general* dictionaries possible; or do they have to be domain-specific?
2. How do we know if our dictionary is accurate?
3. How could we enhance/supplement dictionary-based techniques?

**Required reading**:

- @martins_rise_2020
- @young_affective_2012

- @loughran_when_2011
- @krippendorff_content_2004 (pp.283-289)

**Further reading**:

- @tausczik_psychological_2010
- @brier_computer_2011
- @barbera_automated_2021

**Slides**:

- Week 3 [Slides](https://docs.google.com/presentation/d/1rgYCYGtZ7resCd7oVsGnaCpwWKduunl8s1famjtGtBY/edit?usp=sharing)

<!--chapter:end:03-week3.Rmd-->

# Week 3 Demo

In this section, we'll have a quick overview of how we're processing text data when conducting basic sentiment analyses.

## Setup

We'll first load the packages we need. 

```{r, message=F, warning=F}
library(stringi)
library(dplyr)
library(tidytext)
library(ggplot2)
library(scales)
```

## Happy words

As we discussed in the lectures, we might find in our text of the class's collective thoughts that there was an increase in "happy" words over time. 

I have simulated a dataset of text split by weeks, students, and words plus whether or not the word is the word "happy" where `0` means it is not the word "happy" and `1` means it is. 

```{r, echo = F}
## Varying total happy word numbers example

lipsum_text <- data.frame(text = stri_rand_lipsum(5000, start_lipsum = TRUE))

lipsum_text$happyn <- as.character("")
lipsum_text$happyu <- as.character("")
lipsum_text$happyd <- as.character("")

lipsum_text$week <- as.integer(rep(seq.int(1:10), 5000/10))
lipsum_text$student <- sample(1:30,5000, replace = T)

for(i in 1:nrow(lipsum_text)) {
  week <- lipsum_text[i,5]
  happyn <- paste(rep("happy", times = sample(1:100, 1)), collapse = " ")
  happyu <- paste(rep("happy", times = sample(1:100, 1)*week), collapse = " ")
  happyd <- paste(rep("happy", times = sample(1:100, 1)*1/week), collapse = " ")
  lipsum_text[i, 2] <- happyn
  lipsum_text[i, 3] <- happyu
  lipsum_text[i, 4] <- happyd
}

lipsum_text$happyn <- paste(lipsum_text$text, lipsum_text$happyn)
lipsum_text$happyu <- paste(lipsum_text$text, lipsum_text$happyu)
lipsum_text$happyd <- paste(lipsum_text$text, lipsum_text$happyd)

happyn <- lipsum_text %>% 
  select(happyn, week, student) %>%
  unnest_tokens(word, happyn) %>%
  group_by(week, student) 
happyu <- lipsum_text %>% 
  select(happyu, week, student) %>%
  unnest_tokens(word, happyu) %>%
  group_by(week, student) 
happyd <- lipsum_text %>% 
  select(happyd, week, student) %>%
  unnest_tokens(word, happyd) %>%
  group_by(week, student)

happyn$happy <- as.integer(grepl("happy", x = happyn$word))
happyu$happy <- as.integer(grepl("happy", x = happyu$word))
happyd$happy <- as.integer(grepl("happy", x = happyd$word))

happys <- list(happyn, happyu, happyd)

```

We have three datasets: one with a constant number of "happy" words; one with an increasing number of "happy" words; and one with a decreasing number of "happy" words. These are called: `happyn`, `happyu`, and `happyd` respectively.

```{r}
head(happyn)
```
```{r}
head(happyu)
```
```{r}
head(happyd)
```

We can then see the trend in "happy" words over by week and student.

First, the dataset where we have a constant number of happy words over time.

```{r, echo = F}
happyn %>%
  group_by(week, student) %>%
  mutate(index_total = n()) %>%
  filter(happy == 1) %>%
  summarise(
    sum_hap = sum(happy),
    index_total = index_total,
    prop_hap = sum_hap / index_total
  ) %>%
  distinct() %>%
  ggplot() +
  geom_point(aes(week, prop_hap)) +
  geom_smooth(aes(week, prop_hap)) +
  scale_x_continuous(breaks = pretty_breaks())
```

And now the simulated data with an increasing number of happy words.

```{r, echo = F}
happyu %>%
  group_by(week, student) %>%
  mutate(index_total = n()) %>%
  filter(happy == 1) %>%
  summarise(
    sum_hap = sum(happy),
    index_total = index_total,
    prop_hap = sum_hap / index_total
  ) %>%
  distinct() %>%
  ggplot() +
  geom_point(aes(week, prop_hap)) +
  geom_smooth(aes(week, prop_hap)) +
  scale_x_continuous(breaks = pretty_breaks())
```

And finally a decreasing number of happy words.

```{r, echo = F}
happyd %>%
  group_by(week, student) %>%
  mutate(index_total = n()) %>%
  filter(happy == 1) %>%
  summarise(
    sum_hap = sum(happy),
    index_total = index_total,
    prop_hap = sum_hap / index_total
  ) %>%
  distinct() %>%
  ggplot() +
  geom_point(aes(week, prop_hap)) +
  geom_smooth(aes(week, prop_hap)) +
  scale_x_continuous(breaks = pretty_breaks())
```

## Normalizing sentiment

But as we discussed in the lecture, we also know that just because the total number of happy words increases, this isn't indication on its own that we're getting happier as a class over time. 

Before we can begin to make any such inference, we need to normalize by the total number of words each week. 

Below, I simulate data where the number of happy words is actually the same each week (our `happyn` dataset above). 

I join these data with three other datasets: `happylipsumn`, `happylipsumu`, and `happylipsumd`. These are datasets of random text, with the *same* number of happy words. 

The first of these also has the *same* number of total words each week. The second two, however, have *differing* number of *total* words each week: `happylipsumu` has an increasing number of total words each week; `happylipsumd` has a decreasing number of total words each week.

```{r, echo=F}
lipsum_text$lipsumn <- as.character("")
lipsum_text$lipsumu <- as.character("")
lipsum_text$lipsumd <- as.character("")

for(i in 1:nrow(lipsum_text)) {
  week <- lipsum_text[i,5]
  lipsumn <- paste(rep(stri_rand_lipsum(1, start_lipsum = TRUE), 
                       times = 1), collapse = " ")
  lipsumu <- paste(rep(stri_rand_lipsum(1, start_lipsum = TRUE), 
                       times = sample(1:10,1)*week), collapse = " ")
  lipsumd <- paste(rep(stri_rand_lipsum(1, start_lipsum = TRUE), 
                       times = sample(1:10,1)*1/week), collapse = " ")
  lipsum_text[i, 7] <- lipsumn
  lipsum_text[i, 8] <- lipsumu
  lipsum_text[i, 9] <- lipsumd
}

lipsum_text$happylipsumn <- paste(lipsum_text$lipsumn, lipsum_text$happyn)
lipsum_text$happylipsumu <- paste(lipsum_text$lipsumu, lipsum_text$happyn)
lipsum_text$happylipsumd <- paste(lipsum_text$lipsumd, lipsum_text$happyn)

happylipsumn <- lipsum_text %>% 
  select(happylipsumn, week, student) %>%
  unnest_tokens(word, happylipsumn) %>%
  group_by(week, student) 
happylipsumu <- lipsum_text %>% 
  select(happylipsumu, week, student) %>%
  unnest_tokens(word, happylipsumu) %>%
  group_by(week, student) 
happylipsumd <- lipsum_text %>% 
  select(happylipsumd, week, student) %>%
  unnest_tokens(word, happylipsumd) %>%
  group_by(week, student) 

happylipsumn$happy <- as.integer(grepl("happy", x = happylipsumn$word))
happylipsumu$happy <- as.integer(grepl("happy", x = happylipsumu$word))
happylipsumd$happy <- as.integer(grepl("happy", x = happylipsumd$word))

lipsums <- list(happylipsumn, happylipsumu, happylipsumd)

```

Again, as you see below, we're splitting by week, student, word, and whether or not it is a "happy" word. 

```{r}
head(happylipsumn)

head(happylipsumu)

head(happylipsumd)
```

Then if we plot the number of happy words *divided* by the number of total words each week for each student in each of these datasets, we get the below. 

To get this normalized sentiment score--or "happy" score--we need to create a variable (column) in our dataframe that is the sum of happy words divided by the total number of words in the dataframe. 

We can do this in the following way. 

```{r}
happylipsumn %>%
    group_by(week, student) %>%
    mutate(index_total = n()) %>%
    filter(happy==1) %>%
    summarise(sum_hap = sum(happy),
              index_total = index_total,
              prop_hap = sum_hap/index_total) %>%
    distinct()
```

Then if we repeat this for each of our datasets and plot we see the following.


```{r, echo = F, message = F, warning = F}

library(gridExtra)

plots=list()

for (i in seq_along(lipsums)) {
  
  lipsum <- lipsums[[i]]
  
  plots[[i]] <- lipsum %>%
    group_by(week, student) %>%
    mutate(index_total = n()) %>%
    filter(happy==1) %>%
    summarise(sum_hap = sum(happy),
              index_total = index_total,
              prop_hap = sum_hap/index_total) %>%
    distinct() %>%
    ggplot() +
    geom_point(aes(week, prop_hap)) +
    geom_smooth(aes(week, prop_hap)) +
    scale_x_continuous(breaks= pretty_breaks()) +
      theme(axis.text.y=element_text(size=20),
            axis.text.x=element_text(size=20),
            axis.title.x=element_text(size=20, face="bold"),
            axis.title.y=element_text(size=20, face="bold"))
}

do.call("grid.arrange", c(plots, ncol = 2))

```

Why do the plots look like this?

Well, in the first, we have about the *same* number of total words each week and about the *same* number of happy words each week. If we divided the latter by the former, we get a proportion that is also stable over time. 

In the second, however, we have an *increasing* number of total words each week, but about the *same* number of happy words over time. This means that we are dividing by an ever larger number, giving ever smaller proportions. As such, the trend is decreasing over time. 

In the third, we have a *decreasing* number of total words each week, but about the *same* number of happy words over time. This means that we are dividing by an ever smaller number, giving ever larger proportions. As such, the trend is increasing over time. 

<!--chapter:end:03-week3demo.Rmd-->

# Week 4

## Natural language, complexity, and similarity

This week we will be delving more deeply into how language is used in text. In previous weeks, we have tried out two main techniques both of which rely, in different ways, on counting words. This week, we will be thinking about some more sophisticated techniques to identify and measure language use, as well as how to compare texts to each other. The article by @gomaa_survey_2013 provides an overview of different approaches. We will be covering these technical dimensions in the lecture.

The article by @urman_matter_2021 investigates a key question in contemporary communications research---what information we are exposed to online---and shows how we might compare between web search results using similarity measures. The @schoonvelde_liberals_2019 article, on the other hand, looks at the "complexity" of texts, and compares how politicians of different ideological stripes communicate.

Questions:

1. How do we measure linguistic complexity/sophistication?
2. What biases might be involved in measuring sophistication?
3. What other applications might there be for similarity measures?

**Required reading**:

- @urman_matter_2021
- @schoonvelde_liberals_2019
- @gomaa_survey_2013

**Further reading**:

- @voigt_language_2017
- @peng_quantitative_2002
- @lowe_understanding_2008
- @bail_fringe_2012
- @ziblatt_wealth_2020
- @benoit_measuring_2019

**Slides**:

- Week 4 [Slides](https://docs.google.com/presentation/d/1SpEZVfejaul9dQyeaIvQlSIyLVyRJ8w5yMzr7LoVnc4/edit?usp=sharing)

<!--chapter:end:04-week4.Rmd-->

# Week 4 Demo

## Setup
First, we'll load the packages we'll be using in this week's brief demo. 

```{r, message = F, warning = F}
library(quanteda)
library(quanteda.textstats)
library(quanteda.textplots)
library(tidytext)
library(stringdist)
library(corrplot)
library(janeaustenr)
```

## Character-based similarity

A first measure of text similarity is at the level of characters. We can look *for the last time* (I promise) at the example from the lecture and see how similarity compares. 

We'll make two sentences and create two character objects from them. These are two thoughts imagined up from our classes. 

```{r}
a <- "We are all very happy to be at a lecture at 10AM"
b <- "We are all even happier that we don’t have a lecture next week"
```

We know that the "longest common substring measure" is, according to the [stringdist](https://cran.r-project.org/web/packages/stringdist/stringdist.pdf) package documentation, "the longest string that can be obtained by pairing characters from *a* and *b* while keeping the order of characters intact."

And we can easily get different distance/similarity measures by comparing our character objects `a` and `b` as so. 

```{r}

## longest common substring distance
stringdist(a, b,
           method = "lcs")

## levenshtein distance
stringdist(a, b,
           method = "lv")

## jaro distance
stringdist(a, b,
           method = "jw", p =0)

```

## Term-based similarity

In this second example from the lecture, we're taking the opening line of *Pride and Prejudice* alongside my own versions of this same famous opening line. 

We can get the text of Jane Austen very easily thanks to the `janeaustenr` package.

```{r}
## similarity and distance example

text <- janeaustenr::prideprejudice

sentences <- text[10:11]

sentence1 <- paste(sentences[1], sentences[2], sep = " ")

sentence1
```

We're then going to specify our alternative versions of this same sentence. 

```{r}
sentence2 <- "Everyone knows that a rich man without wife will want a wife"

sentence3 <- "He's loaded so he wants to get married. Everyone knows that's what happens."
```

Finally, we're going to convert these into a document feature matrix. We're doing this with the `quanteda` package, which is a package that we'll begin using more and more over coming weeks as the analyses we're performing get gradually more technical. 

```{r, warning=F}
dfmat <- dfm(tokens(c(sentence1,
                      sentence2,
                      sentence3)),
             remove_punct = TRUE, remove = stopwords("english"))

dfmat
```

What do we see here?

Well, it's clear that `text2` and `text3` are not very similar to `text1` at all---they share few words. But we also see that `text2` does at least contain some words that are shared with `text1`, which is the original opening line of Jane Austen's *Pride and Prejudice*. 

So, how do we then measure the similarity or distance between these texts?

The first way is simply by correlating the two sets of ones and zeroes. We can do this with the `quanteda.textstats` package like so.

```{r}
## correlation
textstat_simil(dfmat, margin = "documents", method = "correlation")
```

And you'll see that this is the same as what we would get if we manipulated the data into tidy format (rows for words and columns of 1s and 0s).

```{r, warning = F}
test <- tidy(dfmat)
test <- test %>%
  cast_dfm(term, document, count)
test <- as.data.frame(test)

res <- cor(test[,2:4])
res
```

And we see that as expected `text2` is more highly correlated with `text1` than is `text3`. 

```{r}
corrplot(res, type = "upper", order = "hclust", 
         tl.col = "black", tl.srt = 45)
```
As for Euclidean distances, we can again use `quanteda` as so.

```{r}
textstat_dist(dfmat, margin = "documents", method = "euclidean")
```

Or we could define our own function just so we see what's going on behind the scenes.

```{r}
# function for Euclidean distance
euclidean <- function(a,b) sqrt(sum((a - b)^2))
# estimating the distance
euclidean(test$text1, test$text2)
euclidean(test$text1, test$text3)
euclidean(test$text2, test$text3)

```

For Manhattan distance, we could use `quanteda` again.

```{r}
textstat_dist(dfmat, margin = "documents", method = "manhattan")
```

Or we could again define our own function.

```{r}
## manhattan
manhattan <- function(a, b){
  dist <- abs(a - b)
  dist <- sum(dist)
  return(dist)
}

manhattan(test$text1, test$text2)
manhattan(test$text1, test$text3)
manhattan(test$text2, test$text3)
```

And for the cosine similarity, `quanteda` again makes this straightforward.

```{r}
textstat_simil(dfmat, margin = "documents", method = "cosine")
```

But to make clear what's going on here, we could again write our own function. 

```{r}
## cosine
cos.sim <- function(a, b) 
{
  return(sum(a*b)/sqrt(sum(a^2)*sum(b^2)) )
}  

cos.sim(test$text1, test$text2)
cos.sim(test$text1, test$text3)
cos.sim(test$text2, test$text3)
```

## Complexity

Note: this section borrows notation from the materials for the [`texstat_readability()` function](https://quanteda.io/reference/textstat_readability.html).

We also talked about different document-level measures of text characteristics. One of these is the "complexity" or readability of a text. One of the most frequently used is Flesch's Reading Ease Score (Flesch 1948).

This is computed as:


\item{\code{"Flesch"}:}{Flesch's Reading Ease Score (Flesch 1948).
\deqn{206.835 - (1.015 \times ASL) - (84.6 \times \frac{n_{sy}}{n_{w}})}{
  206.835 - (1.015 * ASL) - (84.6 * (Nsy / Nw))}}
  
We can estimate a readability score for our respective sentences as such. The Flesch score from 1948 is the default. 

```{r}
textstat_readability(sentence1)
textstat_readability(sentence2)
textstat_readability(sentence3)
```

What do we see here? The original Austen opening line is marked lower in readability than our more colloquial alternatives. 

But there are other alternatives measures we might use. You can check these out by clicking through the links of the function `textstat_readability()`. Below I display a few of these. 

One such is the McLaughlin (1969) "Simple Measure of Gobbledygook, which is based on the recurrence of words with 3 syllables or more and is calculated as:


\item{\code{"SMOG"}:}{Simple Measure of Gobbledygook (SMOG) (McLaughlin 1969). \deqn{ 1.043
   \times \sqrt{n_{wsy>=3}} \times \frac{30}{n_{st}} + 3.1291}{1.043 * sqrt(Nwmin3sy
   * 30 / Nst) + 3.1291}

where \eqn{n_{wsy>=3}} = Nwmin3sy = the number of words with 3 syllables or more.
This measure is regression equation D in McLaughlin's original paper.}

We can calculate this for our three sentences as below.

```{r}
textstat_readability(sentence1, measure = "SMOG")
textstat_readability(sentence2, measure = "SMOG")
textstat_readability(sentence3, measure = "SMOG")
```

Here, again, we see that the original Austen sentence has a higher level of complexity (or gobbledygook!).

<!--chapter:end:04-week4demo.Rmd-->

# Week 5

## Scaling techniques

Here we begin thinking about more automated techniques for analyzing texts. And there are a bunch of additional considerations we now need to bring to mind. These considerations have sparked significant debates... and the matter is by no means settled.

So what is at stake here? In weeks to come, we will be studying various techniques to 'classify,' 'position' or 'score' texts based on their features. The success of these techniques depends on their suitability to the question at hand but also on higher-level questions about meaning. In short, we have to ask ourselves: is there a way we can access the underlying processes governing the generation of text? Is meaning governed by a set of structural processes? And can we derive 'objective' measures of the contents of any given text?

The readings by @grimmer_machine_2021, @denny_text_2018, and @goldenstein_analyzing_2019 (as well as the response and replies by @nelson_measure_2019 and @goldenstein_quest_2019) will be required reading for Flexible Learning Week. 

- @grimmer_machine_2021
- @grimmer_text_2013
- @denny_text_2018

- @goldenstein_analyzing_2019
  - @nelson_measure_2019
  - @goldenstein_quest_2019
  
The substantive focus of this week are a set of readings that all employ different types of "scaling" or "low-dimensional document embedding" techniques. The article by @lowe_understanding_2008 provides a technical overview of the "wordfish" algorithm and its uses in a political science contexts. The article by @kluver_measuring_2009 also uses "wordfish" in a different way---to measure the "influence" of interest groups. The response to this article by @bunea_quantitative_2015 and subsequent reply by @kluver_promises_2015 helps illuminate some of the debates around these questions. 

Questions:

1. What assumptions underlie scaling models of text?; What is latent in a text and who decides?
2. What might scaling be useful for outside of estimating ideological position from text?

**Required reading**:

- @slapin_scaling_2008
- @kaneko_estimating_2021
- @kluver_measuring_2009
  - @bunea_quantitative_2015
  - @kluver_promises_2015

**Further reading**:

- @benoit_crowd-sourced_2016
- @laver_extracting_2003
- @lowe_understanding_2008
- @schwemmer_methodological_2020
- @kaneko_estimating_2021

**Slides**:

- Week 5 [Slides](https://docs.google.com/presentation/d/1oG0JBlnRaOZ1kJIkzM13kmEpM1R29mFE_50QAZnv0bQ/edit?usp=sharing)

<!--chapter:end:05-week5.Rmd-->

# Week 5 Demo

## Setup
First, we'll load the packages we'll be using in this week's brief demo. 

```{r, message = F, warning = F}
devtools::install_github("conjugateprior/austin")
library(austin)
library(quanteda)
library(quanteda.textstats)
```

## Wordscores

We can inspect the function for the wordscores model by @laver_extracting_2003 in the following way:

```{r}
classic.wordscores
```

We can then take some example data included in the `austin` package.

```{r}
data(lbg)
ref <- getdocs(lbg, 1:5)
```

And here our reference documents are all those documents marked with an "R" for reference; i.e., columns one to five.

```{r}
ref
```

This matrix is simply a series of words (here: letters) and reference texts with word counts for each of them. 

We can then look at the wordscores for each of the words, calculated using the reference dimensions for each of the reference documents.

```{r}

ws <- classic.wordscores(ref, scores=seq(-1.5,1.5,by=0.75))
ws
```

And here we see the thetas contained in this wordscores object, i.e., the reference dimensions for each of the reference documents and the pis, i.e., the estimated wordscores for each word. 

We can now use these to score the so-called "virgin" texts as follows. 

```{r}

#get "virgin" documents
vir <- getdocs(lbg, 'V1')
vir

# predict textscores for the virgin documents
predict(ws, newdata=vir)
```

## Wordfish


If we wish, we can inspect the function for the wordscores model by @slapin_scaling_2008 in the following way. This is a much more complex algorithm, which is not printed here, but you can inspect on your own devices. 


```{r, eval=F}
wordfish
```

We can then simulate some data, formatted appropriately for wordfiash estimation in the following way:

```{r}
dd <- sim.wordfish()

dd
```

Here we can see the document and word-level FEs, as well as the specified range of the thetas to be estimates. 

Then estimating the document positions is simply a matter of implementing this algorithm.

```{r}
wf <- wordfish(dd$Y)
summary(wf)
```

## Using `quanteda`

We can also use `quanteda` to implement the same scaling techniques, as demonstrated in Exercise 4. 


<!--chapter:end:05-week5demo.Rmd-->

# Week 6

## Unsupervised learning (topic models)

This week builds upon past the scaling techniques we explored in Week 5 and instead turns to another form of unsupervised approach---topic modelling.

The substantive articles by @nelson_computational_2020 and @alrababah_authoritarian_2020 provide, in turn, illuminating insights using topic models to categorize the thematic content of text information.

The article by @ying_topics_2021 provides a valuable overview and accompaniment to the earlier work of @denny_text_2018 when thinking about how we validate our findings and test the robustness of any inferences we make from these models.

Questions:

1. What assumptions underlie topic modelling approaches?
2. Can we develop structural models of text?
3. Is topic modelling a discovery or measurement strategy?
4. How do we validate any model?

**Required reading**:

- @nelson_computational_2020
- @alrababah_authoritarian_2020
- @ying_topics_2021

**Further reading**:

- @chang_reading_2009
- @grimmer_general_2011
- @denny_text_2018
- @smith_automatic_2021
- @boyd_characterizing_2018

**Slides**:

- Week 6 [Slides](https://docs.google.com/presentation/d/1SeL25sA0a7OoJhPOy5lvYuvqOZAUJBkh17VRTG5_VAw/edit?usp=sharing)

<!--chapter:end:06-week6.Rmd-->

# Week 6 Demo

## Setup
First, we'll load the packages we'll be using in this week's brief demo. 

```{r, message = F, warning = F}
library(topicmodels)
library(dplyr)
library(tidytext)
library(ggplot2)
library(ggthemes)
```

Estimating a topic model requires us first to have our data in the form of a document-term-matrix. This is another term for what we have referred to in previous weeks as a document-feature-matrix.

We can take some example data from the `topicmodels` package. This text is from news releases by the Associated Press. It consists of around 2,200 articles (documents) and over 10,000 terms (words).

```{r}

data("AssociatedPress", 
     package = "topicmodels")

```

To estimate the topic model we need only to specify the document-term-matrix we are using, and the number (`k`) of topics that we are estimating. To speed up estimation, we are here only estimating it on 100 articles.

```{r}

lda_output <- LDA(AssociatedPress[1:100,], k = 10)

```

We can then inspect the contents of each topic as follows.

```{r}
terms(lda_output, 10)
```

We can then use the `tidy()` function from `tidytext` to gather the relevant parameters we've estimated. To get the $\beta$ per-topic-per-word probabilities (i.e., the probability that the given term belongs to a given topic) we can do the following.

```{r}

lda_beta <- tidy(lda_output, matrix = "beta")

lda_beta %>%
  arrange(-beta)

```

And to get the $\gamma$ per-document-per-topic probabilities (i.e., the probability that a given document (here: article) belongs to a particular topic) we do the following.


```{r}

lda_gamma <- tidy(lda_output, matrix = "gamma")

lda_gamma %>%
  arrange(-gamma)


```

And we can easily plot our $\beta$ estimates as follows.

```{r}

lda_beta %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta) %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free", ncol = 4) +
  scale_y_reordered() +
  theme_tufte(base_family = "Helvetica")

```

Which shows us the words associated with each topic, and the size of the associated $\beta$ coefficient. 

<!--chapter:end:06-week6demo.Rmd-->

# Week 7

## Unsupervised learning (word embedding)

This week we will be discussing a second form of "unsupervised" learning---word embeddings. If previous weeks allowed us to characterize the complexity of text, or cluster text by potential topical focus, word embeddings permit us a more expansive form of measurement. In essence, we are producing here a matrix representation of an entire corpus. 

The reading by @rodriguez_word_2022 provides an effective overview of the technical dimensions of this technique. The articles by @garg_word_2018 and @kozlowski_geometry_2019 are two substantive articles that use word embeddings to provide insights into prejudice and bias as manifested in language over time.

**Required reading**:

- @garg_word_2018
- @kozlowski_geometry_2019
- @rodriguez_word_2022

**Further reading**:

- @rodriguez_word_2021
- @osnabrugge_playing_2021
- @rheault_word_2020
- @jurafsky_speech_2021 [ch.6]: [https://web.stanford.edu/~jurafsky/slp3/](https://web.stanford.edu/~jurafsky/slp3/)]

**Slides**:

- Week 7 [Slides](https://docs.google.com/presentation/d/1sS3xk0NqpaGLuvrrHVgYpW-yATxUwPuEBt-Ke_sK-Eo/edit?usp=sharing)

<!--chapter:end:07-week7.Rmd-->

# Week 7 Demo

## Setup

First, we'll load the packages we'll be using in this week's brief demo. here we are pre-loading an already-estimated PMI matrix and results from the singular value decomposition approach.

```{r, warning = F, message = F}
library(Matrix) #for handling matrices
library(tidyverse)
library(irlba) # for SVD
library(umap) # for dimensionality reduction

load("data/wordembed/pmi_svd.RData")
load("data/wordembed/pmi_matrix.RData")
```


How does it work?

- Various approaches, including:
  - 1. SVD
  - 2. Neural network-based techniques like GloVe and Word2Vec
  
In both approaches, we are:

1. Defining a context window (see figure below)
2. Looking at probabilities of word appearing near another words

![Context window](data/wordembed/window.png)

The implementation of this technique using the singular value decomposition approach requires the following data structure:

  - Word pair matrix with PMI (Pairwise mutual information)
  - where   PMI = log(P(x,y)/P(x)P(y))
  - and   P(x,y)   is the probability of word x appearing within a six-word window of word y
  - and   P(x)   is the probability of word x appearing in the whole corpus
  - and   P(y)   is the probability of word y appearing in the whole corpus


```{r ,echo=F}

head(pmi_matrix[1:6, 1:6])

```

And the resulting matrix object will take the following format:

```{r ,echo=F}

glimpse(pmi_matrix)

```

We will then use the "Singular Value Decomposition" (SVD) techique. This is another multidimensional scaling technique, where the first axis of the resulting coordinates captures the most variance, the second the second-most etc...

And to do this, we simply need to do the following.

```{r, eval = F}

pmi_svd <- irlba(pmi_matrix, 256, maxit = 500)

```

After which we can collect our vectors for each word and inspect them. 

```{r}
word_vectors <- pmi_svd$u
rownames(word_vectors) <- rownames(pmi_matrix)
dim(word_vectors)

head(word_vectors[1:5, 1:5])
```

## Using `GloVe` or `word2vec`

The neural network approach is considerably more involved, and the figure below gives an overview picture of the differing algorithmic approaches we might use.

<center>
<img src="data/wordembed/skip_gram_mikolov.png" >
</center>

<!--chapter:end:07-week7demo.Rmd-->

# Week 8

## Sampling text information

This week we'll be thinking about how best to sample text information, thinking about the different biases that might inhere in the data-generating process, as well as the representativeness and generalizability of any text corpus we construct. 

The reading by @barbera_understanding_2015 invesitgates the representativeness of Twitter data, and should give us pause when thinking about using digital trace data as a general barometer of public opinion. 

The reading by @michalopoulos_folklore_2021 takes an entirely different tack, but illustrates how we can think systematically about text information more broadly representative of societies in general. 

**Required reading**:

- @barbera_understanding_2015
- @michalopoulos_folklore_2021
- @krippendorff_content_2004 [chs. 5 and 6]

**Further reading**:

- @martins_rise_2020
- @baumard_cultural_2022

**Slides**:

- Week 8 [Slides](https://docs.google.com/presentation/d/1gAuaqirCG-Db1Q77tudp_Z4QIh0esvHBjRDiFEm8yWk/edit?usp=sharing)

<!--chapter:end:08-week8.Rmd-->

# Week 9

## Supervised learning

**Required reading**:

- @hopkins_method_2010
- @king_how_2017-1
- @siegel_trumping_2021
- @yu_classifying_2008
- @manning_introduction_2007 [chs. 13,14, and 15]: [https://nlp.stanford.edu/IR-book/information-retrieval-book.html](https://nlp.stanford.edu/IR-book/information-retrieval-book.html)]

**Further reading**:

- @denny_text_2018
- @king_computer-assisted_2017
- 

<!--chapter:end:09-week9.Rmd-->

# Week 10

## Validation

This week we'll be thinking about how to validate techniques we've used in the preceding weeks. Validation is a necessary and important part of any text analysis technique. 

Often we speak of validation in the context of machine labelling of large text data. But validation need not---and should not---be restricted to automated classification tasks. The articles by @ying_topics_2021 and @rodriguez_models_2021 describe ways to approach validation in unsupervised contexts. Finally, the article by @peterson_classification_2018 shows how validation and accuracy might provide a measure of substantive significance. 

**Required reading**:

- @ying_topics_2021
- @rodriguez_models_2021
- @peterson_classification_2018
- @manning_introduction_2007 [ch.2: [https://nlp.stanford.edu/IR-book/information-retrieval-book.html](https://nlp.stanford.edu/IR-book/information-retrieval-book.html)]

**Further reading**:

- @krippendorff_reliability_2004
- @denny_text_2018
- @grimmer_text_2013-1
- @barbera_automated_2021
- @schiller_stance_2021

**Slides**:

- Week 10 [Slides](https://docs.google.com/presentation/d/1Ib3_7MxS1WKs9Th3hiQ4HmvOsrCBpgAf3EcCWDC5GDc/edit?usp=sharing)

<!--chapter:end:10-week10.Rmd-->

# Exercise 1: Word frequency analysis

## Introduction

In this tutorial, you will learn how to summarise, aggregate, and analyze text in R:

* How to tokenize and filter text
* How to clean and preprocess text
* How to visualize results with ggplot
* How to perform automated gender assignment from name data (and think about possible biases these methods may enclose)

## Setup 

To practice these skills, we will use a dataset that I have already collected from the Edinburgh Fringe Festival website. 

You can try this out yourself too: to obtain these data, you must first obtain an API key. Instructions on how to do this are available at the [Edinburgh Fringe API page](https://api.edinburghfestivalcity.com/documentation/fringe_approval):

![Alt Text](data/wordfreq/fringeapi.png)

##  Load data and packages 

Before proceeding, we'll load the remaining packages we will need for this tutorial.

```{r, message=F}
library(tidyverse) # loads dplyr, ggplot2, and others
library(tidytext) # includes set of functions useful for manipulating text
library(ggthemes) # includes a set of themes to make your visualizations look nice!
library(readr) # more informative and easy way to import data
library(babynames) #for gender predictions
```

For this tutorial, we will be using data that I have pre-cleaned and provided in .csv format. The data come from the Edinburgh Book Festival API, and provide data for every event that has taken place at the Edinburgh Book Festival, which runs every year in the month of August, for nine years: 2012-2020. There are many questions we might ask of these data. In this tutorial, we will investigate the contents of each event, and the speakers at each event, to determine if there are any trends in gender representation over time.

The first task, then, is to read in these data. We can do this with the `read_csv()` function.

The `read_csv()` function takes the .csv file and loads it into the working environment as a data frame object called "edbfdata." We can call this object anything though. Try changing the name of the object before the <- arrow. Note that R does not allow names with spaces in, however. It is also not a good idea to name the object something beginning with numbers, as this means you have to call the object within ` marks.

```{r}
edbfdata <- read_csv("data/wordfreq/edbookfestall.csv")
```

If you're working on this document from your own computer ("locally") you can download the Edinburgh Fringe data in the following way:

```{r, eval = F}
edbfdata <- read_csv("https://github.com/cjbarrie/CTA-ED/blob/main/data/wordfreq/edbookfestall.csv"
                   
```


## Inspect and filter data 

Our next job is to cut down this dataset to size, including only those columns that we need. But first we can inspect it to see what the existing column names are, and how each variable is coded. To do this we can first call:

```{r}
colnames(edbfdata)
```

And then: 

```{r}
glimpse(edbfdata)
```

We can see that the description of each event is included in a column named "description" and the year of that event as "year." So for now we'll just keep these two. Remember: we're interested in this tutorial firstly in the representation of gender and feminism in forms of cultural production given a platform at the Edinburgh International Book Festival. Given this, we are first and foremost interested in the reported content of each artist's event.

We use pipe `%>%` functions in the tidyverse package to quickly and efficiently select the columns we want from the edbfdata data.frame object. We pass this data to a new data.frame object, which we call "evdes."

```{r}
# get simplified dataset with only event contents and year
evdes <- edbfdata %>%
  select(description, year)

head(evdes)
```

And let's take a quick look at how many events there were over time at the festival. To do this, we first calculate the number of individual events (row observations) by year (column variable).

```{r, echo=T, warning=F}
evtsperyr <- evdes %>%
  mutate(obs=1) %>%
  group_by(year) %>%
  summarise(sum_events = sum(obs))
```

And then we can plot this using ggplot!

```{r, echo=T, warning=F}
ggplot(evtsperyr) +
  geom_line(aes(year, sum_events)) +
  theme_tufte(base_family = "Helvetica") + 
  scale_y_continuous(expand = c(0, 0), limits = c(0, NA))
```

Perhaps unsurprisingly, in the context of the pandemic, the number of recorded bookings for the 2020 Festival is drastically reduced. 

## Tidy the text 

Given that these data were obtained from an API that outputs data originally in HTML format, some of the text still contains some HTML and PHP encodings for e.g. bold font or paragraphs. We'll need to get rid of this, as well as other punctuation before analyzing these data.

The below set of commands takes the event descriptions, extracts individual words, and counts the number of times they appear in each of the years covered by our book festival data. 

```{r}
#get year and word for every word and date pair in the dataset
tidy_des <- evdes %>% 
  mutate(desc = tolower(description)) %>%
  unnest_tokens(word, desc) %>%
  filter(str_detect(word, "[a-z]"))

```

## Back to the Fringe

We see that the resulting dataset is large (~446k rows). This is because the above commands have first taken the events text, and has "mutated" it into a set of lower case character string. With the "unnest_tokens" function it has then taken each individual string and create a new column called "word" that contains each individual word contained in the event description texts.

Some terminology is also appropriate here. When we tidy our text into this format, we often refer to these data structures as consisting of "documents" and "terms." This is because by "tokenizing" our text with the "unnest_tokens" functions we are generating a dataset with one term per row. 

Here, our "documents" are the collection of descriptions for all events in each year at the Edinburgh Book Festival. The way in which we sort our text into "documents" depends on the choice of the individual researcher. 

Instead of by year, we might have wanted to sort our text into "genre." Here, we have two genres: "Literature" and "Children." Had we done so, we would then have only two "documents," which contained all of the words included in the event descriptions for each genre. 

Alternatively, we might be interested in the contributions of individual authors over time. Were this the case, we could have sorted our text into documents by author. In this case, each "document" would represent all the words included in event descriptions for events by the given author (many of whom do have multiple appearances over time or in the same festival for a given year).
 
We can yet tidy this further, though. First we'll remove all stop words and then we'll remove all apostrophes:

```{r}
tidy_des <- tidy_des %>%
    filter(!word %in% stop_words$word)
```

We see that the number of rows in our dataset reduces by about half to ~223k rows. This is natural since a large proportion of any string will contain many so-called "stop words". We can see what these stop words are by typing:

```{r}
stop_words
```

This is a lexicon (list of words) included in the <tt>tidytext</tt> package produced by Julia Silge and David Robinson (see [here](https://cran.r-project.org/web/packages/tidytext/vignettes/tidytext.html)). We see it contains over 1000 such words. We remove them here because they are not very informative if we are interested in the substantive content of text (rather than, say, its grammatical content).

Now let's have a look at the most common words in these data:

```{r}
tidy_des %>%
  count(word, sort = TRUE)
```

We can see that one of the most common words is "rsquo," which is an HTML encoding for an apostrophe. Clearly we need to clean the data a bit more. This is a common issue in large-n text analysis and is a key step if you want to conduct reliably robust forms of text analysis. We'll have another go using the the filter command, specifying that we only keep the words that are not included in the string of words `r c("rsquo", "em", "ndash", "nbsp", "lsquo") `.

```{r}
remove_reg <- c("&amp;","&lt;","&gt;","<p>", "</p>","&rsquo", "&lsquo;",  "&#39;", "<strong>", "</strong>", "rsquo", "em", "ndash", "nbsp", "lsquo", "strong")
                  
tidy_des <- tidy_des %>%
  filter(!word %in% remove_reg)

```

```{r}
tidy_des %>%
  count(word, sort = TRUE)
```

That's more like it! The words that feature most seem to make sense now (and are actual words rather than random HTML and UTF-8 encodings). 

Let's now collect these words into a data.frame object, which we'll call edbf_term_counts:

```{r}
edbf_term_counts <- tidy_des %>% 
  group_by(year) %>%
  count(word, sort = TRUE)
```

```{r}
head(edbf_term_counts)
```

For each year, we see that "book" is the most common word... perhaps no surprises here. But this is some evidence that we're properly pre-processing and cleaning the data. Cleaning text data is an important element of preparing for any text analysis. It is often a process of trial and error as not all text data looks alike, may come from e.g. webpages with HTML encoding, unrecognized fonts or unicode, and all of these have the potential to cause issues! But finding these errors is also a chance to get to know your data...

## Analyze keywords 

Okay, now we have our list of words, and the number of times they appear, we can tag those words we think might be related to issues of gender inequality and sexism. You may decide that this list is imprecise or inexhaustive. If so, then feel free to change the terms we are including after the `grepl()` function. 

```{r}
edbf_term_counts$womword <- as.integer(grepl("women|feminist|feminism|gender|harassment|sexism|sexist", 
                                            x = edbf_term_counts$word))
```

```{r}
head(edbf_term_counts)
```

## Compute aggregate statistics 

Now that we have tagged individual words relating to gender inequality and feminism, we can sum up the number of times these words appear each year and then denominate them by the total number of words in the event descriptions.

The intuition here is that any increase or decrease in the percentage of words relating to these issues is capturing a substantive change in the representation of issues related to sex and gender.

What do we think of this measure? Is this an adequate measure of representation for such issues in the cultural sphere?

Are the keywords we used precise enough? If not, what would you change?

```{r}
#get counts by year and word
edbf_counts <- edbf_term_counts %>%
  group_by(year) %>%
  mutate(year_total = sum(n)) %>%
  filter(womword==1) %>%
  summarise(sum_wom = sum(n),
            year_total= min(year_total))
```

```{r}
head(edbf_counts)
```

## Plot time trends 

So what do we see? Let's take the count of words relating to gender in this dataset, and denominate them by the total number of words in these data per year. 

```{r, warning=F}
ggplot(edbf_counts, aes(year, sum_wom / year_total, group=1)) +
  geom_line() +
  xlab("Year") +
  ylab("% gender-related words") +
  scale_y_continuous(labels = scales::percent_format(),
                     expand = c(0, 0), limits = c(0, NA)) +
  theme_tufte(base_family = "Helvetica") 
```

We can add visual guides to draw attention to apparent changes in these data. Here, we might wish to signal the year of the #MeToo movement in 2017.

```{r, warning=F}
ggplot(edbf_counts, aes(year, sum_wom / year_total, group=1)) +
  geom_line() +
  geom_vline(xintercept = 2017, col="red") +
  xlab("Year") +
  ylab("% gender-related words") +
  scale_y_continuous(labels = scales::percent_format(),
                     expand = c(0, 0), limits = c(0, NA)) +
  theme_tufte(base_family = "Helvetica")
```

And we could label why we are highlighting the year of 2017 by including a text label along the vertical line. 

```{r, warning=F}
ggplot(edbf_counts, aes(year, sum_wom / year_total, group=1)) +
  geom_line() +
  geom_vline(xintercept = 2017, col="red") +
  geom_text(aes(x=2017.1, label="#metoo year", y=.0015), 
            colour="black", angle=90, text=element_text(size=8)) +
  xlab("Year") +
  ylab("% gender-related words") +
  scale_y_continuous(labels = scales::percent_format(),
                     expand = c(0, 0), limits = c(0, NA)) +
  theme_tufte(base_family = "Helvetica")
```

## Bonus: gender prediction

We might decide that this measure is inadequate or too expansive to answer the question at hand. Another way of measuring representation in cultural production is to measure the gender of the authors who spoke at these events.

Of course, this would take quite some time if we were to individually code each of the approximately 6000 events included in this dataset.

But there do exist alternative techniques for imputing gender based on the name of an individual. 

We first create a new data.frame object, selecting just the columns for artist name and year. Then we generate a new column containing just the artist's (author's) first name:

```{r}
# get columns for artist name and year, omitting NAs
gendes <- edbfdata %>%
  select(artist, year) %>%
  na.omit()

# generate new column with just the artist's (author's) first name
gendes$name <- sub(" .*", "", gendes$artist)
```

A set of packages called <tt>gender</tt> and <tt>genderdata</tt> used to make the process of predicting gender based on a given individual's name pretty straightforward. This technique worked with reference to  U.S. Social Security Administration baby name data. 

Given that the most common gender associated with a given name changes over time, the function also allows us to specify the range of years for the cohort in question whose gender we are inferring. Given that we don't know how wide the cohort of artists is that we have here, we could specify a broad range of 1920-2000.

```{r, eval =F}
genpred <- gender(gendes$name,
       years = c(1920, 2000))
```

Unfortunately, this package no longer works with newer versions of R; fortunately, I have recreated it using the original "babynames" data, which comes bundled in the <tt>babynames</tt> package. 

You don't necessarily have to follow each step of how I have done this---I include this information for the sake of completeness.

The <tt>babynames</tt> package. contains, for each year, the number of children born with a given name, as well as their sex. With this information, we can then calculate the total number of individuals with a given name born for each sex in a given year. 

Given we also have the total number of babies born in total cross these records, we can denominate (divide) the sums for each name by the total number of births for each sex in each year. We can take this proportion as representing the probability that a given individual in our Edinburgh Fringe dataset is male or female. 

More information on the <tt>babynames</tt> package can be found [here](https://www.ssa.gov/oact/babynames/limits.html). 

We first load the babynames package into the R environment as a data.frame object. Because the data.frame "babynames" is contained in the <tt>babynames</tt> package we can just call it as an object and store it with `r babynames <- babynames`. 

This dataset contains names for all years over a period 1800--2019. The variable "n" represents the number of babies born with the given name and sex in that year, and the "prop" represents, according to the package materials accessible [here](https://cran.r-project.org/web/packages/babynames/babynames.pdf), "n divided  by  total  number  of applicants in that year, which means proportions are of people of that gender with that name born in that year."

```{r}

babynames <- babynames
head(babynames)
```

We then calculate the total number of babies of female and male sex born in each year. Then we merge these to get a combined dataset of male and female baby names by year. We then merge this information back into the original babynames data.frame object.

```{r}

totals_female <- babynames %>%
  filter(sex=="F") %>%
  group_by(year) %>%
  summarise(total_female = sum(n))

totals_male <- babynames %>%
  filter(sex=="M") %>%
  group_by(year) %>%
  summarise(total_male = sum(n))

totals <- merge(totals_female, totals_male)

totsm <- merge(babynames, totals, by = "year")
head(totsm)

```

We can then calculate, for all babies born on or after 1920, the number of babies born with that name and sex. With this information, we can then get the proportion of all babies with a given name that were of a particular sex. For example, if 92% of babies born with the name "Mary" were female, this would give us a .92 probability that an individual with the name "Mary" is female. 

We do this for every name in the dataset, excluding names for which the proportion is equal to .5; i.e., for names that we cannot adjudicate between whether they are more or less likely male or female.

```{r}

totprops <- totsm %>%
  filter(year >= 1920) %>%
  group_by(name, year) %>%
  mutate(sumname = sum(n),
         prop = ifelse(sumname==n, 1,
                       n/sumname)) %>%
  filter(prop!=.5) %>%
  group_by(name) %>%
  slice(which.max(prop)) %>%
  summarise(prop = max(prop),
            totaln = sum(n),
            name = max(name),
            sex = unique(sex))

head(totprops)

```

Once we have our proportions for all names, we can then merge these back with the names of our artists from the Edinburgh Fringe Book Festival. We can then easily plot the proportion of artists at the Festival who are male versus female in each year of the Festival. 

```{r}

ednameprops <- merge(totprops, gendes, by = "name")

ggplot(ednameprops, aes(x=year, fill = factor(sex))) +
  geom_bar(position = "fill") +
  xlab("Year") +
  ylab("% women authors") +
  labs(fill="") +
  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
  theme_tufte(base_family = "Helvetica") +
  geom_abline(slope=0, intercept=0.5,  col = "black",lty=2)

```

What can we conclude form this graph?

Note that when we merged the proportions from th "babynames" data with our Edinburgh Fringe data we lost some observations. This is because some names in the Edinburgh Fringe data had no match in the "babynames" data. Let's look at the names that had no match:

```{r}
names1 <- ednameprops$name
names2 <- gendes$name
diffs <- setdiff(names2, names1)
diffs
```

Do we notice anything about these names? What does this tell us about the potential biases of using such sources as US baby names data as a foundation for gender prediction? What are alternative ways we might go about this task?

## Exercises

1. Filter the books by genre (selecting e.g., "Literature" or "Children") and plot frequency of women-related words over time.
2. Choose another set of terms by which to filter (e.g., race-related words) and plot their frequency over time.

## References 

<!--chapter:end:11-word-freq.Rmd-->

# Exercise 2: Dictionary-based methods

## Introduction

In this tutorial, you will learn how to:

* Use dictionary-based techniques to analyze text
* Use common sentiment dictionaries
* Create your own "dictionary"
* Use the Lexicoder sentiment dictionary from @young_affective_2012

## Setup 

The hands-on exercise for this week uses dictionary-based methods for filtering and scoring words. Dictionary-based methods use pre-generated lexicons, which are no more than list of words with associated scores or variables measuring the valence of a particular word. In this sense, the exercise is not unlike our analysis of Edinburgh Book Festival event descriptions. Here, we were filtering descriptions based on the presence or absence of a word related to women or gender. We can understand this approach as a particularly simple type of "dictionary-based" method. Here, our "dictionary" or "lexicon" contained just a few words related to gender. 

##  Load data and packages 

Before proceeding, we'll load the remaining packages we will need for this tutorial.

```{r, echo=F}
library(kableExtra)
```

```{r, message=F}
library(academictwitteR) # for fetching Twitter data
library(tidyverse) # loads dplyr, ggplot2, and others
library(readr) # more informative and easy way to import data
library(stringr) # to handle text elements
library(tidytext) # includes set of functions useful for manipulating text
library(quanteda) # includes functions to implement Lexicoder
library(textdata)

```

In this exercise we'll be using another new dataset. The data were collected from the Twitter accounts of the top eight newspapers in the UK by circulation. You can see the names of the newspapers in the code below:

```{r, eval=F}
newspapers = c("TheSun", "DailyMailUK", "MetroUK", "DailyMirror", 
               "EveningStandard", "thetimes", "Telegraph", "guardian")

tweets <-
  get_all_tweets(
    users = newspapers,
    start_tweets = "2020-01-01T00:00:00Z",
    end_tweets = "2020-05-01T00:00:00Z",
    data_path = "data/sentanalysis/",
    n = Inf,
  )

tweets <- 
  bind_tweets(data_path = "data/sentanalysis/", output_format = "tidy")

saveRDS(tweets, "data/sentanalysis/newstweets.rds")

```


![](data/sentanalysis/guardiancorona.png){width=100%}

For details of how to access Twitter data with `academictwitteR`, check out details of the package [here](https://cran.r-project.org/web/packages/academictwitteR/index.html).

We can download the final dataset with:

```{r}
tweets <- readRDS("data/sentanalysis/newstweets.rds")
```

If you're working on this document from your own computer ("locally") you can download the tweets data in the following way:

```{r, eval = F}
tweets  <- readRDS(gzcon(url("https://github.com/cjbarrie/CTA-ED/blob/main/data/sentanalysis/newstweets.rds?raw=true")))

```

## Inspect and filter data 

Let's have a look at the data:

```{r}
head(tweets)
colnames(tweets)
```

Each row here is a tweets produced by one of the news outlets detailed above over a five month period, January--May 2020. Note also that each tweets has a particular date. We can therefore use these to look at any over time changes.

We won't need all of these variables so let's just keep those that are of interest to us:

```{r}

tweets <- tweets %>%
  select(user_username, text, created_at, user_name,
         retweet_count, like_count, quote_count) %>%
  rename(username = user_username,
         newspaper = user_name,
         tweet = text)

```

```{r, echo = F}
tweets %>% 
  arrange(created_at) %>%
  tail(5) %>%
  kbl() %>%
  kable_styling(c("striped", "hover", "condensed", "responsive"))
```

We manipulate the data into tidy format again, unnesting each token (here: words) from the tweet text. 

```{r}
tidy_tweets <- tweets %>% 
  mutate(desc = tolower(tweet)) %>%
  unnest_tokens(word, desc) %>%
  filter(str_detect(word, "[a-z]"))
```

We'll then tidy this further, as in the previous example, by removing stop words:

```{r}
tidy_tweets <- tidy_tweets %>%
    filter(!word %in% stop_words$word)
```

## Get sentiment dictionaries

Several sentiment dictionaries come bundled with the <tt>tidytext</tt> package. These are:

* `AFINN` from [Finn Årup Nielsen](http://www2.imm.dtu.dk/pubdb/views/publication_details.php?id=6010),
* `bing` from [Bing Liu and collaborators](https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html), and
* `nrc` from [Saif Mohammad and Peter Turney](http://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm)

We can have a look at some of these to see how the relevant dictionaries are stored. 

```{r,}
get_sentiments("afinn")
```

```{r}
get_sentiments("bing")
```

```{r}
get_sentiments("nrc")
```

What do we see here. First, the `AFINN` lexicon gives words a score from -5 to +5, where more negative scores indicate more negative sentiment and more positive scores indicate more positive sentiment.  The `nrc` lexicon opts for a binary classification: positive, negative, anger, anticipation, disgust, fear, joy, sadness, surprise, and trust, with each word given a score of 1/0 for each of these sentiments. In other words, for the `nrc` lexicon, words appear multiple times if they enclose more than one such emotion (see, e.g., "abandon" above). The `bing` lexicon is most minimal, classifying words simply into binary "positive" or "negative" categories. 

Let's see how we might filter the texts by selecting a dictionary, or subset of a dictionary, and using `inner_join()` to then filter out tweet data. We might, for example, be interested in fear words. Maybe, we might hypothesize, there is a uptick of fear toward the beginning of the coronavirus outbreak. First, let's have a look at the words in our tweet data that the `nrc` lexicon codes as fear-related words.

```{r}

nrc_fear <- get_sentiments("nrc") %>% 
  filter(sentiment == "fear")

tidy_tweets %>%
  inner_join(nrc_fear) %>%
  count(word, sort = TRUE)

```

We have a total of 1,174 words with some fear valence in our tweet data according to the `nrc` classification. Several seem reasonable (e.g., "death," "pandemic"); others seems less so (e.g., "mum," "fight").

## Sentiment trends over time

Do we see any time trends? First let's make sure the data are properly arranged in ascending order by date. We'll then add column, which we'll call "order," the use of which will become clear when we do the sentiment analysis.

```{r}
#gen data variable, order and format date
tidy_tweets$date <- as.Date(tidy_tweets$created_at)

tidy_tweets <- tidy_tweets %>%
  arrange(date)

tidy_tweets$order <- 1:nrow(tidy_tweets)

```

Remember that the structure of our tweet data is in a one token (word) per document (tweet) format. In order to look at sentiment trends over time, we'll need to decide over how many words to estimate the sentiment. 

In the below, we first add in our sentiment dictionary with `inner_join()`. We then use the `count()` function, specifying that we want to count over dates, and that words should be indexed in order (i.e., by row number) over every 1000 rows (i.e., every 1000 words). 

This means that if one date has many tweets totalling >1000 words, then we will have multiple observations for that given date; if there are only one or two tweets then we might have just one row and associated sentiment score for that date. 

We then calculate the sentiment scores for each of our sentiment types (positive, negative, anger, anticipation, disgust, fear, joy, sadness, surprise, and trust) and use the `spread()` function to convert these into separate columns (rather than rows). Finally we calculate a net sentiment score by subtracting the score for negative sentiment from positive sentiment. 

```{r}
#get tweet sentiment by date
tweets_nrc_sentiment <- tidy_tweets %>%
  inner_join(get_sentiments("nrc")) %>%
  count(date, index = order %/% 1000, sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative)

tweets_nrc_sentiment %>%
  ggplot(aes(date, sentiment)) +
  geom_point(alpha=0.5) +
  geom_smooth(method= loess, alpha=0.25)

```

How do our different sentiment dictionaries look when compared to each other? We can then plot the sentiment scores over time for each of our sentiment dictionaries like so:

```{r}

tidy_tweets %>%
  inner_join(get_sentiments("bing")) %>%
  count(date, index = order %/% 1000, sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative) %>%
  ggplot(aes(date, sentiment)) +
  geom_point(alpha=0.5) +
  geom_smooth(method= loess, alpha=0.25) +
  ylab("bing sentiment")

tidy_tweets %>%
  inner_join(get_sentiments("nrc")) %>%
  count(date, index = order %/% 1000, sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative) %>%
  ggplot(aes(date, sentiment)) +
  geom_point(alpha=0.5) +
  geom_smooth(method= loess, alpha=0.25) +
  ylab("nrc sentiment")

tidy_tweets %>%
  inner_join(get_sentiments("afinn")) %>%
  group_by(date, index = order %/% 1000) %>% 
  summarise(sentiment = sum(value)) %>% 
  ggplot(aes(date, sentiment)) +
  geom_point(alpha=0.5) +
  geom_smooth(method= loess, alpha=0.25) +
  ylab("afinn sentiment")


```

We see that they do look pretty similar... and interestingly it seems that overall sentiment positivity *increases* as the pandemic breaks.

## Domain-specific lexicons

Of course, list- or dictionary-based methods need not only focus on sentiment, even if this is one of their most common uses. In essence, what you'll have seen from the above is that sentiment analysis techniques rely on a given lexicon and score words appropriately. And there is nothing stopping us from making our own dictionaries, whether they measure sentiment or not. In the data above, we might be interested, for example, in the prevalence of mortality-related words in the news. As such, we might choose to make our own dictionary of terms. What would this look like?

A very minimal example would choose, for example, words like "death" and its synonyms and score these all as 1. We would then combine these into a dictionary, which we've called "mordict" here. 

```{r}

word <- c('death', 'illness', 'hospital', 'life', 'health',
             'fatality', 'morbidity', 'deadly', 'dead', 'victim')
value <- c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
mordict <- data.frame(word, value)
mordict

```

We could then use the same technique as above to bind these with our data and look at the incidence of such words over time. Combining the sequence of scripts from above we would do the following:

```{r}
tidy_tweets %>%
  inner_join(mordict) %>%
  group_by(date, index = order %/% 1000) %>% 
  summarise(morwords = sum(value)) %>% 
  ggplot(aes(date, morwords)) +
  geom_bar(stat= "identity") +
  ylab("mortality words")
```

The above simply counts the number of mortality words over time. This might be misleading if there are, for example, more or longer tweets at certain points in time; i.e., if the length or quantity of text is not time-constant. 

Why would this matter? Well, in the above it could just be that we have more mortality words later on because there are just more tweets earlier on. By just counting words, we are not taking into account the *denominator*.

An alternative, and preferable, method here would simply take a character string of the relevant words. We would then sum the total number of words across all tweets over time. Then we would filter our tweet words by whether or not they are a mortality word or not, according to the dictionary of words we have constructed. We would then do the same again with these words, summing the number of times they appear for each date. 

After this, we join with our data frame of total words for each date. Note that here we are using `full_join()` as we want to include dates that appear in the "totals" data frame that do not appear when we filter for mortality words; i.e., days when mortality words are equal to 0. We then go about plotting as before.

```{r}
mordict <- c('death', 'illness', 'hospital', 'life', 'health',
             'fatality', 'morbidity', 'deadly', 'dead', 'victim')

#get total tweets per day (no missing dates so no date completion required)
totals <- tidy_tweets %>%
  mutate(obs=1) %>%
  group_by(date) %>%
  summarise(sum_words = sum(obs))

#plot
tidy_tweets %>%
  mutate(obs=1) %>%
  filter(grepl(paste0(mordict, collapse = "|"),word, ignore.case = T)) %>%
  group_by(date) %>%
  summarise(sum_mwords = sum(obs)) %>%
  full_join(totals, word, by="date") %>%
  mutate(sum_mwords= ifelse(is.na(sum_mwords), 0, sum_mwords),
         pctmwords = sum_mwords/sum_words) %>%
  ggplot(aes(date, pctmwords)) +
  geom_point(alpha=0.5) +
  geom_smooth(method= loess, alpha=0.25) +
  xlab("Date") + ylab("% mortality words")


```

## Using Lexicoder

The above approaches use general dictionary-based techniques that were not designed for domain-specific text such as news text. The Lexicoder Sentiment Dictionary, by @young_affective_2012 was designed specifically for examining the affective content of news text. In what follows, we will see how to implement an analysis using this dictionary.

We will conduct the analysis using the `quanteda` package. You will see that we can tokenize text in a similar way using functions included in the quanteda package. 

With the `quanteda` package we first need to create a "corpus" object, by declaring our tweets a corpus object. Here, we make sure our date column is correctly stored and then create the corpus object with the `corpus()` function. Note that we are specifying the `text_field` as "tweet" as this is where our text data of interest is, and we are including information on the date that tweet was published. This information is specified with the `docvars` argument. You'll see tthen that the corpus consists of the text and so-called "docvars," which are just the variables (columns) in the original dataset. Here, we have only included the date column.

```{r}

tweets$date <- as.Date(tweets$created_at)

tweet_corpus <- corpus(tweets, text_field = "tweet", docvars = "date")
```


We then tokenize our text using the `tokens()` function from quanteda, removing punctuation along the way:
```{r}
toks_news <- tokens(tweet_corpus, remove_punct = TRUE)

```

We then take the `data_dictionary_LSD2015` that comes bundled with `quanteda` and and we select only the positive and negative categories, excluding words deemed "neutral." After this, we are ready to "look up" in this dictionary how the tokens in our corpus are scored with the `tokens_lookup()` function. 

```{r}
# select only the "negative" and "positive" categories
data_dictionary_LSD2015_pos_neg <- data_dictionary_LSD2015[1:2]

toks_news_lsd <- tokens_lookup(toks_news, dictionary = data_dictionary_LSD2015_pos_neg)
```

This creates a long list of all the texts (tweets) annotated with a series of 'positive' or 'negative' annotations depending on the valence of the words in that text. The creators of `quanteda` then recommend we generate a document feature matric from this. Grouping by date, we then get a dfm object, which is a quite convoluted list object that we can plot using base graphics functions for plotting matrices.

```{r}
# create a document document-feature matrix and group it by date
dfmat_news_lsd <- dfm(toks_news_lsd) %>% 
  dfm_group(groups = date)

# plot positive and negative valence over time
matplot(dfmat_news_lsd$date, dfmat_news_lsd, type = "l", lty = 1, col = 1:2,
        ylab = "Frequency", xlab = "")
grid()
legend("topleft", col = 1:2, legend = colnames(dfmat_news_lsd), lty = 1, bg = "white")

# plot overall sentiment (positive  - negative) over time

plot(dfmat_news_lsd$date, dfmat_news_lsd[,"positive"] - dfmat_news_lsd[,"negative"], 
     type = "l", ylab = "Sentiment", xlab = "")
grid()
abline(h = 0, lty = 2)

```

Alternatively, we can recreate this in tidy format as follows:

```{r}

negative <- dfmat_news_lsd@x[1:121]
positive <- dfmat_news_lsd@x[122:242]
date <- dfmat_news_lsd@Dimnames$docs


tidy_sent <- as.data.frame(cbind(negative, positive, date))

tidy_sent$negative <- as.numeric(tidy_sent$negative)
tidy_sent$positive <- as.numeric(tidy_sent$positive)
tidy_sent$sentiment <- tidy_sent$positive - tidy_sent$negative
tidy_sent$date <- as.Date(tidy_sent$date)
```

And plot accordingly:

```{r}
tidy_sent %>%
  ggplot() +
  geom_line(aes(date, sentiment))

```

## Exercises

1. Take a subset of the tweets data by "user_name" These names describe the name of the newspaper source of the Twitter account. Do we see different sentiment dynamics if we look only at different newspaper sources?
2. Build your own (minimal) dictionary-based filter technique and plot the result
3. Apply the Lexicoder Sentiment Dictionary to the news tweets, but break down the analysis by newspaper

## References 

<!--chapter:end:12-sent-analysis.Rmd-->

# Exercise 3: Comparison and complexity

## Introduction

The hands-on exercise for this week focuses on: 1) comparing texts; 2) measuring he document-level characteristics of text---here, complexity.

In this tutorial, you will learn how to:
  
* Compare texts using character-based measures of similarity and distance
* Compare texts using term-based measures of similarity and distance
* Calculate the complexity of texts
* Replicate analyses from @schoonvelde_liberals_2019

## Setup 

Before proceeding, we'll load the remaining packages we will need for this tutorial.

```{r, echo=F}
library(kableExtra)
```

```{r, message=F}
library(readr) # more informative and easy way to import data
library(quanteda) # includes functions to implement Lexicoder
library(quanteda.textstats) # for estimating similarity and complexity measures
library(stringdist) # for basic character-based distance measures
library(dplyr) #for wrangling data
library(tibble) #for wrangling data
library(ggplot2) #for visualization
```


For this example we'll be using data from the 2017-2018 Theresa May Cabinet in the UK. The data are tweets by members of this cabinet. 

We can load the data as follows.

```{r}
tweets <- readRDS("data/comparison-complexity/cabinet_tweets.rds")
```

If you're working on this document from your own computer ("locally") you can download the tweets data in the following way:

```{r, eval = F}

tweets  <- readRDS(gzcon(url("https://github.com/cjbarrie/CTA-ED/blob/main/data/comparison-complexity/cabinet_tweets.rds?raw=true")))

```

And we see that the data contain three variables: "username," which is the username of the MP in question; "tweet," which is the text of the given tweet, and "date" in days in yyyy-mm-dd format. 

```{r}

head(tweets)

```

And there are 24 MPs whose tweets we're examining. 

```{r}
unique(tweets$username)

length(unique(tweets$username))
```

## Generate document feature matrix

In order to use the `quanteda` package and its accompanying `quanteda.textstats` package, we need to reformat the data into a quanteda "corpus" object. To do this we just need to specify the text we're interested in as well as any associated document-level variables in which we're interested. 

We can do this as follows. 

```{r}
#make corpus object, specifying tweet as text field
tweets_corpus <- corpus(tweets, text_field = "tweet")

#add in username document-level information
docvars(tweets_corpus, "username") <- tweets$username

tweets_corpus
```

We are now ready to reformat the data into a document feature matrix.

```{r}
dfmat <- dfm(tokens(tweets_corpus),
             remove_punct = TRUE, 
             remove = stopwords("english"))

dfmat
```

Note that when we do this we need to have tokenized our corpus object first. We can do this by wrapping the `tokens` function inside the `dfm()` function as above. 

So what is this object? Well the documents here are tweets. And the matrix is a sparse (i.e., mostly zeroes) matrix of 1s and 0s for whether a given word appears in the document (tweet) in question. 

The vertical elements (columns) of this vector are made up of all the words used in all of the tweets combined. Here, it helps to imagine every tweet positioned side by side to understand what's going on here. 

## Compare between MPs

Once we have our data in this format, we are ready to compare between the text produced by members of Theresa May's Cabinet.

Here's an example of the correlations between the combined tweets of 5 of the MPs with each other.

```{r}
corrmat <- dfmat %>%
  dfm_group(groups = username) %>%
  textstat_simil(margin = "documents", method = "correlation")

corrmat[1:5,1:5]

```

Note that here we're using the `dfm_group()` function, which allows you to take a document feature matrix and make calculations while grouping by one of the document-level variables we specified above. 

There are many different measures of similarity, however, that we might think about using.

In the below, we combine four different measures of similarity, and see how they compare to each other across MPs. Note that here we're looking only at the similarity between an MP's tweets and those of then Prime Minister, Theresa May.

## Compare between measures

Let's see what this looks like for one of these measures---cosine similarity.

We first get similarities between the text of MP tweets and all other MPs.

```{r}

#estimate similarity, grouping by username

cos_sim <- dfmat %>%
  dfm_group(groups = username) %>%
  textstat_simil(margin = "documents", method = "cosine") #specify method here as character object

```

But remember we're only interested in how they compare to what Theresa May has been saying. 

So we need to take these cosine similarities and retain only those similarity measures corresponding to the text of Theresa May's tweets. 

We first convert the `textstat_simil()` output to a matrix.

```{r}

cosmat <- as.matrix(cos_sim) #convert to a matrix
  
```

And we can see that the 23rd row of this matrix contains the similarity measures with the Theresa May tweets. 

We take this row, removing the similarity of Theresa May with herself (which will always = 1), and convert it to a datframe object. 

```{r}
#generate data frame keeping only the row for Theresa May
cosmatdf <- as.data.frame(cosmat[23, c(1:22, 24)])
```

We then rename the cosine similarity column with an appropriate name and convert row names to a column variable so that we have cells containing information on the MP to which the cosine similarity measure refers. 

```{r}
#rename column
colnames(cosmatdf) <- "corr_may"
  
#create column variable from rownames
cosmatdf <- tibble::rownames_to_column(cosmatdf, "username")
```

And like so we have our data in tidy format, which we can then plot like so. 

```{r}
ggplot(cosmatdf) +
  geom_point(aes(x=reorder(username, -corr_may), y= corr_may)) + 
  coord_flip() +
  xlab("MP username") +
  ylab("Cosine similarity score") + 
  theme_minimal()

```

Combining these steps into a single `for` loop, we can see how our different similarity measures of interest compare. 

```{r}

#specify different similarity measures to explore
methods <- c("correlation", "cosine", "dice", "edice")

#create empty dataframe
testdf_all <- data.frame()

#gen for loop across methods types
for (i in seq_along(methods)) {
  
  #pass method to character string object
  sim_method <- methods[[i]]
  
  #estimate similarity, grouping by username
  test <- dfmat %>%
    dfm_group(groups = username) %>%
    textstat_simil(margin = "documents", method = sim_method) #specify method here as character object created above
  
  testm <- as.matrix(test) #convert to a matrix
  
  #generate data frame keeping only the row for Theresa May
  testdf <- as.data.frame(testm[23, c(1:22, 24)])
  
  #rename column
  colnames(testdf) <- "corr_may"
  
  #create column variable from rownames
  testdf <- tibble::rownames_to_column(testdf, "username")
  
  #record method in new column variable
  testdf$method <- sim_method

  #bind all together
  testdf_all <- rbind(testdf_all, testdf)  
  
}

#create variable (for viz only) that is mean of similarity scores for each MP
testdf_all <- testdf_all %>%
  group_by(username) %>%
  mutate(mean_sim = mean(corr_may))

ggplot(testdf_all) +
  geom_point( aes(x=reorder(username, -mean_sim), y= corr_may, color = method)) + 
  coord_flip() +
  xlab("MP username") +
  ylab("Similarity score") + 
  theme_minimal()

```

## Complexity

We now move to document-level measures of text characteristics. And here we will focus on the paper by @schoonvelde_liberals_2019. 

We will be using a subset of these data, taken from EU speeches given by four politicians. These are provided by the authors at [https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/S4IZ8K](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/S4IZ8K).

We can load the data as follows.

```{r}
speeches <- readRDS("data/comparison-complexity/speeches.rds")
```

If you're working on this document from your own computer ("locally") you can download the tweets data in the following way:

```{r, eval = F}

speeches  <- readRDS(gzcon(url("https://github.com/cjbarrie/CTA-ED/blob/main/data/comparison-complexity/speeches.rds?raw=true")))

```

And we can take a look at what the data contains below.

```{r}

head(speeches)

```

The data contain speeches by four different politicians, each of whom are positioned at different points on a liberal-conservative scale.

We can then calculate the Flesch-Kincaid readability/complexity score with the `quanteda.textstats` package like so.

```{r}
speeches$flesch.kincaid <- textstat_readability(speeches$text, measure = "Flesch.Kincaid")

# returned as quanteda data.frame with document-level information;
# need just the score:
speeches$flesch.kincaid <- speeches$flesch.kincaid$Flesch.Kincaid
```

We want this information aggregated over each of our politicians: Gordon Brown, Jose Zapatero", David Cameron, and Mariano Rajoy. These are recorded in the data under a column called "speaker."

```{r}
#get mean and standard deviation of Flesch-Kincaid, and N of speeches for each speaker
sum_corpus <- speeches %>%
  group_by(speaker) %>%
  summarise(mean = mean(flesch.kincaid, na.rm=TRUE),
                   SD=sd(flesch.kincaid, na.rm=TRUE),
                   N=length(speaker))

# calculate standard errors and confidence intervals
sum_corpus$se <- sum_corpus$SD / sqrt(sum_corpus$N)
sum_corpus$min <- sum_corpus$mean - 1.96*sum_corpus$se
sum_corpus$max <- sum_corpus$mean + 1.96*sum_corpus$se
```

And this gives us data in tidy format that looks like so. 

```{r}
sum_corpus
```

Which we can then plot---and we see that our results look like those in Figure 1 of the published article by @schoonvelde_liberals_2019. 


```{r}

ggplot(sum_corpus, aes(x=speaker, y=mean)) +
  geom_bar(stat="identity") + 
  geom_errorbar(ymin=sum_corpus$min,ymax=sum_corpus$max, width=.2) +
  coord_flip() +
  xlab("") +
  ylab("Mean Complexity") + 
  theme_minimal() + 
  ylim(c(0,20))

```

## Exercises

1. Compute distance measures such as "euclidean" or "manhattan" for the MP tweets as above, comparing between tweets by MPs and tweets by PM, Theresa May. 
2. Estimate at least three other complexity measures for the EU speeches as above. Consider how the results compare to the Flesch-Kincaid measure used in the article by @schoonvelde_liberals_2019.
3. (Advanced---optional) Estimate similarity scores between the MP tweets and the PM tweets for each week contained in the data. Plot the results. 

<!--chapter:end:13-comparison-and-complexity.Rmd-->

# Exercise 4: Scaling techniques

## Introduction

The hands-on exercise for this week focuses on: 1) scaling texts ; 2) implementing scaling techniques using `quanteda`. 

In this tutorial, you will learn how to:
  
* Scale texts using the "wordfish" algorithm
* Scale texts gathered from online sources
* Replicate analyses by @kaneko_estimating_2021

Before proceeding, we'll load the packages we will need for this tutorial.

```{r, echo=F}
library(kableExtra)
```

```{r, message=F}
library(dplyr)
library(quanteda) # includes functions to implement Lexicoder
library(quanteda.textmodels) # for estimating similarity and complexity measures
library(quanteda.textplots) #for visualizing text modelling results
```

In this exercise we'll be using the dataset we used for the sentiment analysis exercise. The data were collected from the Twitter accounts of the top eight newspapers in the UK by circulation. The tweets include any tweets by the news outlet from their main account. 

## Importing data

We can download the dataset with:

```{r}
tweets <- readRDS("data/sentanalysis/newstweets.rds")
```

If you're working on this document from your own computer ("locally") you can download the tweets data in the following way:

```{r, eval = F}
tweets  <- readRDS(gzcon(url("https://github.com/cjbarrie/CTA-ED/blob/main/data/sentanalysis/newstweets.rds?raw=true")))

```

We first take a sample from these data to speed up the runtime of some of the analyses. 

```{r}

tweets <- tweets %>%
  sample_n(20000)

```

## Construct `dfm` object

Then, as in the previous exercise, we create a corpus object, specify the document-level variables by which we want to group, and generate our document feature matrix. 

```{r, eval = F}

#make corpus object, specifying tweet as text field
tweets_corpus <- corpus(tweets, text_field = "text")

#add in username document-level information
docvars(tweets_corpus, "newspaper") <- tweets$user_username

dfm_tweets <- dfm(tokens(tweets_corpus),
                  remove_punct = TRUE, 
                  remove = stopwords("english"))

```

```{r, echo = F}
dfm_tweets <- readRDS("data/wordscaling/dfm_tweets.rds")

```

We can then have a look at the number of documents (tweets) we have per newspaper Twitter account. 

```{r}

## number of tweets per newspaper
table(docvars(dfm_tweets, "newspaper"))

```

And this is what our document feature matrix looks like, where each word has a count for each of our eight newspapers. 

```{r}

dfm_tweets

```

## Estimate wordfish model

Once we have our data in this format, we are able to group and trim the document feature matrix before estimating the wordfish model.

```{r}
# compress the document-feature matrix at the newspaper level
dfm_newstweets <- dfm_group(dfm_tweets, groups = newspaper)
# remove words not used by two or more newspapers
dfm_newstweets <- dfm_trim(dfm_newstweets, 
                                min_docfreq = 2, docfreq_type = "count")

## size of the document-feature matrix
dim(dfm_newstweets)

#### estimate the Wordfish model ####
set.seed(123L)
dfm_newstweets_results <- textmodel_wordfish(dfm_newstweets, 
                                             sparse = TRUE)

```

And this is what results.

```{r}
summary(dfm_newstweets_results)
```

We can then plot our estimates of the $\theta$s---i.e., the estimates of the latent newspaper position---as so.

```{r}
textplot_scale1d(dfm_newstweets_results)
```

Interestingly, we seem not to have captured ideology but some other tonal dimension. We see that the tabloid newspapers are scored similarly, and grouped toward the right hand side of this latent dimension; whereas the broadsheet newspapers have an estimated theta further to the left.

Plotting the "features," i.e., the word-level betas shows how words are positioned along this dimension, and which words help discriminate between news outlets.

```{r}

textplot_scale1d(dfm_newstweets_results, margin = "features")

```

And we can also look at these features.

```{r}

features <- dfm_newstweets_results[["features"]]

betas <- dfm_newstweets_results[["beta"]]

feat_betas <- as.data.frame(cbind(features, betas))
feat_betas$betas <- as.numeric(feat_betas$betas)

feat_betas %>%
  arrange(desc(betas)) %>%
  top_n(20) %>% 
  kbl() %>%
  kable_styling(bootstrap_options = "striped")

```

These words do seem to belong to more tabloid-style reportage, and include emojis relating to film, sports reporting on "cristiano" as well as more colloquial terms like "saucy."

## Replicating Kaneko et al.

This section adapts code from the replication data provided for @kaneko_estimating_2021 [here](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/EL3KYD). We can access data from the first study by @kaneko_estimating_2021 in the following way. 

```{r}

kaneko_dfm <- readRDS("data/wordscaling/study1_kaneko.rds")

```

If you're working locally, you can download the `dfm` data with:

```{r, eval = F}
kaneko_dfm  <- readRDS(gzcon(url("https://github.com/cjbarrie/CTA-ED/blob/main/data/wordscaling/study1_kaneko.rds?raw=true")))

```

This data is in the form a document-feature-matrix. We can first manipulate it in the same way as @kaneko_estimating_2021 by grouping at the level of newspaper and removing infrequent words.

```{r}
table(docvars(kaneko_dfm, "Newspaper"))
## prepare the newspaper-level document-feature matrix
# compress the document-feature matrix at the newspaper level
kaneko_dfm_study1 <- dfm_group(kaneko_dfm, groups = Newspaper)
# remove words not used by two or more newspapers
kaneko_dfm_study1 <- dfm_trim(kaneko_dfm_study1, min_docfreq = 2, docfreq_type = "count")

## size of the document-feature matrix
dim(kaneko_dfm_study1)

```

## Exercises

1. Estimate a wordfish model for the @kaneko_estimating_2021 data
2. Visualize the results

<!--chapter:end:14-scaling.Rmd-->

# Exercise 5: Unsupervised learning (topic models)

## Introduction

The hands-on exercise for this week focuses on: 1) estimating a topic model ; 2) interpreting and visualizing results.

In this tutorial, you will learn how to:

* Generate document-term-matrices in format appropriate for topic modelling
* Estimate a topic model using the `quanteda` and `topicmodels` package
* Visualize results
* Reverse engineer a test of model accuracy
* Run some validation tests

## Setup 

Before proceeding, we'll load the packages we will need for this tutorial.

```{r, message=F}
library(tidyverse) # loads dplyr, ggplot2, and others
library(stringr) # to handle text elements
library(tidytext) # includes set of functions useful for manipulating text
library(topicmodels) # to estimate topic models
library(gutenbergr) # to get text data
library(scales)
library(tm)
library(ggthemes) # to make your plots look nice
library(readr)
library(quanteda)
library(quanteda.textmodels)
#devtools::install_github("matthewjdenny/preText")
library(preText)
```

We'll be using data from Alexis de Tocqueville's "Democracy in America." We will download these data , both Volume 1 and Volume 2, and combine them into one data frame. For this, we'll be using the <tt>gutenbergr</tt> package, which allows the user to download text data from over 60,000 out-of-copyright books. The ID for each book appears in the url for the book selected after a search on [https://www.gutenberg.org/ebooks/](https://www.gutenberg.org/ebooks/).

This example is adapted by [Text Mining with R: A Tidy Approach](https://www.tidytextmining.com/) by Julia Silge and David Robinson.

![](data/topicmodels/gutenberg.gif){width=100%}

Here, we see that Volume of Tocqueville's "Democracy in America" is stored as "815". A separate search reveals that Volume 2 is stored as "816".

```{r, eval=F}
tocq <- gutenberg_download(c(815, 816), 
                            meta_fields = "author")
```

Or we can download the dataset with:

```{r}
tocq <- readRDS("data/topicmodels/tocq.rds")
```

If you're working on this document from your own computer ("locally") you can download the data in the following way:

```{r, eval = F}
tocq  <- readRDS(gzcon(url("https://github.com/cjbarrie/CTA-ED/blob/main/data/topicmodels/tocq.RDS?raw=true")))

```

Once we have read in these data, we convert it into a different data shape: the document-term-matrix. We also create a new columns, which we call "booknumber" that recordss whether the term in question is from Volume 1 or Volume 2. To convert from tidy into "DocumentTermMatrix" format we can first use `unnest_tokens()` as we have done in past exercises, remove stop words, and then use the `cast_dtm()` function to convert into a "DocumentTermMatrix" object.

```{r}
tocq_words <- tocq %>%
  mutate(booknumber = ifelse(gutenberg_id==815, "DiA1", "DiA2")) %>%
  unnest_tokens(word, text) %>%
  filter(!is.na(word)) %>%
  count(booknumber, word, sort = TRUE) %>%
  ungroup() %>%
  anti_join(stop_words)

tocq_dtm <- tocq_words %>%
  cast_dtm(booknumber, word, n)

tm::inspect(tocq_dtm)
```

We see here that the data are now stored as a "DocumentTermMatrix." In this format, the matrix records the term (as equivalent of a column) and the document (as equivalent of row), and the number of times the term appears in the given document. Many terms will not appear in the document, meaning that the matrix will be stored as "sparse," meaning there will be a preponderance of zeroes. Here, since we are looking only at two documents that both come from a single volume set, the sparsity is relatively low (only 27%). In most applications, the sparsity will be a lot higher, approaching 99% or more.

Estimating our topic model is then relatively simple. All we need to do if specify how many topics that we want to search for, and we can also set our seed, which is needed to reproduce the same results each time (as the model is a generative probabilistic one, meaning different random iterations will produce different results).

```{r}

tocq_lda <- LDA(tocq_dtm, k = 10, control = list(seed = 1234))

```

After this we can extract the per-topic-per-word probabilities, called "β" from the model:

```{r}
tocq_topics <- tidy(tocq_lda, matrix = "beta")

head(tocq_topics, n = 10)

```

We now have data stored as one topic-per-term-per-row. The betas listed here represent the probability that the given term belongs to a given topic. So, here, we see that the term "democratic" is most likely to belong to topic 4. Strictly, this probability represents the probability that the term is generated from the topic in question.

We can then plots the top terms, in terms of beta, for each topic as follows:

```{r}
tocq_top_terms <- tocq_topics %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

tocq_top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free", ncol = 4) +
  scale_y_reordered() +
  theme_tufte(base_family = "Helvetica")

```

But how do we actually evaluate these topics? Here, the topics all seem pretty similar. 

## Evaluating topic model

Well, one way to evaluate the performance of unspervised forms of classification is by testing our model on an outcome that is already known. 

Here, two topics that are most obvious are the 'topics' Volume 1 and Volume 2 of Tocqueville's "Democracy in America." Volume 1 of Tocqueville's work deals more obviously with abstract constitutional ideas and questions of race; Volume 2 focuses on more esoteric aspects of American society. Listen an "In Our Time" episode with Melvyn Bragg discussing Democracy in America [here](https://www.bbc.co.uk/programmes/b09vyw0x).

Given these differences in focus, we might think that a generative model could accurately assign to topic (i.e., Volume) with some accuracy.

### Plot relative word frequencies

First let's have a look and see whether there really are words obviously distinguishing the two Volumes. 

```{r}

tidy_tocq <- tocq %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words)

## Count most common words in both
tidy_tocq %>%
  count(word, sort = TRUE)

bookfreq <- tidy_tocq %>%
  mutate(booknumber = ifelse(gutenberg_id==815, "DiA1", "DiA2")) %>%
  mutate(word = str_extract(word, "[a-z']+")) %>%
  count(booknumber, word) %>%
  group_by(booknumber) %>%
  mutate(proportion = n / sum(n)) %>% 
  select(-n) %>% 
  spread(booknumber, proportion)

ggplot(bookfreq, aes(x = DiA1, y = DiA2, color = abs(DiA1 - DiA2))) +
  geom_abline(color = "gray40", lty = 2) +
  geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
  scale_x_log10(labels = percent_format()) +
  scale_y_log10(labels = percent_format()) +
  scale_color_gradient(limits = c(0, 0.001), low = "darkslategray4", high = "gray75") +
  theme_tufte(base_family = "Helvetica") +
  theme(legend.position="none", 
        strip.background = element_blank(), 
        strip.text.x = element_blank()) +
  labs(x = "Tocqueville DiA 2", y = "Tocqueville DiA 1") +
  coord_equal()


```

We see that there do seem to be some marked distinguishing characteristics. In the plot above, for example, we see that more abstract notions of state systems appear with greater frequency in Volume 1 while Volume 2 seems to contain words specific to America (e.g., "north" and "south") with greater frequency. The way to read the above plot is that words positioned further away from the diagonal line appear with greater frequency in one volume versus the other.


### Split into chapter documents

In the below, we first separate the volumes into chapters, then we repeat the same procedure as above. The only difference now is that instead of two documents representing the two full volumes of Tocqueville's work, we now have 132 documents, each representing an individual chapter. Notice now that the sparsity is much increased: around 96%. 

```{r}

tocq <- tocq %>%
  filter(!is.na(text))

# Divide into documents, each representing one chapter
tocq_chapter <- tocq %>%
  mutate(booknumber = ifelse(gutenberg_id==815, "DiA1", "DiA2")) %>%
  group_by(booknumber) %>%
  mutate(chapter = cumsum(str_detect(text, regex("^chapter ", ignore_case = TRUE)))) %>%
  ungroup() %>%
  filter(chapter > 0) %>%
  unite(document, booknumber, chapter)

# Split into words
tocq_chapter_word <- tocq_chapter %>%
  unnest_tokens(word, text)

# Find document-word counts
tocq_word_counts <- tocq_chapter_word %>%
  anti_join(stop_words) %>%
  count(document, word, sort = TRUE) %>%
  ungroup()

tocq_word_counts

# Cast into DTM format for LDA analysis

tocq_chapters_dtm <- tocq_word_counts %>%
  cast_dtm(document, word, n)

tm::inspect(tocq_chapters_dtm)

```

We then re-estimate the topic model with this new DocumentTermMatrix object, specifying k equal to 2. This will enable us to evaluate whether a topic model is able to generatively assign to volume with accuracy.

```{r}
tocq_chapters_lda <- LDA(tocq_chapters_dtm, k = 2, control = list(seed = 1234))

```

After this, it is worth looking at another output of the latent dirichlet allocation procedure. The γ probability represents the per-document-per-topic probability or, in other words, the probability that a given document (here: chapter) belongs to a particular topic (and here, we are assuming these topics represent volumes).

The gamma values are therefore the estimated proportion of words within a given chapter allocated to a given volume. 

```{r}

tocq_chapters_gamma <- tidy(tocq_chapters_lda, matrix = "gamma")
tocq_chapters_gamma

```

### Examine consensus

Now that we have these topic probabilities, we can see how well our unsupervised learning did at distinguishing the two volumes generatively just from the words contained in each chapter.

```{r}

# First separate the document name into title and chapter

tocq_chapters_gamma <- tocq_chapters_gamma %>%
  separate(document, c("title", "chapter"), sep = "_", convert = TRUE)

tocq_chapter_classifications <- tocq_chapters_gamma %>%
  group_by(title, chapter) %>%
  top_n(1, gamma) %>%
  ungroup()

tocq_book_topics <- tocq_chapter_classifications %>%
  count(title, topic) %>%
  group_by(title) %>%
  top_n(1, n) %>%
  ungroup() %>%
  transmute(consensus = title, topic)

tocq_chapter_classifications %>%
  inner_join(tocq_book_topics, by = "topic") %>%
  filter(title != consensus)

# Look document-word pairs were to see which words in each documents were assigned
# to a given topic

assignments <- augment(tocq_chapters_lda, data = tocq_chapters_dtm)
assignments

assignments <- assignments %>%
  separate(document, c("title", "chapter"), sep = "_", convert = TRUE) %>%
  inner_join(tocq_book_topics, by = c(".topic" = "topic"))

assignments %>%
  count(title, consensus, wt = count) %>%
  group_by(title) %>%
  mutate(percent = n / sum(n)) %>%
  ggplot(aes(consensus, title, fill = percent)) +
  geom_tile() +
  scale_fill_gradient2(high = "red", label = percent_format()) +
  geom_text(aes(x = consensus, y = title, label = scales::percent(percent))) +
  theme_tufte(base_family = "Helvetica") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1),
        panel.grid = element_blank()) +
  labs(x = "Book words assigned to",
       y = "Book words came from",
       fill = "% of assignments")

```

Not bad! We see that the model estimated with accuracy 91% of chapters in Volume 2 and 79% of chapters in Volume 1

## Validation

In the articles by @ying_topics_2021 and @denny_text_2018 from this and previous weeks, we read about potential validation techniques. 

In this section, we'll be using the `preText` package mentioned in @denny_text_2018 to see the impact of different pre-processing choices on our text. Here, I am adapting from a [tutorial](http://www.mjdenny.com/getting_started_with_preText.html) by Matthew Denny.

First we need to reformat our text into a `quanteda` corpus object. 

```{r}

# load in U.S. presidential inaugural speeches from Quanteda example data.
corp <- corpus(tocq, text_field = "text")
# use first 10 documents for example
documents <- corp[sample(1:30000,1000)]
# take a look at the document names
print(names(documents[1:10]))

```
And now we are ready to preprocess in different ways. Here, we are including n-grams so we are preprocessing the text in 128 different ways. This takes about ten minutes to run on a machine with 8GB RAM. 

```{r, eval = F}

preprocessed_documents <- factorial_preprocessing(
    documents,
    use_ngrams = TRUE,
    infrequent_term_threshold = 0.2,
    verbose = FALSE)

```

We can then get the results of our pre-processing, comparing the distance between documents that have been processed in different ways. 


```{r, eval = F}

preText_results <- preText(
    preprocessed_documents,
    dataset_name = "Tocqueville text",
    distance_method = "cosine",
    num_comparisons = 20,
    verbose = FALSE)

```

And we can plot these accordingly. 

```{r, eval = F}
preText_score_plot(preText_results)
```

![](data/topicmodels/pretext_results.png){width=100%}

## Exercises

1. Choose another book or set of books from Project Gutenberg
2. Run your own topic model on these books, changing the k of topics, and evaluating accuracy.
3. Validate different pre-processing techniques using `preText` on the new book(s) of your choice. 

<!--chapter:end:15-unsupervised-topicmodels.Rmd-->

# Exercise 6: Unsupervised learning (word embedding)

## Introduction

The hands-on exercise for this week focuses on word embedding and provides an overview of the data structures, and functions relevant for, estimating word vectors for word-embedding analyses.

In this tutorial, you will learn how to:

* Generate word vectors (embeddings) via SVD
* Train a local word embedding model in GloVe
* Visualize and inspect results
* Load and examine pre-trained embeddings

Note: Adapts from tutorials by Chris Bail [here](https://cbail.github.io/textasdata/word2vec/rmarkdown/word2vec.html) and Julia Silge [here](https://juliasilge.com/blog/tidy-word-vectors/) and Emil Hvitfeldt and Julia Silge [here](https://smltar.com/).

## Setup 

```{r, message=F}
library(tidyverse) # loads dplyr, ggplot2, and others
library(stringr) # to handle text elements
library(tidytext) # includes set of functions useful for manipulating text
library(ggthemes) # to make your plots look nice
library(text2vec) # for word embedding implementation
library(widyr) # for reshaping the text data
library(irlba) # for svd
library(umap) # for dimensionality reduction
```

We begin by reading in the data. These data come from a sample of 1m tweets by elected UK MPs over the period 2017-2019. The data contain just the name of the MP-user, the text of the tweet, and the MP's party. We then just add an ID variable called "postID."

```{r}
twts_sample <- readRDS("data/wordembed/twts_corpus_sample.rds")

#create tweet id
twts_sample$postID <- row.names(twts_sample)

```

If you're working on this document from your own computer ("locally") you can download the tweets sample data in the following way:

```{r, eval = F}

twts_sample <- readRDS(gzcon(url("https://github.com/cjbarrie/CTA-ED/blob/main/data/wordembed/twts_corpus_sample.rds?raw=true")))
```

## Word vectors via SVD

We're going to set about generating a set of word vectors with from our text data. Note that many word embedding applications will use pre-trained embeddings from a much larger corpus, or will generate local embeddings using neural net-based approaches. 

Here, we're instead going to generate a set of embeddings or word vectors by making a series of calculations based on the frequencies with which words appear in different contexts. We will then use a technique called the "Singular Value Decomposition" (SVD). This is a dimensionality reduction technique where the first axis of the resulting composition is designed to capture the most variance, the second the second-most etc...

How do we achieve this?

## Implementation

The first thing we need to do is to get our data in the right format to calculate so-called "skip-gram probabilties." If you go through the code line by the line in the below you will begin to understand what these are. 

What's going on?

Well, we're first unnesting our tweet data as in previous exercises. But importantly, here, we're not unnesting to individual tokens but to ngrams of length 6 or, in other words, for postID n with words k indexed by i, we take words i~1~ ...i~6~, then we take words i~2~ ...i~7~. Try just running the first two lines of the code below to see what this means in practice. 

After this, we make a unique ID for the particular ngram we create for each postID, and then we make a unique skipgramID for each postID and ngram. And then we unnest the words of each ngram associated with each skipgramID.

You can see the resulting output below.

```{r, eval = F}

#create context window with length 6
tidy_skipgrams <- twts_sample %>%
    unnest_tokens(ngram, tweet, token = "ngrams", n = 6) %>%
    mutate(ngramID = row_number()) %>% 
    tidyr::unite(skipgramID, postID, ngramID) %>%
    unnest_tokens(word, ngram)

head(tidy_skipgrams, n=20)
```

```{r, echo=F}
load("data/wordembed/tidy_skipgrams.RData")
head(tidy_skipgrams, n=20)
```

What next?

Well we can now calculate a set of probabilities from our skipgrams. We do so with the `pairwise_count()` function from the <tt>widyr</tt> package. Essentially, this function is saying: for each skipgramID count the number of times a word appears with another word for that feature (where the feature is the skipgramID). We set `diag` to `TRUE` when we also want to count the number of times a word appears near itself. 

The probability we are then calculating is the number of times a word appears with another word denominated by the total number of word pairings across the whole corpus. 

```{r, eval=F}
#calculate probabilities
skipgram_probs <- tidy_skipgrams %>%
    pairwise_count(word, skipgramID, diag = TRUE, sort = TRUE) %>% # diag = T means that we also count when the word appears twice within the window
    mutate(p = n / sum(n))

head(skipgram_probs[1000:1020,], n=20)
```
```{r, echo=F}
#calculate probabilities
skipgram_probs <- tidy_skipgrams %>%
    pairwise_count(word, skipgramID, diag = TRUE, sort = TRUE) %>%
    mutate(p = n / sum(n))

head(skipgram_probs[1000:1020,], n=20)

```

So we see, for example, the words vote and for appear 4099 times together. Denominating that by the total n of word pairings (or `sum(skipgram_probs$n)`), gives us our probability p. 

Okay, now we have our skipgram probabilities we need to get our "unigram probabilities" in order to normalize the skipgram probabilities before applying the singular value decomposition. 

What is a "unigram probability"? Well, this is just a technical way of saying: count up all the appearances of a given word in our corpus then divide that by the total number of words in our corpus. And we can do this as such:

```{r, warning=FALSE}

#calculate unigram probabilities (used to normalize skipgram probabilities later)
unigram_probs <- twts_sample %>%
    unnest_tokens(word, tweet) %>%
    count(word, sort = TRUE) %>%
    mutate(p = n / sum(n))

```

Finally, it's time to normalize our skipgram probabilities. 

We take our skipgram probabilities, we filter out word pairings that appear twenty times or less. We rename our words "item1" and "item2," we merge in the unigram probabilities for both words. 

And then we calculate the joint probability as the skipgram probability divided by the unigram probability for the first word in the pairing divided by the unigram probability for the second word in the pairing. This is equivalent to: P(x,y)/P(x)P(y). 

In essence, the interpretation of this value is: *"do events (words) x and y occur together more often than we would expect if they were independent"*?

Once we've recovered these normalized probabilities, we can have a look at the joint probabilities for a given item, i.e., word. Here, we look at the word "brexit" and look at those words with the highest value for "p_together."

Higher values greater than 1 indicate that the words are more likely to appear close to each other; low values less than 1 indicate that they are unlikely to appear close to each other. This, in other words, gives an indication of the association of two words.

```{r, warning=F}

#normalize skipgram probabilities
normalized_prob <- skipgram_probs %>%
    filter(n > 20) %>% #filter out skipgrams with n <=20
    rename(word1 = item1, word2 = item2) %>%
    left_join(unigram_probs %>%
                  select(word1 = word, p1 = p),
              by = "word1") %>%
    left_join(unigram_probs %>%
                  select(word2 = word, p2 = p),
              by = "word2") %>%
    mutate(p_together = p / p1 / p2)

normalized_prob %>% 
    filter(word1 == "brexit") %>%
    arrange(-p_together)

```

Using this normalized probabilities, we then calculate the PMI or "Pointwise Mutual Information" value, which is simply the log of the joint probability we calculated above. 

**Definition time**: "PMI is logarithm of the probability of finding two words together, normalized for the probability of finding each of the words alone."

We then cast our word pairs into a sparse matrix where values correspond to the PMI between two corresponding words. 

```{r, eval=F}

pmi_matrix <- normalized_prob %>%
    mutate(pmi = log10(p_together)) %>%
    cast_sparse(word1, word2, pmi)

#remove missing data
pmi_matrix@x[is.na(pmi_matrix@x)] <- 0
#run SVD
pmi_svd <- irlba(pmi_matrix, 256, maxit = 500)

glimpse(pmi_matrix)

```

```{r ,echo=F}
load("data/wordembed/pmi_svd.RData")

pmi_matrix <- normalized_prob %>%
    mutate(pmi = log10(p_together)) %>%
    cast_sparse(word1, word2, pmi)

pmi_matrix@x[is.na(pmi_matrix@x)] <- 0

glimpse(pmi_matrix)

```

Notice here that we are setting the vector size to equal 256. This just means that we have a vector length of 256 for any given word. 

That is, the set of numbers used to represent a word has length limited to 256. This is arbitrary and can be changed. Typically, a size in the low hundreds is chosen when representing a word as a vector. 

The word vectors are then taken as the "u" column, or the left-singular vectors, of the SVD.

```{r}
#next we output the word vectors:
word_vectors <- pmi_svd$u
rownames(word_vectors) <- rownames(pmi_matrix)

dim(word_vectors)

```

## Exploration 

We can define a simple function below to then take our word vector, and find the most similar words, or nearest neighbours, for a given word:

```{r}

nearest_words <- function(word_vectors, word){
  selected_vector = word_vectors[word,]
  mult = as.data.frame(word_vectors %*% selected_vector) #dot product of selected word vector and all word vectors
  
  mult %>%
  rownames_to_column() %>%
  rename(word = rowname,
         similarity = V1) %>%
    anti_join(get_stopwords(language = "en")) %>%
  arrange(-similarity)

}

boris_synonyms <- nearest_words(word_vectors, "boris")

brexit_synonyms <- nearest_words(word_vectors, "brexit")

head(boris_synonyms, n=10)

head(brexit_synonyms, n=10)

#then we can visualize
brexit_synonyms %>%
  mutate(selected = "brexit") %>%
  bind_rows(boris_synonyms %>%
              mutate(selected = "boris")) %>%
  group_by(selected) %>%
  top_n(15, similarity) %>%
  mutate(token = reorder(word, similarity)) %>%
  filter(token!=selected) %>%
  ggplot(aes(token, similarity, fill = selected)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~selected, scales = "free") +
  scale_fill_manual(values = c("#336B87", "#2A3132")) +
  coord_flip() +
  theme_tufte(base_family = "Helvetica")

```


## GloVe Embeddings

This section adapts from tutorials by Pedro Rodriguez [here](https://github.com/prodriguezsosa/conText/blob/master/vignettes/quickstart_local_transform.md) and Dmitriy Selivanov [here](http://text2vec.org/glove.html) and Wouter van Gils [here](https://medium.com/broadhorizon-cmotions/nlp-with-r-part-2-training-word-embedding-models-and-visualize-results-ae444043e234).

## GloVe algorithm

This section is taken from <tt>text2vec</tt> package page [here](http://text2vec.org/glove.html).

The GloVe algorithm by pennington_glove_2014 consists of the following steps:

1. Collect word co-occurence statistics in a form of word co-ocurrence matrix $X$. Each element $X_{ij}$ of such matrix represents how often word *i* appears in context of word *j*. Usually we scan our corpus in the following manner: for each term we look for context terms within some area defined by a *window_size* before the term and a *window_size* after the term. Also we give less weight for more distant words, usually using this formula: $$decay = 1/offset$$

2. Define soft constraints for each word pair:  $$w_i^Tw_j + b_i + b_j = log(X_{ij})$$ Here $w_i$ - vector for the main word, $w_j$ - vector for the context word, $b_i$, $b_j$ are scalar biases for the main and context words.

3. Define a cost function 
$$J = \sum_{i=1}^V \sum_{j=1}^V \; f(X_{ij}) ( w_i^T w_j + b_i + b_j - \log X_{ij})^2$$
Here $f$ is a weighting function which help us to prevent learning only from extremely common word pairs. The GloVe authors choose the following function:

$$
f(X_{ij}) = 
\begin{cases}
(\frac{X_{ij}}{x_{max}})^\alpha & \text{if } X_{ij} < XMAX \\
1 & \text{otherwise}
\end{cases}
$$

How do we go about implementing this algorithm in R?

Let's first make sure we have loaded the packages we need:

```{r, eval = F}
library(text2vec) # for implementation of GloVe algorithm
library(stringr) # to handle text strings
library(umap) # for dimensionality reduction later on

```

## Implementation

We then need to set some of the choice parameters of the GloVe model. The first is the window size `WINDOW_SIZE`, which, as above, is arbitrary but normally set around 6-8. This means we are looking for word context of words up to 6 words around the target word. The image below illustrates this choice parameter for the word "cat" in a given sentence, with increase context window size:

![Context window](data/wordembed/window.png){width=50%}

And this will ultimately be understood in matrix format as:

![Context window](data/wordembed/matrix_context.png){width=50%}

The iterations parameter `ITERS` simply sets the maximum number of iterations to allow for model convergence. This number of iterations is relatively high and the model will likely converge before 100 iterations. 

The `DIM` parameter specifies the length of the word vector we want to result (i.e., just as we set a limit of 256 for the SVD approach above). Finally, `COUNT_MIN` is specifying the minimum count of words that we want to keep. In other words, if a word appears fewer than ten times, it is discarded. Again, this is the same as above where we discarded word pairings that appeared fewer than twenty times.

```{r, eval = F}
# ================================ choice parameters
# ================================
WINDOW_SIZE <- 6
DIM <- 300
ITERS <- 100
COUNT_MIN <- 10

```

We next "shuffle" our text. This just means we are randomly reordering the character vector of tweets. 

```{r, eval = F}
# shuffle text
set.seed(42L)
text <- sample(twts_sample$tweet)

```

We then create a list object, tokenizing the text of each tweet within each item of the list. After this, we create the vocabulary object needed to implement the GloVe algorithm. We do this by creating an "itoken" object with `itoken()` and then creating a vocabulary with `create_vocabulary`. We then remove words that do not exceed our specified threshold above with `prune_vocabulary()`. 

```{r, eval = F}
# ================================ create vocab ================================
tokens <- space_tokenizer(text)
it <- itoken(tokens, progressbar = FALSE)
vocab <- create_vocabulary(it)
vocab_pruned <- prune_vocabulary(vocab, term_count_min = COUNT_MIN)  # keep only words that meet count threshold

```

Next up we vectorize our vocabulary and create our term co-occurrence matrix. Again, this is similar to the above where we created a matrix of PMIs for each of the word pairings in our corpus. 

```{r, eval = F}
# ================================ create term co-occurrence matrix
# ================================
vectorizer <- vocab_vectorizer(vocab_pruned)
tcm <- create_tcm(it, vectorizer, skip_grams_window = WINDOW_SIZE, skip_grams_window_context = "symmetric", 
    weights = rep(1, WINDOW_SIZE))

```

Then we set our final model parameters, learning rate and fit the model. This whole process will take some time. To save time when working through this tutorial, you may also download the resulting embedding from the Github repo linked a little further below.


```{r, eval = F}
# ================================ set model parameters
# ================================
glove <- GlobalVectors$new(rank = DIM, x_max = 100, learning_rate = 0.05)

# ================================ fit model ================================
word_vectors_main <- glove$fit_transform(tcm, n_iter = ITERS, convergence_tol = 0.001, 
    n_threads = RcppParallel::defaultNumThreads())


```

Finally, we get the resulting word embedding and save it as a .rds file.

```{r, eval=FALSE}
# ================================ get output ================================
word_vectors_context <- glove$components
glove_embedding <- word_vectors_main + t(word_vectors_context)  # word vectors

# ================================ save ================================
saveRDS(glove_embedding, file = "local_glove.rds")

```

**To save time when working through this tutorial, you may also download the resulting embedding from the Github repo with**:

```{r, eval = F}

url <- "https://github.com/cjbarrie/CTA-ED/blob/main/data/wordembed/local_glove.rds?raw=true"
glove_embedding <- readRDS(url(url, method="libcurl"))

```

## Visualization

```{r, echo = F}

glove_embedding <- readRDS("data/wordembed/local_glove.rds")
```

How do we explore these embeddings? Well, imagine that our embeddings will look something not dissimilar to this visualization of another embedding [here](https://anvaka.github.io/pm/#/galaxy/word2vec-wiki?cx=-17&cy=-237&cz=-613&lx=-0.0575&ly=-0.9661&lz=-0.2401&lw=-0.0756&ml=300&s=1.75&l=1&v=d50_clean). In other words, we are talking about something that doesn't lend itself to projection in 2D space!

But...

All hope is not lost, space travellers. A smart technique by @mcinnes_umap_2020 linked [here](https://arxiv.org/abs/1802.03426) describes a way to reduce the dimensionality of such embedding layers using what is called "Uniform Manifold Approximation and Projection." How do we do this? Well, happily, with the <tt>umap</tt> package it is pretty straightforward!

```{r, eval = F}

# GloVe dimension reduction
glove_umap <- umap(glove_embedding, n_components = 2, metric = "cosine", n_neighbors = 25, min_dist = 0.1, spread=2)

```

```{r, echo=F}

load("data/wordembed/glove_umap.RData")

```

Why is this helpful? Well, for a number of reasons, but it is particularly helpful here for visualizing our embeddings in two-dimensional space. 

```{r}

# Put results in a dataframe for ggplot
df_glove_umap <- as.data.frame(glove_umap[["layout"]])

# Add the labels of the words to the dataframe
df_glove_umap$word <- rownames(df_glove_umap)
colnames(df_glove_umap) <- c("UMAP1", "UMAP2", "word")

# Plot the UMAP dimensions
ggplot(df_glove_umap) +
  geom_point(aes(x = UMAP1, y = UMAP2), colour = 'blue', size = 0.05) +
  ggplot2::annotate("rect", xmin = -3, xmax = -2, ymin = 5, ymax = 7,alpha = .2) +
  labs(title = "GloVe word embedding in 2D using UMAP")

# Plot the shaded part of the GloVe word embedding with labels
ggplot(df_glove_umap[df_glove_umap$UMAP1 < -2.5 & df_glove_umap$UMAP1 > -3 & df_glove_umap$UMAP2 > 5 & df_glove_umap$UMAP2 < 6.5,]) +
      geom_point(aes(x = UMAP1, y = UMAP2), colour = 'blue', size = 2) +
      geom_text(aes(UMAP1, UMAP2, label = word), size = 2.5, vjust=-1, hjust=0) +
      labs(title = "GloVe word embedding in 2D using UMAP - partial view") +
      theme(plot.title = element_text(hjust = .5, size = 14))


# Plot the word embedding of words that are related for the GloVe model
word <- glove_embedding["economy",, drop = FALSE]
cos_sim = sim2(x = glove_embedding, y = word, method = "cosine", norm = "l2")
select <- data.frame(rownames(as.data.frame(head(sort(cos_sim[,1], decreasing = TRUE), 25))))
colnames(select) <- "word"
selected_words <- df_glove_umap %>% inner_join(y=select, by= "word", match = "all") 

#The ggplot visual for GloVe
ggplot(selected_words, aes(x = UMAP1, y = UMAP2)) + 
      geom_point(show.legend = FALSE) + 
      geom_text(aes(UMAP1, UMAP2, label = word), show.legend = FALSE, size = 2.5, vjust=-1.5, hjust=0) +
      labs(title = "GloVe word embedding of words related to 'economy'") +
      theme(plot.title = element_text(hjust = .5, size = 14))
```


We can see, here, then that our embeddings seem to make sense. We zoomed in first on that little outgrowth of our 2D mapping, which seemed to correspond to numbers and number words. Then we looked at words around "economy" and we see other related terms like "growth" and "jobs."

## Exercises

1. Inspect and visualize the nearest neighbour synonyms of other relevant words in the tweets corpus
2. Identify another region of interest in the GloVe-trained model and visualize

<!--chapter:end:16-unsupervised-word-embedding.Rmd-->

# Exercise 7: Sampling text information

## Introduction

The hands-on exercise for this week focuses on how to collect and/or sample text information. 

In this tutorial, you will learn how to:

* Access text information from online corpora
* Query text information using different APIs
* Scrape text information programmatically
* Transcribe text information from audio
* Extract text information from images

## Online corpora

### Replication datasets

There are large numbers of online corpora and replication datasets available to access freely online. We will first access such an example using the `dataverse` package in R, which allows us to download directly from replication data repositories stored at the [Harvard Dataverse](https://dataverse.harvard.edu/).

```{r, message = F, warning = F}
library(dataverse)
library(dplyr)
```

Let's take an example dataset in which we might be interested: the UK parliamentary speech data from 

We first need to set an en environment variable as so. 

```{r}
Sys.setenv("DATAVERSE_SERVER" = "dataverse.harvard.edu")
```

We can then search out the files that we want by specifying the DOI of the publication data in question. We can find this as a series of numbers and letters that come after "https://doi.org/" as shown below. 

![](data/sampling/doi.png){width=100%}

```{r}
dataset <- get_dataset("10.7910/DVN/QDTLYV")
dataset$files[c("filename", "contentType")]
```

We choose to get the UK data from these files, which is listed under "UK_data.csv." We can then download this directly in the following way (this will take some time as the file size is >1GB).

```{r, eval = F}
data <- get_dataframe_by_name(
  "uk_data.csv",
  "10.7910/DVN/QDTLYV",
  .f = function(x) read.delim(x, sep = ","))

```

Of course, we could also download these data manually, by clicking the buttons at the relevant [Harvard Dataverse](https://dataverse.harvard.edu/)---but it is sometimes useful to build in every step of your data collection to your code documentation, making the analysis entirely programatically reproducible from start to finish.

Note as well that we don't have to search out specific datasets that we already know about. We can also use the `dataverse` package to search datasets or dataverses. We can do this very simply in the following way. 

```{r}
search_results <- dataverse_search("corpus politics text", type = "dataset", per_page = 10)

search_results[,1:3]
```

### Curated corpora

There are, of course, many other sources you might go to for text information. I list some of these that might be of interest below:

- Large English-language corpora: [https://www.corpusdata.org/](https://www.corpusdata.org/)
- Wikipedia data dumps: [https://meta.wikimedia.org/wiki/Data_dumps](https://meta.wikimedia.org/wiki/Data_dumps)
  - English version of dumps [here](https://dumps.wikimedia.org/enwiki/)
- Scottish Corpus of Texts & Speech: [https://www.scottishcorpus.ac.uk/](https://www.scottishcorpus.ac.uk/)
- Corpus of Scottish modern writing: [https://www.scottishcorpus.ac.uk/cmsw/](https://www.scottishcorpus.ac.uk/cmsw/)
- The Manifesto Corpus: [https://manifesto-project.wzb.eu/information/documents/corpus](https://manifesto-project.wzb.eu/information/documents/corpus)
- Reddit Pushshift data: [https://files.pushshift.io/reddit/](https://files.pushshift.io/reddit/)
- Mediacloud: [https://mediacloud.org/](https://mediacloud.org/)
  - R package: [https://github.com/joon-e/mediacloud](https://github.com/joon-e/mediacloud)

**Feel free to recommend any further sources and I will add them to this list, which is intended as a growing index of relevant text corpora for social science research!**

## Using APIs

To practice these skills, you might want to create a new account for your academic research. But you needn't create a new account to follow the steps below. You can simply use your own account---if you have one---as using the developer tools will not change anything about your public Twitter account. 

![](data/sampling/twitterdev.png){width=100%}

Before proceeding, we'll load the remaining packages we will need for this tutorial.

```{r, message=F}
library(tidyverse) # loads dplyr, ggplot2, and others
library(rtweet) # to query the Twitter API in R
```

Once you've create your new account, or have logged into your existing one, go to the Twitter developer portal log in page [here](https://developer.twitter.com/en).

Click on Apply in the navigation bar on the top right of the page. You'll be asked "what best describes you?" For the purposes of this tutorial, select academic, and then select student. Fill in all the relevant information and submit your application. Your application will then be reviewed by Twitter before access is granted. This might take hours or days.

Once you have authorization, a new tab will appear in the navigation bar at the top of the develop portal, as below:

![](data/sampling/twitterdev2.png){width=100%}

Navigate to the developer portal and you will there be able to create a new "app" to query the API. You see in my account that I have several apps for different purposes.

![](data/sampling/twitterdev3.png){width=100%}
We can create a new app on this page too. When we click "Create App" we will first be asked to name the app. Most importantly, we will then be given an "API key"; an "API secret key"; and a "Bearer token" as below.

![](data/sampling/twitterdev4.png){width=30%}

You **MUST** make a record of these. Once you have done so, you can then use these to access the API. Once you have recorded these, navigate to the App setting tabs for the App you've created now listed in the Overview tab on the left hand side navigation window.

![](data/sampling/twitterdev5.png){width=50%}

Navigate to "Keys and tokens" on this page, and click generate in the Access token & secret box as below:

![](data/sampling/twitterdev6.png){width=50%}

Record these as well. Once you have all of these keys and tokens recorded somewhere safe, you are ready to collect data!

This is pretty simple using the <tt>rtweet</tt> package. Below, we'll collect the last 50 tweets of the founder of Twitter: Jack Dorsey. 

```{r, eval=FALSE}
api_key <-" XXXXXXXXXXXXXXXXXXXXXXX"
api_key_secret <- "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX"
access_token <- "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX"
access_token_secret <- "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX"

token <- create_token(
  app = "NAME YOUR APP",
  consumer_key = api_key,
  consumer_secret = api_key_secret,
  access_token = access_token,
  access_secret = access_token_secret
)

user <- "@jack"
jacktweets <- get_timeline(user, n = 50)

head(jacktweets)

```

Once you have enter your keys, tokens, and key/token secrets, store them with the `create_token()` function. Here, we are collecting the last 50 tweets for Jack Dorsey, though you can change this by specifying a higher n---be aware, though, that the maximum you are able to collect with the basic API access is 3200 tweets. 

```{r, echo=F, warning=-F, message=F}

library(kableExtra)

jacktweets <- readRDS("data/sampling/jacktweets.rds")

jacktweets <- jacktweets %>%
  select(created_at, screen_name, text)

kbl(jacktweets[1:10,]) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"))
```

Now you can play around with the different API calls possible with the <tt>rtweet</tt> package. See the full documentation [here](https://cran.r-project.org/web/packages/rtweet/rtweet.pdf) and [here](https://github.com/ropensci/rtweet).

And for those interested, you can my package to collect tweets from the Academic Research Product Track API [here](https://github.com/cjbarrie/academictwitteR).

Getting access to the Academic Research Product Track is a bit more complicated but for more information on how to apply see [here](https://developer.twitter.com/en/solutions/academic-research/products-for-researchers).

In order to use the Twitter Academic Research Product Track you will first need to obtain an authorization token. You will find details about the process of obtaining authorization [here](https://developer.twitter.com/en/solutions/academic-research/application-info). 

**In order to gain authorization you first need a Twitter account.**

First, Twitter will ask for details about your academic profile. Per the documentation linked above, they will ask for the following:

> Your full name as it is appears on your institution’s documentation
> 
>   Links to webpages that help establish your identity; provide one or more of the following:
> 
>   - A link to your profile in your institution’s faculty or student directory
>   - A link to your Google Scholar profile
>   - A link to your research group, lab or departmental website where you are listed
> 
>   Information about your academic institution: its name, country, state, and city
> 
>   Your department, school, or lab name
> 
>   Your academic field of study or discipline at this institution
> 
>   Your current role as an academic (whether you are a graduate student, doctoral candidate,       post-doc, professor, research scientist, or other faculty member)

Twitter will then ask for details of the proposed research project. Here, questions include:

> 1. What is the name of your research project?
>
> 2. Does this project receive funding from outside your academic institution? If yes, please list all your sources of funding.
>
> 3. In English, describe your research project. Minimum 200 characters.
>
> 4. In English, describe how Twitter data via the Twitter API will be used in your research project. Minimum 200 characters.
>
> 5. In English, describe your methodology for analyzing Twitter data, Tweets, and/or Twitter users. Minimum 200 characters.
>
> 6. Will your research present Twitter data individually or in aggregate?
>
> 7. In English, describe how you will share the outcomes of your research (include tools, data, and/or other resources you hope to build and share). Minimum 200 characters.
>
> 8. Will your analysis make Twitter content or derived information available to a government entity?

Once you have gained authorization for your project you will be able to see the new project on your Twitter developer portal.

The Academic Research Product Track permits the user to access larger volumes of data, over a far longer time range, than was previously possible. From the Twitter [documentation](https://developer.twitter.com/en/solutions/academic-research/application-info):

> "The Academic Research product track includes full-archive search, as well as increased access and other v2 endpoints and functionality designed to get more precise and complete data for analyzing the public conversation, at no cost for qualifying researchers. Since the Academic Research track includes specialized, greater levels of access, it is reserved solely for non-commercial use".

The new "v2 endpoints" refer to the v2 API, introduced around the same time as the new Academic Research Product Track. Full details of the v2 endpoints are available [here](https://developer.twitter.com/en/docs/twitter-api/early-access).

In summary the Academic Research product track allows the authorized user:

1. Access to the full archive of (as-yet-undeleted) tweets published on Twitter
2. A higher monthly tweet cap (10m--or 20x what was previously possible with the standard v1.1 API)
3. Ability to access these data with more precise filters permitted by the v2 API

If you do get authorization for using the Twitter Academic API, you can then follow the next steps to begin collecting data. Instead of four different keys or secrets, we will have one "bearer token" that we use, associated with one of the apps that we created as above. This is all we then need to begin collecting data. 

First we need to load the package into memory as follows.

```{r}
library(academictwitteR)
```

The first task is set authorization credentials with the `set_bearer()` function, which allows the user to store their bearer token in the .Renviron file.

To do so, use:

```{r, eval=F}
set_bearer()
```

and enter authorization credentials as below:

![](data/sampling/TWITTER_BEARER.gif)

This will mean that the bearer token is automatically called during API calls. It also avoids the inadvisable practice of hard-coding authorization credentials into scripts. 

The workhorse function is `get_all_tweets()`, which is able to collect tweets matching a specific search query or all tweets by a specific set of users.

```{r, eval=F}

tweets <-
  get_all_tweets(
    query = "#BlackLivesMatter",
    start_tweets = "2020-01-01T00:00:00Z",
    end_tweets = "2020-01-05T00:00:00Z",
    file = "blmtweets",
    data_path = "data/",
    n = 1000000,
  )
  
```

Here, we are collecting tweets containing a hashtag related to the Black Lives Matter movement over the period January 1, 2020 to January 5, 2020. 

We have also set an upper limit of one million tweets. When collecting large amounts of Twitter data we recommend including a `data_path` and setting `bind_tweets = FALSE` such that data is stored as JSON files and can be bound at a later stage upon completion of the API query.

```{r, eval=F}

tweets <-
  get_all_tweets(
    users = c("jack", "cbarrie"),
    start_tweets = "2020-01-01T00:00:00Z",
    end_tweets = "2020-01-05T00:00:00Z",
    file = "blmtweets",
    n = 1000
  )
  
```

Whereas here we are not specifying a search query and instead are requesting all tweets by users "jack" and "cbarrie" over the period January 1, 2020 to January 5, 2020. Here, we set an upper limit of 1000 tweets.

The search query and user query arguments can be combined in a single API call as so:

```{r, eval=F}

get_all_tweets(
  query = "twitter",
  users = c("cbarrie", "jack"),
  start_tweets = "2020-01-01T00:00:00Z",
  end_tweets = "2020-05-01T00:00:00Z",
  n = 1000
)

```

Where here we would be collecting tweets by users "jack" and "cbarrrie" over the period January 1, 2020 to January 5, 2020 containing the word "twitter."

```{r, eval=F}

get_all_tweets(
  query = c("twitter", "social"),
  users = c("cbarrie", "jack"),
  start_tweets = "2020-01-01T00:00:00Z",
  end_tweets = "2020-05-01T00:00:00Z",
  n = 1000
)

```

While here we are collecting tweets by users "jack" and "cbarrrie" over the period January 1, 2020 to January 5, 2020 containing the words "twitter" or "social."

Note that the "AND" operator is implicit when specifying more than one character string in the query. See [here](https://developer.twitter.com/en/docs/twitter-api/tweets/search/integrate/build-a-query) for information on building queries for search tweets. Thus, when searching for all elements of a character string, a call may look like:

```{r, eval=F}

get_all_tweets(
  query = c("twitter social"),
  users = c("cbarrie", "jack"),
  start_tweets = "2020-01-01T00:00:00Z",
  end_tweets = "2020-05-01T00:00:00Z",
  n = 1000
)

```

, which will capture tweets containing *both* the words "twitter" and "social." The same logics apply for hashtag queries.

Whereas if we specify our query as separate elements of a character vector like this:

```{r, eval=F}

get_all_tweets(
  query = c("twitter", "social"),
  users = c("cbarrie", "jack"),
  start_tweets = "2020-01-01T00:00:00Z",
  end_tweets = "2020-05-01T00:00:00Z",
  n = 1000
)

```
, this will be capturing tweets by users "cbarrie" or "jack" containing the words "twitter" *or* social. 

Finally, we may wish to query an exact phrase. To do so, we can either input the phrase in escape quotes, e.g., `query ="\"Black Lives Matter\""` or we can use the optional parameter `exact_phrase = T` to search for tweets containing the exact phrase string:

```{r, eval=F}

tweets <-
  get_all_tweets(
    query = "#BlackLivesMatter",
    start_tweets = "2020-01-01T00:00:00Z",
    end_tweets = "2020-01-05T00:00:00Z",
    file = "blmtweets",
    data_path = "data/",
    n = 1000000,
  )
  
```

Here, we are collecting tweets containing a hashtag related to the Black Lives Matter movement over the period January 1, 2020 to January 5, 2020.

And the Twitter API is, of course, not the only API out there!

### Other APIs (R packages)

- [https://cran.r-project.org/web/packages/manifestoR/index.html](https://cran.r-project.org/web/packages/manifestoR/index.html)
- [https://cran.r-project.org/web/packages/vkR/vkR.pdf](https://cran.r-project.org/web/packages/vkR/vkR.pdf)
- [https://cran.r-project.org/web/packages/tuber/index.html](https://cran.r-project.org/web/packages/tuber/index.html)

## Scraping

To practice this skill, we will use a series of webpages on the Internet Archive that host material collected at the Arab Spring protests in Egypt in 2011. The original website can be seen [here](https://www.tahrirdocuments.org/) and below.

![](data/sampling/tahrir_page.png){width=100%}

Before proceeding, we'll load the remaining packages we will need for this tutorial.

```{r, message=F}
library(tidyverse) # loads dplyr, ggplot2, and others
library(ggthemes) # includes a set of themes to make your visualizations look nice!
library(readr) # more informative and easy way to import data
library(stringr) # to handle text elements
library(rvest) #for scraping
```

We can download the final dataset we will produce with:

```{r}
pamphdata <- read_csv("data/sampling/pamphlets_formatted_gsheets.csv")
```

You can also view the formatted output of this scraping exercise, alongside images of the documents in question, in Google Sheets [here](https://docs.google.com/spreadsheets/d/1rg2VTV6uuknpu6u-L5n7kvQ2cQ6e6Js7IHp7CaSKe90/edit?usp=sharing).

If you're working on this document from your own computer ("locally") you can download the Tahrir documents data in the following way:

```{r, eval = F}
pamphdata <- read_csv("https://github.com/cjbarrie/CTA-ED/blob/main/data/sampling/pamphlets_formatted_gsheets.csv")

```

Let's have a look at what we will end up producing:


```{r}
head(pamphdata)
```

We are going to return to the Internet Archived webpages to see how we can produce this final formatted dataset. The archived Tahrir Documents webpages can be accessed [here](https://wayback.archive-it.org/2358/20120130143023/http://www.tahrirdocuments.org/).

We first want to expect how the contents of each webpage is stored.

When we scroll to the very bottom of the page, we see listed a number of hyperlinks to documents stored by month:

![](data/sampling/tahrir_archives.png)

We will click through the documents stored for March and then click on the top listed pamphlet entitled "The Season of Anger Sets in Among the Arab Peoples." You can access this [here](https://wayback.archive-it.org/2358/20120130161341/http://www.tahrirdocuments.org/2011/03/voice-of-the-revolution-3-page-2/).

We will store this url to inspect the HTML it contains as follows:

```{r}
url <- "https://wayback.archive-it.org/2358/20120130161341/http://www.tahrirdocuments.org/2011/03/voice-of-the-revolution-3-page-2/"

html <- read_html(url)

html
```

Well, this isn't particularly useful. Let's now see how we can extract the text contained inside.

```{r}
pagetext <- html %>%
  html_text()

pagetext
```

Well this looks pretty terrifying now...

We need a way of quickly identifying where the relevant text is so that we can specify this when we are scraping. The most widely-used tool to achieve this is the "Selector Gadget" Chrome Extension. You can add this to your browser for free [here](https://chrome.google.com/webstore/detail/selectorgadget/mhjhnkcfbdhnjickkkdbjoemdmbfginb?hl=en). 

The tool works by allowing the user to point and click on elements of a webpage (or "CSS selectors"). Unlike alternatives, such as "Inspect Element" browser tools, we are easily able to see how the webpage item is contained within CSS selectors (rather than HTML tags alone), which is easier to parse. 

We can do this with our Tahrir documents as below:

![](data/sampling/gifcap4.gif){width=100%}

So now we know that the main text of the translated document is contained between "p" HTML tags. To identify the text between these HTML tags we can run:

```{r}
pagetext <- html %>%
  html_elements("p") %>%
  html_text(trim=TRUE)

pagetext
  
```

, which looks quite a lot more manageable...! 

What is happening here? Essentially, the `html_elements()` function is scanning the page and collecting all HTML elements contained between `<p>` tags, which we collect using the "p" CSS selector. We are then just grabbing the text contained in this part of the page with the `html_text()` function. 

So this gives us one way of capturing the text, but what about if we wanted to get other elements of the document, for example the date or the tags attributed to each document? Well we can do the same thing here too. Let's take the example of getting the date:

![](data/sampling/gifcap5.gif){width=100%}

We see here that the date is identified by the ".calendar" CSS selector and so we enter this into the same `html_elements()` function as before:

```{r}
pagedate <- html %>% 
  html_elements(".calendar") %>%
  html_text(trim=TRUE)

pagedate
  
```

Of course, this is all well and good, but we also need a way of doing this at scale---we can't just keep repeating the same process for every page we find as this wouldn't be much quicker than just copy pasting. So how can we do this? Well we need first to understand the URL structure of the website in question.

When we scroll down the page we see listed a number of documents. Each of these directs to an individual pamphlet distributed at protests during the 2011 Egyptian Revolution. 

Click on one of these and see how the URL changes.

We see that if our starting URL was:

```{r, echo=F}
starturl <- "https://wayback.archive-it.org/2358/20120130135111/http://www.tahrirdocuments.org/"
```

```{r, echo=F, comment=NA}
cat(starturl)
```

Then if we click on March 2011, the first month for which we have documents, we see that the url becomes:

```{r, echo=F}
marchurl <- "https://wayback.archive-it.org/2358/20120130143023/http://www.tahrirdocuments.org/2011/03/"
 
augusturl <- "https://wayback.archive-it.org/2358/20120130142155/http://www.tahrirdocuments.org/2011/08/"

jan2012url <- "https://wayback.archive-it.org/2358/20120130142014/http://www.tahrirdocuments.org/2012/01/"

```

```{r, echo=F, comment=NA}
cat(marchurl)
```

, for August 2011 it becomes:

```{r, echo=F, comment=NA}
cat(augusturl)
```

, and for January 2012 it becomes:

```{r, echo=F, comment=NA}
cat(jan2012url)
```

We notice that for each month, the URL changes with the addition of month and year between back slashes at the end or the URL. In the next section, we will go through how to efficiently create a set of URLs to loop through and retrieve the information contained in each individual webpage.

We are going to want to retrieve the text of documents archived for each month. As such, our first task is to store each of these webpages as a series of strings. We could do this manually by, for example, pasting year and month strings to the end of each URL for each month from March, 2011 to January, 2012:

```{r}
url <- "https://wayback.archive-it.org/2358/20120130143023/http://www.tahrirdocuments.org/"

url1 <- paste0(url,"2011/03/")
url2 <- paste0(url,"2011/04/")
url3 <- paste0(url,"2011/04/")

#etc...

urls <- c(url1, url2, url3)
    
```

But this wouldn't be particularly efficient...

Instead, we can wrap all of this in a loop. 

```{r}
urls <- character(0)
for (i in 3:13) {
  url <- "https://wayback.archive-it.org/2358/20120130143023/http://www.tahrirdocuments.org/"
  newurl <- ifelse(i <10, paste0(url,"2011/0",i,"/"), 
                   ifelse(i>=10 & i<=12 , paste0(url,"2011/",i,"/"), 
                          paste0(url,"2012/01/")))
  urls <- c(urls, newurl)
}
```

What's going on here? Well, we are first specifying the starting URL as above. We are then iterating through the numbers 3 to 13. And we are telling R to take the new URL and then, depending on the number in the loop we are on, to take the base starting url--- `r url` --- and to paste on the end of it the string "2011/0", then the number of the loop we are on, and then "/". So, for the first "i" in the loop---the number 3---then we are effectively calling the equivalent of:

```{r}

i <- 3

url <- "https://wayback.archive-it.org/2358/20120130143023/http://www.tahrirdocuments.org/"

newurl <- paste0(url,"2011/0",i,"/")
```

Which gives:

```{r, echo=F}
newurl
```

In the above, the `ifelse()` commands are simply telling R: if i (the number of the loop we are on) is less than 10 then `paste0(url,"2011/0",i,"/")`; i.e., if i is less than 10 then paste "2011/0", then "i" and then "/". So for the number 3 this becomes:

`"https://wayback.archive-it.org/2358/20120130143023/http://www.tahrirdocuments.org/2011/03/"` 

, and for the number 4 this becomes 

`"https://wayback.archive-it.org/2358/20120130143023/http://www.tahrirdocuments.org/2011/04/"`

If, however, `i>=10 & i<=12` (i is greater than or equal to 10 and less than or equal to 12) then we are calling `paste0(url,"2011/",i,"/")` because here we do not need the first "0" in the months. 

Finally, if (else) i is greater than 12 then we are calling `paste0(url,"2012/01/")`. For this last call, notice, we do not have to specify whether i is greater than or equal to 12 because we are wrapping everything in `ifelse()` commands. With `ifelse()` calls like this, we are telling R if x "meets condition" then do y, otherwise do z. When we are wrapping multiple `ifelse()` calls within each other, we are effectively telling R if x "meets condition" then do y, or if x "meets other condition" then do z, otherwise do a. So here, the "otherwise do a" part of the `ifelse()` calls is saying: if i is not less than 10, and is not between 10 and 12, then paste "2012/01/" to the end of the URL.

Got it? I didn't even get it on first reading... and I wrote it. The best way to understand what is going on is to run this code yourself and look at what each part is doing.

So now we have our list of URLs for each month. What next?

Well if we go onto the page of a particular month, let's say March, we will see that the page has multiple paginated tabs at the bottom. Let's see what happens to the URL when we click on one of these:

```{r, echo=F}
marchurl <- "https://wayback.archive-it.org/2358/20120130143023/http://www.tahrirdocuments.org/2011/03/"
marchurlp2 <- "https://wayback.archive-it.org/2358/20120130163651/http://www.tahrirdocuments.org/2011/03/page/2/"
marchurlp3 <- "https://wayback.archive-it.org/2358/20120130163651/http://www.tahrirdocuments.org/2011/03/page/3/"
```

We see that if our starting point URL for March, as above, was:

```{r, echo=F, comment=NA}
cat(marchurl)
```

When we click through to page 2 it becomes:

```{r, echo=F, comment=NA}
cat(marchurlp2)
```

And for page 3 it becomes:

```{r, echo=F, comment=NA}
cat(marchurlp3)
```

We can see pretty clearly that as we navigate through each page, there appears appended to the URL the string "page/2/" and "page/3/". So this shouldn't be too tricky to add to our list of URLs. But we want to avoid having to manually click through the archive for each month to figure out how many pagination tabs are at the bottom of each page. 

Fortunately, we don't have to. Using the "Selector Gadget" tool again we can automate this process by grabbing the highest number that appears in the pagination bar for each month's pages. The code below achieves this:

```{r, eval=F}

urlpages_all <- character(0) #create empty character string to deposit our final set of urls
urlpages <- character(0) #create empty character string to deposit our urls for each page of each month
for (i in seq_along(urls)) { #for loop for each url stored above
  url <- urls[i] #take the first url from the vector of urls created above
  html <- read_html(url) #read the html
  pages <- html %>%
    html_elements(".page") %>% #grab the page element
    html_text() #convert to text
  pageints <- as.integer(pages) #convert to set of integers
  npages <- max(pageints, na.rm = T) #get number of highest integer
  
  for (j in 1:npages) { #for loop for each of 1:highest page integer for that month's url
  newurl <- paste0(url,"page/",j,"/") #create new url by pasting "page/" and then the number of that page, and then "/", matching the url structure identified above
  urlpages <- c(urlpages, newurl) #bind with previously created page urls for each month
  }
  urlpages_all <- c(urlpages_all, urlpages) #bind the monthly page by page urls together
  urlpages <- character(0) #empty urlpages for next iteration of the first for loop
  urlpages_all <- gsub("page/1/", "", urlpages_all) #get rid of page/1/ as not needed
}
```

```{r, echo=F}
urlpages_all <- readRDS("data/sampling/urlpages_all.RDS")
```

What's going on here? Well, in the first two lines, we are simply creating an empty character string that we're going to populate in the subsequent loop. Remember that we have a set of eleven starting URLs for each of months archived on this webpage. 

So in the code beginning `for (i in seq_along(files)` we saying, similar to above, for the beginning url to the end url, do the following in a loop: first, read in the url with `url <- urls[i]` then read the html it contains with   `html <- read_html(url)`.

After this line, we are getting the pages as a character vector of page numbers by calling the `html_elements()` function on the ".page" tag. this gives a series of pages stored as e.g. "1" "2" "3". 

In order to be able to see how many there are, we need to extract the highest number that appears in this string. To do this, we first need to reformat it as an "integer" object rather than a "character" object so that R can recognize that these are numbers. So we call `pageints <- as.integer(pages)`. Then we get the maximum by simply calling: `npages <- max(pageints, na.rm = T)`. 

In the next part of the loop, we are taking the new information we have stored as "npages," i.e., the number of pagination tabs for each month, and telling R: for each of these pages, define a new url by adding "page/" then the number of the pagination tab "j", and then "/". After we've bound all of these together, we get a list of URLs that look like this:

```{r}
head(urlpages_all)
```

So what next?

The next step is to get the URLs for each of the documents contained in the archive for each month. How do we do this? Well, we can once again use the "Selector Gadget" tool to work this out. For the main landing pages of each month, we see listed, as below, each document in a list. For each of these documents, we see that the title, which links to the revolutionary leaflet in question, has two CSS selectors: "h2" and ".post".

![](data/sampling/gifcap6.gif){width=100%}

We can again pass these tags through `html_elements()` to grab what's contained inside. We can then grab what's contained inside these by extracting the "children" of these classes. In essence, this just means a lower level tag: tags can have tags within tags and these flow downwards like a family tree (hence the name, I suppose). 

So one of the "children" of this HTML tag is the link contained inside, which we can get with calling `html_children()` followed by specifying that we want the specific attribute of the web link it encloses with `html_attr("href")`. The subsequent lines then just remove extraneous information. 

The complete loop, then, to retrieve the URL of the page for every leaflet contained on this website is:


```{r, eval =F}
#GET URLS FOR EACH PAMPHLET

pamlinks_all <- character(0)
for (i in seq_along(urlpages_all)) {
  url <- urlpages_all[i]
  html <- read_html(url)
  links <- html_elements(html, ".post , h2") %>%
    html_children() %>%
    html_attr("href") %>%
    na.omit() %>%
    `attributes<-`(NULL)
  pamlinks_all <- c(pamlinks_all, links)
}

```

```{r, echo=F}
pamlinks_all <- readRDS("data/sampling/pamlinks_all.RDS")
```

Which gives us:

```{r}
head(pamlinks_all)

length(pamlinks_all)
```

We see now that we have collected all 523 separate URLs for every revolutionary leaflet contained on these pages. Now we're in a great position to be able to crawl each page and collect the information we need. This final loop is all we need to go through each URL we're interested in and collect relevant information on document text, title, date, tags, and the URL to the image of the revolutionary literature itself.

See if you can work out yourselves how each part of this is fitting together. NOTE: if you want to run the final loop on your own machines it will take several hours to complete. 

```{r, eval=F}

df_empty <- data.frame()
for (i in seq_along(pamlinks_all)) {
  url <- pamlinks_all[i]
  html <- read_html(url)
  cat("Collecting url number ",i,": ", url, "\n")
  
  error <- tryCatch(html <- read_html(url),
                    error=function(e) e)
  if (inherits(error, 'error')) {
    df <- data.frame(title = NA,
                     date = NA,
                     text = NA,
                     imageurl = NA,
                     tags = NA)
    next
  }
  
  df <- data.frame(matrix(ncol=0, nrow=length(1)))
  #get titles
  titles <- html_elements(html, ".title") %>%
    html_text(trim=TRUE)
  
  title <- titles[1]
  df$title <- title
  
  #get date
  date <- html_elements(html, ".calendar") %>%
    html_text(trim=TRUE)
  df$date <- date
  
  #get text
  textsep <-  html_elements(html, "p") %>%
    html_text(trim=TRUE)
  text <- paste(textsep, collapse = ",")
  df$text <- text
  
  #get tags
  pamtags <- html_elements(html, ".category") %>%
    html_text(trim=TRUE)
  df$tags <- pamtags
  
  #get link to original pamphlet image
  elements_other <-  html_elements(html, "a") %>%
    html_children()
  url_element <- as.character(elements_other[2])
  imgurl <- str_extract(url_element, "src=\\S+")
  imgurl <- substr(imgurl, 6, (nchar(imgurl)-1))
  
  df$imageurl <- imgurl
  
  df_empty <- rbind(df_empty, df)
}


```

And now... we're pretty much there...back where we started! 

## Speech to text

There are some R packages that allow the user to connect to the Google Speech-to-Text engine (billing information required to access). You can find such a package [here](https://github.com/ropensci/googleLanguageR) but it doesn't seem to be readily maintained. There is a far wider range of options if you are open to using Python: see, e.g.: [here](https://realpython.com/python-speech-recognition/). 

Here, we're instead going to do the next best thing: capture text that has already been converted from speech. Here, we're going to be collecting the automated English captioning from YouTube videos. For this, we will use the R package `youtubecaption`. Because this package connects to an already existing Python package under the hood, you'll need to have a Conda environment installed. You can download Anaconda Individual Edition [here](https://www.anaconda.com/products/individual).

And to grab the text from a video is as simple as below.

```{r, eval = F}
library(youtubecaption)

url <- "https://www.youtube.com/watch?v=cpbtcsGE0OA"
caption <- get_caption(url)
caption

```

And if you wanted to the same thing in Python you would do the following. 

```{python, eval = F}
!pip install youtube_transcript_api

from youtube_transcript_api import YouTubeTranscriptApi

YouTubeTranscriptApi.get_transcript("cpbtcsGE0OA")
```

## OCR

The last major technique that might be of interest for extracting text is Optical Character Recognition (OCR). This is a technique that reads in image files (e.g., .jpg or .png or .pdf) automatically extracts the text contained therein. 

A very good package in R for extracting text from images is `daiR`, details of which can be found [here](https://cran.r-project.org/web/packages/daiR/index.html). This connects again to the Google Cloud Engine (and thus requires billing info.) but it won't charge you until you've spent your first free $300 of credit. This will take a while to spend so you have a good deal amount of tokens before you start incurring expenses.

You can consult [here](https://dair.info/articles/setting_up_google_storage.html) for info. on setting up your Google Cloud service account. \

Once you have done this you'll be ready to connect to the Google Cloud Engine. 




<!--chapter:end:17-sampling-text.Rmd-->

# Exercise 9: Validation

## Introduction

The hands-on exercise for this week focuses on validating text analysis techniques. We'll focus on articles and data that we have already encountered in weeks past and investigate how the authors went about validating their findings.

In this tutorial, you will learn how to:

* Manually review 
* Compare to reference data
* Conduct tests for unsupervised approaches
* Conduct tests for supervised approaches

## Setup

Before proceeding, we'll load the packages we will need for this tutorial.

```{r, echo=F}
library(rmarkdown)
library(shiny)
```

```{r, message=F}
library(ggplot2)
library(readr)
library(dplyr)
library(tidytext)
library(stringr)
library(quanteda) 
library(quanteda.textmodels) 
library(oolong) #for topic model validation tasks
```

## Manual review

A first step in any text analysis technique should be to eyeball your data and the results of any analysis you carry out. This might not sound very formalized as an approach---and it isn't---but it is what every good practitioner will find themselves doing at some stage. 

Some refer to this kind of check as a "sanity check." What would such a check look like in practice?

Well, let's take an example from the beginning of the course, where we looked at event descriptions from the Edinburgh International Book Festival. 

We'll first read in this data.

```{r}
edbfdata <- read_csv("data/wordfreq/edbookfestall.csv")
```

As ever, if you're working on this document from your own computer ("locally") you can download the Edinburgh Fringe data in the following way:

```{r, eval = F}
edbfdata <- read_csv("https://github.com/cjbarrie/CTA-ED/blob/main/data/wordfreq/edbookfestall.csv"
```

We'll get the data into tidy format as before, tagging individual words for whether or not they appear in our dictionary of woman- and gender-related terms. One addition here is that we're ascribing each event an individual ID, which will be useful in the next step. 

```{r}
evdes <- edbfdata %>%
  mutate(ID = paste0("EVENT",seq(1:nrow(edbfdata)))) %>%
  select(description, year, ID)

remove_reg <- c("&amp;","&lt;","&gt;","<p>", "</p>","&rsquo", "&lsquo;",  "&#39;", "<strong>", "</strong>", "rsquo", "em", "ndash", "nbsp", "lsquo", "strong", "<p>\n\t")

tidy_des <- evdes %>% 
  mutate(desc = tolower(description)) %>%
  unnest_tokens(word, desc) %>%
  filter(!word %in% remove_reg)

tidy_des$womword <- as.integer(grepl("women|feminist|feminism|gender|harassment|sexism|sexist", 
                                            x = tidy_des$word))

```


Now we can summarise which event IDs have one or more words that appear in our dictionary of reference terms. 

```{r}
tidy_agg <- tidy_des %>%
  group_by(ID) %>%
  summarise(womwords = sum(womword))

womarts <- tidy_agg %>%
  filter(womwords>=1) %>%
  pull(ID)

womdes <- evdes %>%
  filter(ID %in% womarts)

womdes$description <- gsub("<p>\n\t", "", womdes$description)
```

We can then  inspect these. In this instance we have several hundred examples. This kind of number is amenable to manual inspection in full. If that number were larger, we might take a random sample of these articles and inspect this random subsample. 

```{r}
paged_table(womdes)
```

We can now easily inspect these articles and determine whether we are capturing our target concept of interest. Additionally, we may wish to add additional codes to these descriptions based on some qualitative coding procedure. 

## Reference comparison

### Word scaling example

A second technique for determining the validity of data involves comparing to external data with which we might expect to see a close comparison---if our text-based measure is precisely estimated.

A good example of this comes from the week on scaling techniques. Here, the article by @kaneko_estimating_2021 took editorial texts from newspapers to determine whether we could estimate the ideological outlook of these newspapers. In order to determine whether these measures are accurate, however, we need to compare them to some reference data. 

For newspapers, this information might come from expert surveys scoring the bias of newspapers or similar.

We can access data from the first study by @kaneko_estimating_2021 in the following way. 

```{r}
kaneko_dfm <- readRDS("data/wordscaling/study1_kaneko.rds")
```

If you're working locally, you can download the `dfm` data with:

```{r, eval = F}
kaneko_dfm  <- readRDS(gzcon(url("https://github.com/cjbarrie/CTA-ED/blob/main/data/wordscaling/study1_kaneko.rds?raw=true")))
```

As in the previous exervise using these data, the text is in the form a document-feature-matrix. We again remove infrequent words and group at the level of newspaper.

```{r}
table(docvars(kaneko_dfm, "Newspaper"))
## prepare the newspaper-level document-feature matrix
# compress the document-feature matrix at the newspaper level
kaneko_dfm_study1 <- dfm_group(kaneko_dfm, groups = Newspaper)
# remove words not used by two or more newspapers
kaneko_dfm_study1 <- dfm_trim(kaneko_dfm_study1, min_docfreq = 2, docfreq_type = "count")

## size of the document-feature matrix
dim(kaneko_dfm_study1)

```
We'll then estimate the Wordfish model in the same way as in the article by @kaneko_estimating_2021.

```{r}
#### estimate the Wordfish model ####
set.seed(12345)
Study1.wordfish.result <- textmodel_wordfish(kaneko_dfm_study1, sparse = TRUE)

## positions of newspapers
wordfishscores <- data.frame(Newspaper = Study1.wordfish.result$docs, 
                                  score = Study1.wordfish.result$theta)

wordfishscores$method <- "Wordfish"

```

The replication materials for the @kaneko_estimating_2021 article also include the human-coded positions, which can be accessed as follows.

```{r}
# human-coded positions of the newspapers
humancoded.data <- read.csv("data/wordscaling/humancoded_positions.csv")
```

Or you can download these locally as follows.

```{r, eval = F}
humancoded.data  <- readRDS(gzcon(url("https://github.com/cjbarrie/CTA-ED/blob/main/data/wordscaling/humancoded_positions.csv?raw=true")))
```

We take the mean of the human-coded positions, aggregating across dates and coders for all newspapers. 
```{r}
## human-coded positions
# average of editorial positions coded by three coders
humancoded.data$Ideology <- (humancoded.data$CoderA + 
                               humancoded.data$CoderB + 
                               humancoded.data$CoderC) / 3

humanscores <- humancoded.data %>%
  group_by(Newspaper) %>%
  summarise(score = mean(Ideology))

humanscores$method <- "Human-coded"
```

We then bind our Wordfish and human-coded scores into one data frame and we are ready to plot. 

```{r}
wordfish.human.positions <- rbind(wordfishscores, humanscores)

wordfish.human.positions %>%
  ggplot() +
  geom_point(aes(Newspaper, score, color = method)) + 
  coord_flip() +
  theme_minimal()
```

## Unsupervised learning checks

In the example worksheet for topic modelling, we encountered several tests included in the `preText` package to validate pre-processing steps for our topic models. 

Another article, by @ying_topics_2021, describes several additional tests we might conduct to validate our model output. 

These include:

1. Word lists
2. Fit statistics
3. Additional (qualitative) coding
4. Human coders
  - Word intrusion (identify the most irrelevant word)
  - Word set intrusion (identify lowest probability topic word sets for a document)
  - Label intrusion
  - Optimal label selection
  
A very good package for implementing some of these procedures is `oolong` by  Chung-Hong Chan.

You can find details of the package [here](https://www.theoj.org/joss-papers/joss.02461/10.21105.joss.02461.pdf) and [here](https://cran.r-project.org/web/packages/oolong/oolong.pdf). For intrusion-type tasks, the package allows the user to easily launch interactive windows to complete the tasks and record the data.
 
We take here an example from scientific abstracts data bundled in the package. We then generate a "Shiny" app---basically an R-based form of interactice UIs. 

```{r, eval = F}
wsi_test <- wsi(abstracts_keyatm)
wsi_test
export_oolong(wsi_test, dir = "data/validation/wsi_test/", use_full_path = FALSE)
```

To launch the the Shiny app generated by the `export_oolong()` call, we simply do the following, pointing to the same directory we used to save our exported oolong app. 

```{r, eval = FALSE}
shiny::runApp("data/validation//wsi_test")
```

And we should see a window that looks something like this. 

![](data/validation/oolong.png){width=50%}

<!--chapter:end:19-validation.Rmd-->

# Assessment data

## Introduction

Below you will find a series of datasets. You can choose to use these for the summative assessment. Alternatively, you can contact me with a suggestion of a dataset and a relevant research question. See the [Course Overview](https://cjbarrie.github.io/CTA-ED/course-overview.html) page for full details of the assessment.

## @osnabrugge_playing_2021 data

We can access data from @osnabrugge_playing_2021 [here](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/QDTLYV)

To prepare these data, we can use the same code as used by the original authors:

```{r, eval= FALSE}
library("ggplot2")
library("plyr")
library("gdata")
library("stringr")
library("data.table")

## Prep Osnabrugge et al. 

data = fread("/Users/cbarrie6/Dropbox/Teaching/Edinburgh/teaching/CTA_21-22/assessment/data/uk_data.csv", encoding="UTF-8")


data$date = as.Date(data$date)


#Table 2: Examples: Emotive and neutral speeches
example1 = subset(data, id_speech==854597)
example1$emotive_rhetoric
example1$text

example2 = subset(data, id_speech==778143)
example2$emotive_rhetoric
example2$text

#Create time variable
data$time= NA
data$time[data$date>=as.Date("2001-01-01") & data$date<=as.Date("2001-06-30")] = "01/1"
data$time[data$date>=as.Date("2001-07-01") & data$date<=as.Date("2001-12-31")] = "01/2"
data$time[data$date>=as.Date("2002-01-01") & data$date<=as.Date("2002-06-30")] = "02/1"
data$time[data$date>=as.Date("2002-07-01") & data$date<=as.Date("2002-12-31")] = "02/2"
data$time[data$date>=as.Date("2003-01-01") & data$date<=as.Date("2003-06-30")] = "03/1"
data$time[data$date>=as.Date("2003-07-01") & data$date<=as.Date("2003-12-31")] = "03/2"
data$time[data$date>=as.Date("2004-01-01") & data$date<=as.Date("2004-06-30")] = "04/1"
data$time[data$date>=as.Date("2004-07-01") & data$date<=as.Date("2004-12-31")] = "04/2"
data$time[data$date>=as.Date("2005-01-01") & data$date<=as.Date("2005-06-30")] = "05/1"
data$time[data$date>=as.Date("2005-07-01") & data$date<=as.Date("2005-12-31")] = "05/2"
data$time[data$date>=as.Date("2006-01-01") & data$date<=as.Date("2006-06-30")] = "06/1"
data$time[data$date>=as.Date("2006-07-01") & data$date<=as.Date("2006-12-31")] = "06/2"
data$time[data$date>=as.Date("2007-01-01") & data$date<=as.Date("2007-06-30")] = "07/1"
data$time[data$date>=as.Date("2007-07-01") & data$date<=as.Date("2007-12-31")] = "07/2"
data$time[data$date>=as.Date("2008-01-01") & data$date<=as.Date("2008-06-30")] = "08/1"
data$time[data$date>=as.Date("2008-07-01") & data$date<=as.Date("2008-12-31")] = "08/2"
data$time[data$date>=as.Date("2009-01-01") & data$date<=as.Date("2009-06-30")] = "09/1"
data$time[data$date>=as.Date("2009-07-01") & data$date<=as.Date("2009-12-31")] = "09/2"
data$time[data$date>=as.Date("2010-01-01") & data$date<=as.Date("2010-06-30")] = "10/1"
data$time[data$date>=as.Date("2010-07-01") & data$date<=as.Date("2010-12-31")] = "10/2"
data$time[data$date>=as.Date("2011-01-01") & data$date<=as.Date("2011-06-30")] = "11/1"
data$time[data$date>=as.Date("2011-07-01") & data$date<=as.Date("2011-12-31")] = "11/2"
data$time[data$date>=as.Date("2012-01-01") & data$date<=as.Date("2012-06-30")] = "12/1"
data$time[data$date>=as.Date("2012-07-01") & data$date<=as.Date("2012-12-31")] = "12/2"
data$time[data$date>=as.Date("2013-01-01") & data$date<=as.Date("2013-06-30")] = "13/1"
data$time[data$date>=as.Date("2013-07-01") & data$date<=as.Date("2013-12-31")] = "13/2"
data$time[data$date>=as.Date("2014-01-01") & data$date<=as.Date("2014-06-30")] = "14/1"
data$time[data$date>=as.Date("2014-07-01") & data$date<=as.Date("2014-12-31")] = "14/2"
data$time[data$date>=as.Date("2015-01-01") & data$date<=as.Date("2015-06-30")] = "15/1"
data$time[data$date>=as.Date("2015-07-01") & data$date<=as.Date("2015-12-31")] = "15/2"
data$time[data$date>=as.Date("2016-01-01") & data$date<=as.Date("2016-06-30")] = "16/1"
data$time[data$date>=as.Date("2016-07-01") & data$date<=as.Date("2016-12-31")] = "16/2"
data$time[data$date>=as.Date("2017-01-01") & data$date<=as.Date("2017-06-30")] = "17/1"
data$time[data$date>=as.Date("2017-07-01") & data$date<=as.Date("2017-12-31")] = "17/2"
data$time[data$date>=as.Date("2018-01-01") & data$date<=as.Date("2018-06-30")] = "18/1"
data$time[data$date>=as.Date("2018-07-01") & data$date<=as.Date("2018-12-31")] = "18/2"
data$time[data$date>=as.Date("2019-01-01") & data$date<=as.Date("2019-06-30")] = "19/1"
data$time[data$date>=as.Date("2019-07-01") & data$date<=as.Date("2019-12-31")] = "19/2"

data$time2 = data$time
data$time2 = str_replace(data$time2, "/", "_")

data$stage = 0
data$stage[data$m_questions==1]= 1
data$stage[data$u_questions==1]= 2
data$stage[data$queen_debate_others==1]= 3
data$stage[data$queen_debate_day1==1]= 4
data$stage[data$pm_questions==1]= 5

```

Below, I display a sample of these data.

```{r, echo=FALSE}

data <- readRDS("data/assessment/osnabrugge_samp.rds")

```

```{r, echo=F}

data <- data[1:3,]

data %>%
  select(id_speech, text, last_name, first_name, date, government, female, age) %>%
  kbl() %>%
  kable_styling(bootstrap_options = "striped")
```

If the full dataset is too large for your machines, you can easily take a sample of it with:

```{r, eval=F}

data_samp <- data %>%
  sample_n(10000)

```

##  Twitter Transparency data

Select a dataset/datasets of interest from the Twitter Transparency archive [here](https://transparency.twitter.com/en/reports/information-operations.html). These are datasets that have been flagged for "information operations" activity; that is, activity designed to distort, often through automated messaging, the information landscape to the benefit of a given entity (normally a government). 

The datasets are all listed and downloadable in ".csv" format if you scroll down to "03. Download Archive." Here, you will just be asked to enter your email address as agreement to Terms of Use. 

## @michalopoulos_folklore_2021

Data from the article by @michalopoulos_folklore_2021 on Folkloric belief using text data derived from ethnographic materials, which can be downloaded [here](https://dataverse.harvard.edu/dataset.xhtml;jsessionid=8ff772a5d630bc91f1e9c543e537?persistentId=doi%3A10.7910%2FDVN%2FIXOHKB&version=&q=&fileTypeGroupFacet=&fileAccess=&fileSortField=name&fileSortOrder=desc). 

We can read the main "Motif" dataset from this dataset (stored in Stata .dta format) as follows. 

```{r}
library(haven)

folklore <- read_dta("data/assessment/Motif_Master.dta")

head(folklore[,1:5])
```

## R Markdown

You can access a template R Markdown response for your code from the Github repo for this book by clicking this [link](https://raw.githubusercontent.com/cjbarrie/CTA-ED/main/data/assessment/CTA_example.Rmd?raw=true) and download the word document it outputs by clicking this [link](https://github.com/cjbarrie/CTA-ED/blob/main/data/assessment/CTA_example.docx?raw=true).

Though you **should submit the R markdown output in word** you can also see what it looks like when generated as html [here](https://raw.githack.com/cjbarrie/CTA-ED/main/data/assessment/CTA_example.html).

<!--chapter:end:20-assessment-data.Rmd-->

`r if (knitr:::is_html_output()) '# References'`

<!--chapter:end:99-references.Rmd-->

