[{"path":"index.html","id":"computational-text-analysis-pgsp11584","chapter":"“Computational Text Analysis” (PGSP11584)","heading":"“Computational Text Analysis” (PGSP11584)","text":"dedicated webpage course Computational Text Analysis\" (PGSP11584) University Edinburgh, taught Christopher Barrie. Go Course Overview Introduction tabs course overview introduction R.","code":""},{"path":"course-overview.html","id":"course-overview","chapter":"Course Overview","heading":"Course Overview","text":"recent years, use computational techniques quantitative analysis text exploded. volume quantity text data now access digital age enormous. led social scientists seek new means analyzing text data scale.see text records, form digital traces left social media platforms, archived works literature, parliamentary speeches, video transcripts, print news, can help us answer huge range important questions.","code":""},{"path":"course-overview.html","id":"learning-outcomes","chapter":"Course Overview","heading":"Learning outcomes","text":"course give students training use computational text analysis techniques. course prepare students dissertation work uses textual data provide hands-training use R programming language () Python.course provide venue seminar discussion examples using methods empirical social sciences well lectures technical /statistical dimensions application.","code":""},{"path":"course-overview.html","id":"course-structure","chapter":"Course Overview","heading":"Course structure","text":"using online book ten-week course “Computational Text Analysis” (PGSP11584). chapter contains readings week. book also includes worksheets example code conduct text analysis techniques discuss week.week (partial exception week 1), discussing, alternately, substantive technical dimensions published research empirical social sciences. readings week generally contain two “substantive” readings—, examples application text analysis techniques empirical data—one “technical” reading focuses mainly statistical computational aspects given technique.study first technical aspects analytical approaches , second, substantive dimensions applications. means , discussing readings, able discuss satisfactory given approach illuminating question topic hand.Lectures primarily focused technical dimensions given technique. seminar (Q&) follows give us opportunity study discuss questions social scientific interest, computational text analysis used answer .","code":""},{"path":"course-overview.html","id":"course-pre-preparation","chapter":"Course Overview","heading":"Course pre-preparation","text":"NOTE: lecture Week 2, students complete two introductory R exercises. First, consult worksheet , introduction setting understanding basics working R. Second, Ugur Ozdemir provided comprehensive introductory R course Research Training Centre University Edinburgh can follow instructions access .","code":""},{"path":"course-overview.html","id":"reference-sources","chapter":"Course Overview","heading":"Reference sources","text":"several reference texts use course:Wickham, Hadley Garrett Grolemund. R Data Science: https://r4ds..co.nz/Silge, Julia David Robinson. Text Mining R: https://www.tidytextmining.com/\nlearning tidytext, online tutorial used: https://juliasilge.shinyapps.io/learntidytext/\nlearning tidytext, online tutorial used: https://juliasilge.shinyapps.io/learntidytext/(later course) Hvitfelft, Emil Julia Silge. Supervised Machine Learning Text Analysis R: https://smltar.com/several weeks, also referring two textbooks, available online, information retrieval text processing. :Jurafsky, Dan James H. Martin. Speech Language Processing (3rd ed. draft): https://nlp.stanford.edu/IR-book/information-retrieval-book.htmlManning, Christopher D.,Prabhakar Raghavan, Hinrich Schütze. Introduction Information Retrieval: https://nlp.stanford.edu/IR-book/information-retrieval-book.html","code":""},{"path":"course-overview.html","id":"assessment","chapter":"Course Overview","heading":"Assessment","text":"","code":""},{"path":"course-overview.html","id":"fortnightly-worksheets","chapter":"Course Overview","heading":"Fortnightly worksheets","text":"fortnight, provide one worksheet walks implement different text analysis technique. end worksheets find set questions. buddy someone else class go together.called “pair programming” ’s reason . Firstly, coding can isolating difficult thing—’s good bring friend along ride! Secondly, ’s something don’t know, maybe buddy . saves time. Thirdly, buddy can check code write , vice versa. , means working together produce check something go along.subsequent week’s lecture, pick pair random answer one worksheet’s questions (.e., ~1/3 chance ’re going get picked week). ask walk us code. remember: ’s also fine struggled didn’t get end! encountered obstacle, can work together. matters try.","code":""},{"path":"course-overview.html","id":"fortnightly-flash-talks","chapter":"Course Overview","heading":"Fortnightly flash talks","text":"weeks going tasked coding assignment, ’re hook… selecting pair random (coding pair) talk one readings. pick different pair reading (.e., ~ 1/3 chance ).Don’t let cause great anguish: just want two three minutes lay : 1) research question; 2) data source; 3) method; 4) findings; 5) thought limitations . main portion flash talk focus element 5).last one, want think 1)-4); .e., want think whether really answered research question, whether data appropriate answering question, whether method appropriate answering question, whether results show author claims show. provide wish example flash talk first lecture.","code":""},{"path":"course-overview.html","id":"final-assessment","chapter":"Course Overview","heading":"Final assessment","text":"Assessment takes form one summative assessment. 4000 word essay subject choosing (prior approval ). , required select range data sources provide. may also suggest data source.asked : ) formulate research question; b) use least one computational text analysis technique studied; c) conduct analysis data source provided; d) write initial findings; e) outline potential extensions analysis.provide code used reproducible (markdown) format assessed substantive content essay contribution (social science part) well demonstrated competency coding text analysis (computational part).","code":""},{"path":"introduction-to-r.html","id":"introduction-to-r","chapter":"Introduction to R","heading":"Introduction to R","text":"section designed ensure familiar R environment.","code":""},{"path":"introduction-to-r.html","id":"getting-started-with-r-at-home","chapter":"Introduction to R","heading":"0.1 Getting started with R at home","text":"Given ’re working home days, ’ll need download R RStudio onto devices. R name programming language ’ll using coding exercises; RStudio IDE (“Integrated Development Environment”), .e., piece software almost everyone uses working R.can download Windows Mac easily free. one first reasons use “open-source” programming language: ’s free everyone can contribute!Services University Edinburgh provided walkthrough needed get started. also break :Install R Mac : https://cran.r-project.org/bin/macosx/. Install R Windows : https://cran.r-project.org/bin/windows/base/.Install R Mac : https://cran.r-project.org/bin/macosx/. Install R Windows : https://cran.r-project.org/bin/windows/base/.Download RStudio Windows Mac : https://rstudio.com/products/rstudio/download/, choosing Free version: people use enough needs.Download RStudio Windows Mac : https://rstudio.com/products/rstudio/download/, choosing Free version: people use enough needs.programs free. Make sure load everything listed operating system R work properly!","code":""},{"path":"introduction-to-r.html","id":"some-basic-information","chapter":"Introduction to R","heading":"0.2 Some basic information","text":"script text file write commands (code) comments.script text file write commands (code) comments.put # character front line text line executed; useful add comments script!put # character front line text line executed; useful add comments script!R case sensitive, careful typing.R case sensitive, careful typing.send code script console, highlight relevant line code script click Run, select line hit ctrl+enter PCR cmd+enter MacTo send code script console, highlight relevant line code script click Run, select line hit ctrl+enter PCR cmd+enter MacAccess help files R functions preceding name function ? (e.g., ?table)Access help files R functions preceding name function ? (e.g., ?table)pressing key, can go back commands used beforeBy pressing key, can go back commands used beforePress tab key auto-complete variable names commandsPress tab key auto-complete variable names commands","code":""},{"path":"introduction-to-r.html","id":"getting-started-in-rstudio","chapter":"Introduction to R","heading":"0.3 Getting Started in RStudio","text":"Begin opening RStudio (located desktop). first task create new script (write commands). , click:screen now four panes:Script (top left)Script (top left)Console (bottom left)Console (bottom left)Environment/History (top right)Environment/History (top right)Files/Plots/Packages/Help/Viewer (bottom right)Files/Plots/Packages/Help/Viewer (bottom right)","code":"File --> NewFile --> RScript"},{"path":"introduction-to-r.html","id":"a-simple-example","chapter":"Introduction to R","heading":"0.4 A simple example","text":"Script (top left) write commands R. can try first time writing small snipped code follows:tell R run command, highlight relevant row script click Run button (top right Script) - hold ctrl+enter Windows cmd+enter Mac - send command Console (bottom left), actual evaluation calculations taking place. shortcut keys become familiar quickly!Running command creates object named ‘x,’ contains words message.can now see ‘x’ Environment (top right). view contained x, type Console (bottom left):","code":"\nx <- \"I can't wait to learn Computational Text Analysis\" #Note the quotation marks!\nprint(x)## [1] \"I can't wait to learn Computational Text Analysis\"\n# or alternatively you can just type:\n\nx## [1] \"I can't wait to learn Computational Text Analysis\""},{"path":"introduction-to-r.html","id":"loading-packages","chapter":"Introduction to R","heading":"0.5 Loading packages","text":"‘base’ version R powerful able everything , least ease. technical specialized forms analysis, need load new packages.need install -called ‘package’—program includes new tools (.e., functions) carry specific tasks. can think ‘extensions’ enhancing R’s capacities.take one example, might want something little exciting print excited course. Let’s make map instead.might sound technical. beauty packaged extensions R contain functions perform specialized types analysis ease.’ll first need install one packages, can :package installed, need load environment typing library(). Note , , don’t need wrap name package quotation marks. trick:now? Well, let’s see just easy visualize data using ggplot package comes bundled larger tidyverse package.wanted save ’d got making plots, want save scripts, maybe data used well, return later stage.","code":"\ninstall.packages(\"tidyverse\")\nlibrary(tidyverse)\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy))"},{"path":"introduction-to-r.html","id":"saving-your-objects-plots-and-scripts","chapter":"Introduction to R","heading":"0.6 Saving your objects, plots and scripts","text":"Saving scripts: save script RStudio (.e. top left panel), need click File –> Save (choose name script). script something like: myfilename.R.Saving scripts: save script RStudio (.e. top left panel), need click File –> Save (choose name script). script something like: myfilename.R.Saving plots: made plots like save, click Export (plotting pane) choose relevant file extension (e.g. .png, .pdf, etc.) size.Saving plots: made plots like save, click Export (plotting pane) choose relevant file extension (e.g. .png, .pdf, etc.) size.save individual objects (example x ) environment, run following command (choosing suitable filename):save individual objects (example x ) environment, run following command (choosing suitable filename):save objects (.e. everything top right panel) , run following command (choosing suitable filename):objects can re-loaded R next session running:many file formats might use save output. encounter course progresses.","code":"\nsave(x,file=\"myobject.RData\")\nload(file=\"myobject.RData\")\nsave.image(file=\"myfilname.RData\")\nload(file=\"myfilename.RData\")"},{"path":"introduction-to-r.html","id":"knowing-where-r-saves-your-documents","chapter":"Introduction to R","heading":"0.7 Knowing where R saves your documents","text":"home, open new script make sure check set working directory (.e. folder files create saved). check working directory use getwd() command (type Console write script Source Editor):set working directory, run following command, substituting file directory choice. Remember anything following `#’ symbol simply clarifying comment R process .","code":"\ngetwd()\n## Example for Mac \nsetwd(\"/Users/Documents/mydir/\") \n## Example for PC \nsetwd(\"c:/docs/mydir\") "},{"path":"introduction-to-r.html","id":"practicing-in-r","chapter":"Introduction to R","heading":"0.8 Practicing in R","text":"best way learn R use . workshops text analysis place become fully proficient R. , however, chance conduct hands-analysis applied examples fast-expanding field. best way learn . give shot!practice R programming language, look Wickham Grolemund (2017) , tidy text analysis, Silge Robinson (2017).free online book Hadley Wickham “R Data Science” available hereThe free online book Hadley Wickham “R Data Science” available hereThe free online book Julia Silge David Robinson “Text Mining R” available hereThe free online book Julia Silge David Robinson “Text Mining R” available hereFor practice R, may want consult set interactive tutorials, available package “learnr.” ’ve installed package, can go tutorials calling:practice R, may want consult set interactive tutorials, available package “learnr.” ’ve installed package, can go tutorials calling:","code":"\nlibrary(learnr)\n\navailable_tutorials() # this will tell you the names of the tutorials available\n\nrun_tutorial(name = \"ex-data-basics\", package = \"learnr\") #this will launch the interactive tutorial in a new Internet browser window"},{"path":"introduction-to-r.html","id":"one-final-note","chapter":"Introduction to R","heading":"0.9 One final note","text":"’ve dipped “R Data Science” book ’ll hear lot -called tidyverse R. essentially set packages use alternative, intuitive, way interacting data.main difference ’ll notice , instead separate lines function want run, wrapping functions inside functions, sets functions “piped” using “pipe” functions, look appearance: %>%.using “tidy” syntax weekly exercises computational text analysis workshops. anything unclear, can provide equivalents “base” R . lot useful text analysis packages now composed ‘tidy’ syntax.","code":""},{"path":"week-1.html","id":"week-1","chapter":"1 Week 1","heading":"1 Week 1","text":"","code":""},{"path":"week-1.html","id":"retrieving-and-analyzing-text-information","chapter":"1 Week 1","heading":"1.1 Retrieving and analyzing text information","text":"first task conducting large-scale text analyses gathering curating text information . focus chapters Manning, Raghavan, Schtze (2007) listed . , ’ll find introduction different ways can reformat ‘query’ text data order begin asking questions . often referred computer science natural language processing contexts “information retrieval” foundation many search, including web search, processes.articles Tatman (2017) Pechenick, Danforth, Dodds (2015) focus seminar (Q&). articles get us thinking fundamentals text discovery sampling. reading articles think locating texts, sampling , biases might inhere sampling process, texts represent; .e., population phenomenon interest might provide inferences.Questions seminar:access text? need consider ?sample texts?biases need keep mind?Required reading:Tatman (2017)Tatman (2017)Pechenick, Danforth, Dodds (2015)Pechenick, Danforth, Dodds (2015)Manning, Raghavan, Schtze (2007, chs.1 10): https://nlp.stanford.edu/IR-book/information-retrieval-book.html]Manning, Raghavan, Schtze (2007, chs.1 10): https://nlp.stanford.edu/IR-book/information-retrieval-book.html]Klaus Krippendorff (2004ch. 6)Klaus Krippendorff (2004ch. 6)reading:Olteanu et al. (2019)Biber (1993)Barberá Rivero (2015)","code":""},{"path":"week-2.html","id":"week-2","chapter":"2 Week 2","heading":"2 Week 2","text":"","code":""},{"path":"week-2.html","id":"tokenization-and-word-frequencies","chapter":"2 Week 2","heading":"2.1 Tokenization and word frequencies","text":"approaching large-scale quantiative analyses text, key task identify capture unit analysis. One commonly used approaches, across diverse analytical contexts, text tokenization. , splitting text word units: unigrams, bigrams, trigrams etc.chapters Manning, Raghavan, Schtze (2007), listed , provide technical introduction task “querying” text according different word-based queries. task studying hands-assignment week.seminar discussion, focusing widely-cited examples research applied social sciences employing token-based, word frequency, analyses large corpora. first, Michel et al. (2011) uses enormous Google books corpus measure cultural linguistic trends. second, Bollen et al. (2021a) uses corpus demonstrate specific change time—-called “cognitive distortion.” examples, attentive questions sampling covered previous weeks. question central back--forths short responses replies articles Michel et al. (2011) Bollen et al. (2021a).Questions:Tokenizing counting: capture?Corpus-based sampling: biases might threaten inference?write critique either Michel et al. (2011) Bollen et al. (2021a), focus ?Required reading:Michel et al. (2011)\nSchwartz (2011)\nMorse-Gagné (2011)\nAiden, Pickett, Michel (2011)\nSchwartz (2011)Morse-Gagné (2011)Aiden, Pickett, Michel (2011)Bollen et al. (2021a)\nSchmidt, Piantadosi, Mahowald (2021)\nBollen et al. (2021b)\nSchmidt, Piantadosi, Mahowald (2021)Bollen et al. (2021b)Manning, Raghavan, Schtze (2007ch. 2): https://nlp.stanford.edu/IR-book/information-retrieval-book.html]Klaus Krippendorff (2004ch. 5)reading:Rozado, Al-Gharbi, Halberstadt (2021)Alshaabi et al. (2021)Campos et al. (2015)Greenfield (2013)","code":""},{"path":"week-3.html","id":"week-3","chapter":"3 Week 3","heading":"3 Week 3","text":"","code":""},{"path":"week-3.html","id":"dictionary-based-techniques","chapter":"3 Week 3","heading":"3.1 Dictionary-based techniques","text":"extension word frequency analyses, covered last week, -called “dictionary-based” techniques. basic form, analyses use index target terms classify corpus interest based presence absence. technical dimensions type analysis covered chapter section Klaus Krippendorff (2004), issues attending article - Loughran Mcdonald (2011).also reading two examples application techniques Martins Baumard (2020) Young Soroka (2012). , discussing successful authors measuring phenomenon interest (“prosociality” “tone” respectively). Questions sampling representativeness relevant , naturally inform assessments work.Questions:general dictionaries possible; domain-specific?know dictionary accurate?enhance/supplement dictionary-based techniques?Required reading:Martins Baumard (2020)Martins Baumard (2020)Young Soroka (2012)Young Soroka (2012)Loughran Mcdonald (2011)Loughran Mcdonald (2011)Klaus Krippendorff (2004, 283–89)Klaus Krippendorff (2004, 283–89)reading:Tausczik Pennebaker (2010)Brier Hopp (2011)Barberá et al. (2021)","code":""},{"path":"week-4.html","id":"week-4","chapter":"4 Week 4","heading":"4 Week 4","text":"","code":""},{"path":"week-4.html","id":"natural-language-complexity-and-similarity","chapter":"4 Week 4","heading":"4.1 Natural language, complexity, and similarity","text":"week delving deeply language used text. previous weeks, tried two main techniques rely, different ways, counting words. week, thinking sophisticated techniques identify measure language use, well compare texts . article Gomaa Fahmy (2013) provides overview different approaches. covering technical dimensions lecture.article Urman, Makhortykh, Ulloa (2021) investigates key question contemporary communications research—information exposed online—shows might compare web search results using similarity measures. Schoonvelde et al. (2019) article, hand, looks “complexity” texts, compares politicians different ideological stripes communicate.Questions:measure linguistic complexity/sophistication?biases might involved measuring sophistication?applications might similarity measures?Required reading:Urman, Makhortykh, Ulloa (2021)Schoonvelde et al. (2019)Gomaa Fahmy (2013)reading:Voigt et al. (2017)Peng Hengartner (2002)Lowe (2008)Bail (2012)Ziblatt, Hilbig, Bischof (2020)Benoit, Munger, Spirling (2019)","code":""},{"path":"week-5.html","id":"week-5","chapter":"5 Week 5","heading":"5 Week 5","text":"","code":""},{"path":"week-5.html","id":"scaling-techniques","chapter":"5 Week 5","heading":"5.1 Scaling techniques","text":"begin thinking automated techniques analyzing texts. bunch additional considerations now need bring mind. considerations sparked significant debates… matter means settled.stake ? weeks come, studying various techniques ‘classify,’ ‘position’ ‘score’ texts based features. success techniques depends suitability question hand also higher-level questions meaning. short, ask : way can access underlying processes governing generation text? meaning governed set structural processes? can derive ‘objective’ measures contents given text?readings Justin Grimmer, Roberts, Stewart (2021), Denny Spirling (2018), Goldenstein Poschmann (2019b) (well response replies Nelson (2019) Goldenstein Poschmann (2019a)) required reading Flexible Learning Week.Justin Grimmer, Roberts, Stewart (2021)Justin Grimmer, Roberts, Stewart (2021)Denny Spirling (2018)Denny Spirling (2018)Goldenstein Poschmann (2019b)\nNelson (2019)\nGoldenstein Poschmann (2019a)\nGoldenstein Poschmann (2019b)Nelson (2019)Goldenstein Poschmann (2019a)substantive focus week set readings employ different types “scaling” “low-dimensional document embedding” techniques. article Lowe (2008) provides technical overview “wordfish” algorithm uses political science contexts. article Klüver (2009) also uses “wordfish” different way—measure “influence” interest groups. response article Bunea Ibenskas (2015) subsequent reply Klüver (2015) helps illuminate debates around questions.Questions:assumptions underlie scaling models text?; latent text decides?might scaling useful outside estimating ideological position text?Required reading:Lowe (2008)Lowe (2008)Justin Grimmer Stewart (2013)Justin Grimmer Stewart (2013)Klüver (2009)\nBunea Ibenskas (2015)\nKlüver (2015)\nKlüver (2009)Bunea Ibenskas (2015)Klüver (2015)reading:Benoit et al. (2016)Laver, Benoit, Garry (2003)Schwemmer Wieczorek (2020)Slapin Proksch (2008)Kaneko, Asano, Miwa (2021)","code":""},{"path":"week-6.html","id":"week-6","chapter":"6 Week 6","heading":"6 Week 6","text":"","code":""},{"path":"week-6.html","id":"unupervised-learning","chapter":"6 Week 6","heading":"6.1 Unupervised learning","text":"Required reading:Boyd et al. (2018)J. Grimmer King (2011)reading:Chang et al. (2009)Denny Spirling (2018)","code":""},{"path":"week-7.html","id":"week-7","chapter":"7 Week 7","heading":"7 Week 7","text":"","code":""},{"path":"week-7.html","id":"supervised-learning","chapter":"7 Week 7","heading":"7.1 Supervised learning","text":"Required reading:Hopkins King (2010)King, Pan, Roberts (2017)Siegel et al. (2021)Yu, Kaufmann, Diermeier (2008)Manning, Raghavan, Schtze (2007chs. 13,14, 15): https://nlp.stanford.edu/IR-book/information-retrieval-book.html]reading:Denny Spirling (2018)King, Lam, Roberts (2017)","code":""},{"path":"week-8.html","id":"week-8","chapter":"8 Week 8","heading":"8 Week 8","text":"","code":""},{"path":"week-8.html","id":"sampling-text-information","chapter":"8 Week 8","heading":"8.1 Sampling text information","text":"Required reading:Barberá Rivero (2015)Pechenick, Danforth, Dodds (2015)Klaus Krippendorff (2004chs. 5 6)reading:Martins Baumard (2020)","code":""},{"path":"week-9.html","id":"week-9","chapter":"9 Week 9","heading":"9 Week 9","text":"","code":""},{"path":"week-9.html","id":"word-embedding","chapter":"9 Week 9","heading":"9.1 Word embedding","text":"Required reading:Garg et al. (2018)Kozlowski, Taddy, Evans (2019)P. Rodriguez Spirling (2021)Jurafsky Martin (2021, ch.6): https://web.stanford.edu/~jurafsky/slp3/]reading:","code":""},{"path":"week-10.html","id":"week-10","chapter":"10 Week 10","heading":"10 Week 10","text":"","code":""},{"path":"week-10.html","id":"validation","chapter":"10 Week 10","heading":"10.1 Validation","text":"Required reading:Ying, Montgomery, Stewart (2021)P. L. Rodriguez, Spirling, Stewart (2021)Manning, Raghavan, Schtze (2007, ch.2: https://nlp.stanford.edu/IR-book/information-retrieval-book.html)reading:K. Krippendorff (2004)","code":""},{"path":"exercise-1-word-frequency-analysis.html","id":"exercise-1-word-frequency-analysis","chapter":"11 Exercise 1: Word frequency analysis","heading":"11 Exercise 1: Word frequency analysis","text":"","code":""},{"path":"exercise-1-word-frequency-analysis.html","id":"introduction","chapter":"11 Exercise 1: Word frequency analysis","heading":"11.1 Introduction","text":"tutorial, learn summarise, aggregate, analyze text R:tokenize filter textHow clean preprocess textHow visualize results ggplotHow perform automated gender assignment name data (think possible biases methods may enclose)","code":""},{"path":"exercise-1-word-frequency-analysis.html","id":"setup","chapter":"11 Exercise 1: Word frequency analysis","heading":"11.2 Setup","text":"practice skills, use dataset already collected Edinburgh Fringe Festival website.can try : obtain data, must first obtain API key. Instructions available Edinburgh Fringe API page:Alt Text","code":""},{"path":"exercise-1-word-frequency-analysis.html","id":"load-data-and-packages","chapter":"11 Exercise 1: Word frequency analysis","heading":"11.3 Load data and packages","text":"proceeding, ’ll load remaining packages need tutorial.tutorial, using data pre-cleaned provided .csv format. data come Edinburgh Book Festival API, provide data every event taken place Edinburgh Book Festival, runs every year month August, nine years: 2012-2020. many questions might ask data. tutorial, investigate contents event, speakers event, determine trends gender representation time.first task, , read data. can read_csv() function.read_csv() function takes .csv file loads working environment data frame object called “edbfdata.” can call object anything though. Try changing name object <- arrow. Note R allow names spaces , however. also good idea name object something beginning numbers, means call object within ` marks.’re working document computer (“locally”) can download Edinburgh Fringe data following way:","code":"\nlibrary(tidyverse) # loads dplyr, ggplot2, and others\nlibrary(tidytext) # includes set of functions useful for manipulating text\nlibrary(ggthemes) # includes a set of themes to make your visualizations look nice!\nlibrary(readr) # more informative and easy way to import data\nlibrary(babynames) #for gender predictions\nedbfdata <- read_csv(\"data/wordfreq/edbookfestall.csv\")## New names:\n## * `` -> ...1## Rows: 5938 Columns: 12## ── Column specification ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n## Delimiter: \",\"\n## chr (8): festival_id, title, sub_title, artist, description, genre, age_category, ID\n## dbl (4): ...1, year, latitude, longitude## \n## ℹ Use `spec()` to retrieve the full column specification for this data.\n## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nedbfdata <- read_csv(\"https://raw.githubusercontent.com/cjbarrie/RDL-Ed/main/02-text-as-data/data/edbookfestall.csv\")"},{"path":"exercise-1-word-frequency-analysis.html","id":"inspect-and-filter-data","chapter":"11 Exercise 1: Word frequency analysis","heading":"11.4 Inspect and filter data","text":"next job cut dataset size, including columns need. first can inspect see existing column names , variable coded. can first call::can see description event included column named “description” year event “year.” now ’ll just keep two. Remember: ’re interested tutorial firstly representation gender feminism forms cultural production given platform Edinburgh International Book Festival. Given , first foremost interested reported content artist’s event.use pipe %>% functions tidyverse package quickly efficiently select columns want edbfdata data.frame object. pass data new data.frame object, call “evdes.”let’s take quick look many events time festival. , first calculate number individual events (row observations) year (column variable).can plot using ggplot!Perhaps unsurprisingly, context pandemic, number recorded bookings 2020 Festival drastically reduced.","code":"\ncolnames(edbfdata)##  [1] \"...1\"         \"festival_id\"  \"title\"        \"sub_title\"    \"artist\"       \"year\"         \"description\"  \"genre\"        \"latitude\"     \"longitude\"    \"age_category\" \"ID\"\nglimpse(edbfdata)## Rows: 5,938\n## Columns: 12\n## $ ...1         <dbl> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, …\n## $ festival_id  <chr> \"book\", \"book\", \"book\", \"book\", \"book\", \"book\", \"book\", \"book\", \"book\", \"book\", \"book\", \"book\", \"book\", \"book\", \"book\", \"book\", \"book\", \"book\", \"book\", \"book\", \"book\", \"book\", \"book\",…\n## $ title        <chr> \"Denise Mina\", \"Alex T Smith\", \"Challenging Expectations with Peter Cocks & Simon Mason\", \"Ian Black & Paul Mason\", \"The Last Dragon Chronicles with Chris d'Lacey\", \"Sir Charlie Stink…\n## $ sub_title    <chr> \"HARD MEN AND CARDBOARD GANGSTERS\", NA, NA, \"WHAT CAUSED THE 2011 REVOLUTIONS?\", NA, NA, NA, \"MAKING BROADCAST HISTORY\", \"PRESENTING LIFE WRITING\", \"ARE THERE ANY ALTERNATIVES TO MARK…\n## $ artist       <chr> \"Denise Mina\", \"Alex T Smith\", \"Peter Cocks\", \"Paul Mason\", \"Chris d'Lacey\", \"Kristina Stephenson\", \"Annie Dalton\", \"Mike Shaw\", \"Alison Baverstock\", \"Ferdinand Mount\", \"Irvine Welsh\"…\n## $ year         <dbl> 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 201…\n## $ description  <chr> \"<p>\\n\\tAs the grande dame of Scottish crime fiction, Denise Mina places a deep understanding of the social fabric of Glasgow at the heart of her blistering Alex Morrow series. In the…\n## $ genre        <chr> \"Literature\", \"Children\", \"Children\", \"Literature\", \"Children\", \"Children\", \"Children\", \"Literature\", \"Literature\", \"Literature\", \"Literature\", \"Children\", \"Literature\", \"Children\", \"…\n## $ latitude     <dbl> 55.9519, 55.9519, 55.9519, 55.9519, 55.9519, 55.9519, 55.9519, 55.9519, 55.9519, 55.9519, 55.9519, 55.9519, 55.9519, 55.9519, 55.9519, 55.9519, 55.9519, 55.9519, 55.9519, 55.9519, 55.…\n## $ longitude    <dbl> -3.206913, -3.206913, -3.206913, -3.206913, -3.206913, -3.206913, -3.206913, -3.206913, -3.206913, -3.206913, -3.206913, -3.206913, -3.206913, -3.206913, -3.206913, -3.206913, -3.2069…\n## $ age_category <chr> NA, \"AGE 4 - 7\", \"AGE 11 - 14\", NA, \"AGE 10 - 14\", \"AGE 6 - 10\", \"AGE 6 - 10\", NA, NA, NA, NA, NA, NA, \"AGE 14 - 16\", \"AGE 7 - 10\", \"AGE 8 - 13\", \"AGE 14 - 16\", NA, NA, \"AGE 0 - 3\", \"…\n## $ ID           <chr> \"Denise Mina2012\", \"Alex T Smith2012\", \"Peter Cocks2012\", \"Paul Mason2012\", \"Chris d'Lacey2012\", \"Kristina Stephenson2012\", \"Annie Dalton2012\", \"Mike Shaw2012\", \"Alison Baverstock2012…\n# get simplified dataset with only event contents and year\nevdes <- edbfdata %>%\n  select(description, year)\n\nhead(evdes)## # A tibble: 6 × 2\n##   description                                                                                                                                                                                             year\n##   <chr>                                                                                                                                                                                                  <dbl>\n## 1 \"<p>\\n\\tAs the grande dame of Scottish crime fiction, Denise Mina places a deep understanding of the social fabric of Glasgow at the heart of her blistering Alex Morrow series. In the latest affair…  2012\n## 2 \"<p>\\n\\tWhen Alex T Smith was a little boy he wanted to be a chef, a rabbit or a children&rsquo;s book illustrator. Guess which path he chose? Let Alex introduce you to Ella, the ladybird Cinderell…  2012\n## 3 \"<p>\\n\\tPeter Cocks is known for his fantasy series Triskellion written with Mark Billingham; Simon Mason is the creator of The Quigleys series about an eccentric family, but both tackle darker iss…  2012\n## 4 \"<p>\\n\\tTwo books by influential journalists are among the first to look at the factors that caused a wave of protests across the world in 2011. BBC Newsnight Economics Editor Paul Mason discusses …  2012\n## 5 \"<p>\\n\\tChris d&rsquo;Lacey tells you all about The Fire Ascending, the next exciting instalment in his breathtaking The Last Dragon Chronicles series. Get to know more about hero David Rain, his q…  2012\n## 6 \"<p>\\n\\tIt&rsquo;s time for the honourable, feisty and courageous young hero, Sir Charlie Stinky Socks to return to Edinburgh and this time to discover the secret of the pitiful sobbing coming from…  2012\nevtsperyr <- evdes %>%\n  mutate(obs=1) %>%\n  group_by(year) %>%\n  summarise(sum_events = sum(obs))\nggplot(evtsperyr) +\n  geom_line(aes(year, sum_events)) +\n  theme_tufte(base_family = \"Helvetica\") + \n  scale_y_continuous(expand = c(0, 0), limits = c(0, NA))"},{"path":"exercise-1-word-frequency-analysis.html","id":"tidy-the-text","chapter":"11 Exercise 1: Word frequency analysis","heading":"11.5 Tidy the text","text":"Given data obtained API outputs data originally HTML format, text still contains HTML PHP encodings e.g. bold font paragraphs. ’ll need get rid , well punctuation analyzing data.set commands takes event descriptions, extracts individual words, counts number times appear years covered book festival data.","code":"\n#get year and word for every word and date pair in the dataset\ntidy_des <- evdes %>% \n  mutate(desc = tolower(description)) %>%\n  unnest_tokens(word, desc) %>%\n  filter(str_detect(word, \"[a-z]\"))"},{"path":"exercise-1-word-frequency-analysis.html","id":"regex-sidebar","chapter":"11 Exercise 1: Word frequency analysis","heading":"11.6 Regex sidebar","text":"’ll notice parentheses str_detect() string “[-z].” called character class use square brackets like [].character classes include, helpfully listed vignette stringr package. follows adapted materials.[abc]: matches , b, c.[-z]: matches every character z\n(Unicode code point order).[^abc]: matches anything except , b, c.[\\^\\-]: matches ^ -.Several patterns match multiple characters. include:\\d: matches digit; opposite \\D, matches character \ndecimal digit.\\s: matches whitespace; opposite \\S^: matches start string$: matches end string^ $: exact string matchHold : plus signs etc. mean?+: 1 .*: 0 .?: 0 1.can tell output makes sense, ’re getting !","code":"\nstr_extract_all(\"1 + 2 = 3\", \"\\\\d+\")## [[1]]\n## [1] \"1\" \"2\" \"3\"\nstr_extract_all(\"1 + 2 = 3\", \"\\\\D+\")## [[1]]\n## [1] \" + \" \" = \"\n(text <- \"Some  \\t badly\\n\\t\\tspaced \\f text\")## [1] \"Some  \\t badly\\n\\t\\tspaced \\f text\"\nstr_replace_all(text, \"\\\\s+\", \" \")## [1] \"Some badly spaced text\"\nx <- c(\"apple\", \"banana\", \"pear\")\nstr_extract(x, \"^a\")## [1] \"a\" NA  NA\nx <- c(\"apple\", \"banana\", \"pear\")\nstr_extract(x, \"^a$\")## [1] NA NA NA\nx <- c(\"apple\", \"banana\", \"pear\")\nstr_extract(x, \"^apple$\")## [1] \"apple\" NA      NA\nstr_extract_all(\"1 + 2 = 3\", \"\\\\d+\")[[1]]## [1] \"1\" \"2\" \"3\"\nstr_extract_all(\"1 + 2 = 3\", \"\\\\D+\")[[1]]## [1] \" + \" \" = \"\nstr_extract_all(\"1 + 2 = 3\", \"\\\\d*\")[[1]]##  [1] \"1\" \"\"  \"\"  \"\"  \"2\" \"\"  \"\"  \"\"  \"3\" \"\"\nstr_extract_all(\"1 + 2 = 3\", \"\\\\D*\")[[1]]## [1] \"\"    \" + \" \"\"    \" = \" \"\"    \"\"\nstr_extract_all(\"1 + 2 = 3\", \"\\\\d?\")[[1]]##  [1] \"1\" \"\"  \"\"  \"\"  \"2\" \"\"  \"\"  \"\"  \"3\" \"\"\nstr_extract_all(\"1 + 2 = 3\", \"\\\\D?\")[[1]]##  [1] \"\"  \" \" \"+\" \" \" \"\"  \" \" \"=\" \" \" \"\"  \"\""},{"path":"exercise-1-word-frequency-analysis.html","id":"some-more-regex-resources","chapter":"11 Exercise 1: Word frequency analysis","heading":"11.6.1 Some more regex resources:","text":"Regex crossword: https://regexcrossword.com/.Regexone: https://regexone.com/R4DS chapter 14","code":""},{"path":"exercise-1-word-frequency-analysis.html","id":"back-to-the-fringe","chapter":"11 Exercise 1: Word frequency analysis","heading":"11.7 Back to the Fringe","text":"see resulting dataset large (~293k rows). commands first taken pamphlet text, “mutated” set lower case character string. “unnest_tokens” function taken individual string create new column called “word” contains individual word contained pamphlet texts.terminology also appropriate . tidy text format, often refer data structures consisting “documents” “terms.” “tokenizing” text “unnest_tokens” functions generating dataset one term per row., “documents” collection descriptions events year Edinburgh Book Festival. way sort text “documents” depends choice individual researcher.Instead year, might wanted sort text “genre.” , two genres: “Literature” “Children.” done , two “documents,” contained words included event descriptions genre.Alternatively, might interested contributions individual authors time. case, sorted text documents author. case, “document” represent words included event descriptions events given author (many multiple appearances time festival given year).can yet tidy , though. First ’ll remove stop words ’ll remove apostrophes:see number rows dataset reduces half ~223k rows. natural since large proportion string contain many -called “stop words.” can see stop words typing:lexicon (list words) included tidytext package produced Julia Silge David Robinson (see ). see contains 1000 words. remove informative interested substantive content text (rather , say, grammatical content).Now let’s look common words data:can see one common words “rsquo,” HTML encoding apostrophe. Clearly need clean data bit . common issue large-n text analysis key step want conduct reliably robust forms text analysis. ’ll another go using filter command, specifying keep words included string words rsquo, em, ndash, nbsp, lsquo.’s like ! words feature seem make sense now (actual words rather random HTML UTF-8 encodings).Let’s now collect words data.frame object, ’ll call edbf_term_counts:year, see “book” common word… perhaps surprises . evidence ’re properly pre-processing cleaning data. Cleaning text data important element preparing text analysis. often process trial error text data looks alike, may come e.g. webpages HTML encoding, unrecognized fonts unicode, potential cause issues! finding errors also chance get know data…","code":"\ntidy_des <- tidy_des %>%\n    filter(!word %in% stop_words$word)\nstop_words## # A tibble: 1,149 × 2\n##    word        lexicon\n##    <chr>       <chr>  \n##  1 a           SMART  \n##  2 a's         SMART  \n##  3 able        SMART  \n##  4 about       SMART  \n##  5 above       SMART  \n##  6 according   SMART  \n##  7 accordingly SMART  \n##  8 across      SMART  \n##  9 actually    SMART  \n## 10 after       SMART  \n## # … with 1,139 more rows\ntidy_des %>%\n  count(word, sort = TRUE)## # A tibble: 24,995 × 2\n##    word       n\n##    <chr>  <int>\n##  1 rsquo   5638\n##  2 book    2088\n##  3 event   1356\n##  4 author  1332\n##  5 world   1240\n##  6 story   1159\n##  7 join    1095\n##  8 em      1064\n##  9 life     879\n## 10 strong   864\n## # … with 24,985 more rows\nremove_reg <- c(\"&amp;\",\"&lt;\",\"&gt;\",\"<p>\", \"<\/p>\",\"&rsquo\", \"&lsquo;\",  \"&#39;\", \"<strong>\", \"<\/strong>\", \"rsquo\", \"em\", \"ndash\", \"nbsp\", \"lsquo\", \"strong\")\nreg_match <- paste0(remove_reg, collapse = \"|\")\n                  \ntidy_des <- tidy_des %>%\n  filter(!word %in% remove_reg)\ntidy_des %>%\n  count(word, sort = TRUE)## # A tibble: 24,989 × 2\n##    word        n\n##    <chr>   <int>\n##  1 book     2088\n##  2 event    1356\n##  3 author   1332\n##  4 world    1240\n##  5 story    1159\n##  6 join     1095\n##  7 life      879\n##  8 stories   860\n##  9 chaired   815\n## 10 books     767\n## # … with 24,979 more rows\nedbf_term_counts <- tidy_des %>% \n  group_by(year) %>%\n  count(word, sort = TRUE)\nhead(edbf_term_counts)## # A tibble: 6 × 3\n## # Groups:   year [6]\n##    year word      n\n##   <dbl> <chr> <int>\n## 1  2016 book    295\n## 2  2018 book    283\n## 3  2019 book    265\n## 4  2012 book    254\n## 5  2013 book    241\n## 6  2015 book    239"},{"path":"exercise-1-word-frequency-analysis.html","id":"analyze-keywords","chapter":"11 Exercise 1: Word frequency analysis","heading":"11.8 Analyze keywords","text":"Okay, now list words, number times appear, can tag words think might related issues gender inequality sexism. may decide list imprecise inexhaustive. , feel free change terms including grepl() function.","code":"\nedbf_term_counts$womword <- as.integer(grepl(\"women|feminist|feminism|gender|harassment|sexism|sexist\", \n                                            x = edbf_term_counts$word))\nhead(edbf_term_counts)## # A tibble: 6 × 4\n## # Groups:   year [6]\n##    year word      n womword\n##   <dbl> <chr> <int>   <int>\n## 1  2016 book    295       0\n## 2  2018 book    283       0\n## 3  2019 book    265       0\n## 4  2012 book    254       0\n## 5  2013 book    241       0\n## 6  2015 book    239       0"},{"path":"exercise-1-word-frequency-analysis.html","id":"compute-aggregate-statistics","chapter":"11 Exercise 1: Word frequency analysis","heading":"11.9 Compute aggregate statistics","text":"Now tagged individual words relating gender inequality feminism, can sum number times words appear year denominate total number words event descriptions.intuition increase decrease percentage words relating issues capturing substantive change representation issues related sex gender.think measure? adequate measure representation issues cultural sphere?keywords used precise enough? , change?","code":"\n#get counts by year and word\nedbf_counts <- edbf_term_counts %>%\n  complete(year, word, fill = list(n = 0)) %>%\n  group_by(year) %>%\n  mutate(year_total = sum(n)) %>%\n  filter(womword==1) %>%\n  summarise(sum_wom = sum(n),\n            year_total= min(year_total))\nhead(edbf_counts)## # A tibble: 6 × 3\n##    year sum_wom year_total\n##   <dbl>   <dbl>      <dbl>\n## 1  2012      22      23146\n## 2  2013      40      23277\n## 3  2014      30      25366\n## 4  2015      24      22158\n## 5  2016      34      24356\n## 6  2017      55      27602"},{"path":"exercise-1-word-frequency-analysis.html","id":"plot-time-trends","chapter":"11 Exercise 1: Word frequency analysis","heading":"11.10 Plot time trends","text":"see? Let’s take count words relating gender dataset, denominate total number words data per year.can add visual guides draw attention apparent changes data. , might wish signal year #MeToo movement 2017.label highlighting year 2017 including text label along vertical line.","code":"\nggplot(edbf_counts, aes(year, sum_wom / year_total, group=1)) +\n  geom_line() +\n  xlab(\"Year\") +\n  ylab(\"% gender-related words\") +\n  scale_y_continuous(labels = scales::percent_format(),\n                     expand = c(0, 0), limits = c(0, NA)) +\n  theme_tufte(base_family = \"Helvetica\") \nggplot(edbf_counts, aes(year, sum_wom / year_total, group=1)) +\n  geom_line() +\n  geom_vline(xintercept = 2017, col=\"red\") +\n  xlab(\"Year\") +\n  ylab(\"% gender-related words\") +\n  scale_y_continuous(labels = scales::percent_format(),\n                     expand = c(0, 0), limits = c(0, NA)) +\n  theme_tufte(base_family = \"Helvetica\")\nggplot(edbf_counts, aes(year, sum_wom / year_total, group=1)) +\n  geom_line() +\n  geom_vline(xintercept = 2017, col=\"red\") +\n  geom_text(aes(x=2017.1, label=\"#metoo year\", y=.0015), \n            colour=\"black\", angle=90, text=element_text(size=8)) +\n  xlab(\"Year\") +\n  ylab(\"% gender-related words\") +\n  scale_y_continuous(labels = scales::percent_format(),\n                     expand = c(0, 0), limits = c(0, NA)) +\n  theme_tufte(base_family = \"Helvetica\")"},{"path":"exercise-1-word-frequency-analysis.html","id":"bonus-gender-prediction","chapter":"11 Exercise 1: Word frequency analysis","heading":"11.11 Bonus: gender prediction","text":"might decide measure inadequate expansive answer question hand. Another way measuring representation cultural production measure gender authors spoke events.course, take quite time individually code approximately 6000 events included dataset.exist alternative techniques imputing gender based name individual.first create new data.frame object, selecting just columns artist name year. generate new column containing just artist’s (author’s) first name:set packages called gender genderdata used make process predicting gender based given individual’s name pretty straightforward. technique worked reference U.S. Social Security Administration baby name data.Given common gender associated given name changes time, function also allows us specify range years cohort question whose gender inferring. Given don’t know wide cohort artists , specify broad range 1920-2000.Unfortunately, package longer works newer versions R; fortunately, recreated using original “babynames” data, comes bundled babynames package.don’t necessarily follow step done —include information sake completeness.babynames package. contains, year, number children born given name, well sex. information, can calculate total number individuals given name born sex given year.Given also total number babies born total cross records, can denominate (divide) sums name total number births sex year. can take proportion representing probability given individual Edinburgh Fringe dataset male female.information babynames package can found .first load babynames package R environment data.frame object. data.frame “babynames” contained babynames package can just call object store .dataset contains names years period 1800–2019. variable “n” represents number babies born given name sex year, “prop” represents, according package materials accessible , “n divided total number applicants year, means proportions people gender name born year.”calculate total number babies female male sex born year. merge get combined dataset male female baby names year. merge information back original babynames data.frame object.can calculate, babies born 1920, number babies born name sex. information, can get proportion babies given name particular sex. example, 92% babies born name “Mary” female, give us .92 probability individual name “Mary” female.every name dataset, excluding names proportion equal .5; .e., names adjudicate whether less likely male female.proportions names, can merge back names artists Edinburgh Fringe Book Festival. can easily plot proportion artists Festival male versus female year Festival.can conclude form graph?Note merged proportions th “babynames” data Edinburgh Fringe data lost observations. names Edinburgh Fringe data match “babynames” data. Let’s look names match:notice anything names? tell us potential biases using sources US baby names data foundation gender prediction? alternative ways might go task?","code":"\n# get columns for artist name and year, omitting NAs\ngendes <- edbfdata %>%\n  select(artist, year) %>%\n  na.omit()\n\n# generate new column with just the artist's (author's) first name\ngendes$name <- sub(\" .*\", \"\", gendes$artist)\ngenpred <- gender(gendes$name,\n       years = c(1920, 2000))\nbabynames <- babynames\nhead(babynames)## # A tibble: 6 × 5\n##    year sex   name          n   prop\n##   <dbl> <chr> <chr>     <int>  <dbl>\n## 1  1880 F     Mary       7065 0.0724\n## 2  1880 F     Anna       2604 0.0267\n## 3  1880 F     Emma       2003 0.0205\n## 4  1880 F     Elizabeth  1939 0.0199\n## 5  1880 F     Minnie     1746 0.0179\n## 6  1880 F     Margaret   1578 0.0162\ntotals_female <- babynames %>%\n  filter(sex==\"F\") %>%\n  group_by(year) %>%\n  summarise(total_female = sum(n))\n\ntotals_male <- babynames %>%\n  filter(sex==\"M\") %>%\n  group_by(year) %>%\n  summarise(total_male = sum(n))\n\ntotals <- merge(totals_female, totals_male)\n\ntotsm <- merge(babynames, totals, by = \"year\")\nhead(totsm)##   year sex      name    n       prop total_female total_male\n## 1 1880   F      Mary 7065 0.07238359        90993     110491\n## 2 1880   F      Anna 2604 0.02667896        90993     110491\n## 3 1880   F      Emma 2003 0.02052149        90993     110491\n## 4 1880   F Elizabeth 1939 0.01986579        90993     110491\n## 5 1880   F    Minnie 1746 0.01788843        90993     110491\n## 6 1880   F  Margaret 1578 0.01616720        90993     110491\ntotprops <- totsm %>%\n  filter(year >= 1920) %>%\n  group_by(name, year) %>%\n  mutate(sumname = sum(n),\n         prop = ifelse(sumname==n, 1,\n                       n/sumname)) %>%\n  filter(prop!=.5) %>%\n  group_by(name) %>%\n  slice(which.max(prop)) %>%\n  summarise(prop = max(prop),\n            totaln = sum(n),\n            name = max(name),\n            sex = unique(sex))\n\nhead(totprops)## # A tibble: 6 × 4\n##   name       prop totaln sex  \n##   <chr>     <dbl>  <int> <chr>\n## 1 Aaban         1      5 M    \n## 2 Aabha         1      7 F    \n## 3 Aabid         1      5 M    \n## 4 Aabir         1      5 M    \n## 5 Aabriella     1      5 F    \n## 6 Aada          1      5 F\nednameprops <- merge(totprops, gendes, by = \"name\")\n\nggplot(ednameprops, aes(x=year, fill = factor(sex))) +\n  geom_bar(position = \"fill\") +\n  xlab(\"Year\") +\n  ylab(\"% women authors\") +\n  labs(fill=\"\") +\n  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +\n  theme_tufte(base_family = \"Helvetica\") +\n  geom_abline(slope=0, intercept=0.5,  col = \"black\",lty=2)\nnames1 <- ednameprops$name\nnames2 <- gendes$name\ndiffs <- setdiff(names2, names1)\ndiffs##   [1] \"L\"             \"Kapka\"         \"Menzies\"       \"Ros\"           \"G\"             \"Pankaj\"        \"Uzodinma\"      \"Rodge\"         \"A\"             \"Zoë\"           \"László\"        \"Sadakat\"      \n##  [13] \"Michèle\"       \"Maajid\"        \"Yrsa\"          \"Ahdaf\"         \"Noo\"           \"Dilip\"         \"Sjón\"          \"François\"      \"J\"             \"K\"             \"Aonghas\"       \"S\"            \n##  [25] \"Bashabi\"       \"Kjartan\"       \"Romesh\"        \"T\"             \"Chibundu\"      \"Yiyun\"         \"Fiammetta\"     \"W\"             \"Sindiwe\"       \"Cat\"           \"Jez\"           \"Fi\"           \n##  [37] \"Sunder\"        \"Saci\"          \"C.J\"           \"Halik\"         \"Niccolò\"       \"Sifiso\"        \"C.S.\"          \"DBC\"           \"Phyllida\"      \"R\"             \"Struan\"        \"C.J.\"         \n##  [49] \"SF\"            \"Nadifa\"        \"Jérome\"        \"D\"             \"Xiaolu\"        \"Ramita\"        \"John-Paul\"     \"Ha-Joon\"       \"Niq\"           \"Andrés\"        \"Sasenarine\"    \"Frane\"        \n##  [61] \"Alev\"          \"Gruff\"         \"Line\"          \"Zakes\"         \"Pip\"           \"Witi\"          \"Halsted\"       \"Ziauddin\"      \"J.\"            \"Åsne\"          \"Alecos\"        \".\"            \n##  [73] \"Julián\"        \"Sunjeev\"       \"A.C.S\"         \"Etgar\"         \"Hyeonseo\"      \"Jaume\"         \"A.\"            \"Jesús\"         \"Jón\"           \"Helle\"         \"M\"             \"Jussi\"        \n##  [85] \"Aarathi\"       \"Shappi\"        \"Macastory\"     \"Odafe\"         \"Chimwemwe\"     \"Hrefna\"        \"Bidisha\"       \"Packie\"        \"Tahmima\"       \"Sara-Jane\"     \"Tahar\"         \"Lemn\"         \n##  [97] \"Neu!\"          \"Jürgen\"        \"Barroux\"       \"Jan-Philipp\"   \"Non\"           \"Metaphrog\"     \"Wilko\"         \"Álvaro\"        \"Stef\"          \"Erlend\"        \"Grinagog\"      \"Norma-Ann\"    \n## [109] \"Fuchsia\"       \"Giddy\"         \"Joudie\"        \"Sav\"           \"Liu\"           \"Jayne-Anne\"    \"Wioletta\"      \"Sinéad\"        \"Katherena\"     \"Siân\"          \"Dervla\"        \"Teju\"         \n## [121] \"Iosi\"          \"Daša\"          \"Cosey\"         \"Bettany\"       \"Thordis\"       \"Uršuľa\"        \"Limmy\"         \"Meik\"          \"Zindzi\"        \"Dougie\"        \"Ngugi\"         \"Inua\"         \n## [133] \"Ottessa\"       \"Bjørn\"         \"Novuyo\"        \"Rhidian\"       \"Sibéal\"        \"Hsiao-Hung\"    \"Audur\"         \"Sadek\"         \"Özlem\"         \"Zaffar\"        \"Jean-Pierre\"   \"Lalage\"       \n## [145] \"Yaba\"          \"H\"             \"DJ\"            \"Sigitas\"       \"Clémentine\"    \"Celeste-Marie\" \"Marawa\"        \"Ghillie\"       \"Ahdam\"         \"Suketu\"        \"Goenawan\"      \"Niviaq\"       \n## [157] \"Steinunn\"      \"Shoo\"          \"Ibram\"         \"Venki\"         \"DeRay\"         \"Diarmaid\"      \"Serhii\"        \"Harkaitz\"      \"Adélaïde\"      \"Agustín\"       \"Jérôme\"        \"Siobhán\"      \n## [169] \"Nesrine\"       \"Jokha\"         \"Gulnar\"        \"Uxue\"          \"Taqralik\"      \"Tayi\"          \"E\"             \"Dapo\"          \"Dunja\"         \"Maaza\"         \"Wayétu\"        \"Shokoofeh\""},{"path":"exercise-1-word-frequency-analysis.html","id":"exercises","chapter":"11 Exercise 1: Word frequency analysis","heading":"11.12 Exercises","text":"Filter books genre (selecting e.g., “Literature” “Children”) plot frequency women-related words time.Choose another set terms filter (e.g., race-related words) plot frequency time.","code":""},{"path":"exercise-1-word-frequency-analysis.html","id":"references","chapter":"11 Exercise 1: Word frequency analysis","heading":"11.13 References","text":"","code":""},{"path":"exercise-2-sentiment-analysis.html","id":"exercise-2-sentiment-analysis","chapter":"12 Exercise 2: Sentiment analysis","heading":"12 Exercise 2: Sentiment analysis","text":"","code":""},{"path":"exercise-2-sentiment-analysis.html","id":"introduction-1","chapter":"12 Exercise 2: Sentiment analysis","heading":"12.1 Introduction","text":"tutorial, learn :Use dictionary-based techniques analyze textUse common sentiment dictionariesCreate “dictionary”Use Lexicoder sentiment dictionary Young Soroka (2012)","code":""},{"path":"exercise-2-sentiment-analysis.html","id":"setup-1","chapter":"12 Exercise 2: Sentiment analysis","heading":"12.2 Setup","text":"hands-exercise week uses dictionary-based methods filtering scoring words. Dictionary-based methods use pre-generated lexicons, list words associated scores variables measuring valence particular word. sense, exercise unlike analysis Edinburgh Book Festival event descriptions. , filtering descriptions based presence absence word related women gender. can understand approach particularly simple type “dictionary-based” method. , “dictionary” “lexicon” contained just words related gender.","code":""},{"path":"exercise-2-sentiment-analysis.html","id":"load-data-and-packages-1","chapter":"12 Exercise 2: Sentiment analysis","heading":"12.3 Load data and packages","text":"proceeding, ’ll load remaining packages need tutorial.exercise ’ll using another new dataset. data collected Twitter accounts top eight newspapers UK circulation. can see names newspapers code :details access Twitter data academictwitteR, check details package .can download final dataset :’re working document computer (“locally”) can download tweets data following way:","code":"\nlibrary(academictwitteR) # for fetching Twitter data\nlibrary(tidyverse) # loads dplyr, ggplot2, and others\nlibrary(readr) # more informative and easy way to import data\nlibrary(stringr) # to handle text elements\nlibrary(tidytext) # includes set of functions useful for manipulating text\nlibrary(quanteda) # includes functions to implement Lexicoder\nnewspapers = c(\"TheSun\", \"DailyMailUK\", \"MetroUK\", \"DailyMirror\", \n               \"EveningStandard\", \"thetimes\", \"Telegraph\", \"guardian\")\n\ntweets <-\n  get_all_tweets(\n    users = newspapers,\n    start_tweets = \"2020-01-01T00:00:00Z\",\n    end_tweets = \"2020-05-01T00:00:00Z\",\n    data_path = \"data/sentanalysis/\",\n    n = Inf,\n  )\n\ntweets <- \n  bind_tweets(data_path = \"data/sentanalysis/\", output_format = \"tidy\")\n\nsaveRDS(tweets, \"data/sentanalysis/newstweets.rds\")\ntweets <- readRDS(\"data/sentanalysis/newstweets.rds\")\ntweets  <- readRDS(gzcon(url(\"https://github.com/cjbarrie/CTA-ED/blob/main/data/sentanalysis/newstweets.rds?raw=true\")))"},{"path":"exercise-2-sentiment-analysis.html","id":"inspect-and-filter-data-1","chapter":"12 Exercise 2: Sentiment analysis","heading":"12.4 Inspect and filter data","text":"Let’s look data:row tweets produced one news outlets detailed five month period, January–May 2020. Note also tweets particular date. can therefore use look time changes.won’t need variables let’s just keep interest us:manipulate data tidy format , unnesting token (: words) tweet text.’ll tidy , previous example, removing stop words:","code":"\nhead(tweets)## # A tibble: 6 × 31\n##   tweet_id            user_username text   lang  author_id source  possibly_sensit… conversation_id created_at user_url user_location user_protected user_verified user_name user_profile_im… user_description\n##   <chr>               <chr>         <chr>  <chr> <chr>     <chr>   <lgl>            <chr>           <chr>      <chr>    <chr>         <lgl>          <lgl>         <chr>     <chr>            <chr>           \n## 1 1212334402266521602 DailyMirror   \"Secr… en    16887175  TweetD… FALSE            12123344022665… 2020-01-0… https:/… UK            FALSE          TRUE          The Mirr… https://pbs.twi… \"The official M…\n## 2 1212334169457676289 DailyMirror   \"RT @… en    16887175  TweetD… FALSE            12123341694576… 2020-01-0… https:/… UK            FALSE          TRUE          The Mirr… https://pbs.twi… \"The official M…\n## 3 1212333195879993344 thetimes      \"A ce… en    6107422   Echobox FALSE            12123331958799… 2020-01-0… http://… London        FALSE          TRUE          The Times https://pbs.twi… \"The best of ou…\n## 4 1212333194864988161 TheSun        \"Wayn… en    34655603  Echobox FALSE            12123331948649… 2020-01-0… https:/… London        FALSE          TRUE          The Sun   https://pbs.twi… \"Never miss a s…\n## 5 1212332920507191296 DailyMailUK   \"Stud… en    111556423 Social… FALSE            12123329205071… 2020-01-0… https:/… London, UK    FALSE          TRUE          Daily Ma… https://pbs.twi… \"For the latest…\n## 6 1212332640570875904 TheSun        \"Dad … en    34655603  Twitte… FALSE            12123326405708… 2020-01-0… https:/… London        FALSE          TRUE          The Sun   https://pbs.twi… \"Never miss a s…\n## # … with 15 more variables: user_created_at <chr>, user_pinned_tweet_id <chr>, retweet_count <int>, like_count <int>, quote_count <int>, user_tweet_count <int>, user_list_count <int>,\n## #   user_followers_count <int>, user_following_count <int>, sourcetweet_type <chr>, sourcetweet_id <chr>, sourcetweet_text <chr>, sourcetweet_lang <chr>, sourcetweet_author_id <chr>,\n## #   in_reply_to_user_id <chr>\ncolnames(tweets)##  [1] \"tweet_id\"               \"user_username\"          \"text\"                   \"lang\"                   \"author_id\"              \"source\"                 \"possibly_sensitive\"     \"conversation_id\"       \n##  [9] \"created_at\"             \"user_url\"               \"user_location\"          \"user_protected\"         \"user_verified\"          \"user_name\"              \"user_profile_image_url\" \"user_description\"      \n## [17] \"user_created_at\"        \"user_pinned_tweet_id\"   \"retweet_count\"          \"like_count\"             \"quote_count\"            \"user_tweet_count\"       \"user_list_count\"        \"user_followers_count\"  \n## [25] \"user_following_count\"   \"sourcetweet_type\"       \"sourcetweet_id\"         \"sourcetweet_text\"       \"sourcetweet_lang\"       \"sourcetweet_author_id\"  \"in_reply_to_user_id\"\ntweets <- tweets %>%\n  select(user_username, text, created_at, user_name,\n         retweet_count, like_count, quote_count) %>%\n  rename(username = user_username,\n         newspaper = user_name,\n         tweet = text)\ntidy_tweets <- tweets %>% \n  mutate(desc = tolower(tweet)) %>%\n  unnest_tokens(word, desc) %>%\n  filter(str_detect(word, \"[a-z]\"))\ntidy_tweets <- tidy_tweets %>%\n    filter(!word %in% stop_words$word)"},{"path":"exercise-2-sentiment-analysis.html","id":"get-sentiment-dictionaries","chapter":"12 Exercise 2: Sentiment analysis","heading":"12.5 Get sentiment dictionaries","text":"Several sentiment dictionaries come bundled tidytext package. :AFINN Finn Årup Nielsen,bing Bing Liu collaborators, andnrc Saif Mohammad Peter TurneyWe can look see relevant dictionaries stored.see . First, AFINN lexicon gives words score -5 +5, negative scores indicate negative sentiment positive scores indicate positive sentiment. nrc lexicon opts binary classification: positive, negative, anger, anticipation, disgust, fear, joy, sadness, surprise, trust, word given score 1/0 sentiments. words, nrc lexicon, words appear multiple times enclose one emotion (see, e.g., “abandon” ). bing lexicon minimal, classifying words simply binary “positive” “negative” categories.Let’s see might filter texts selecting dictionary, subset dictionary, using inner_join() filter tweet data. might, example, interested fear words. Maybe, might hypothesize, uptick fear toward beginning coronavirus outbreak. First, let’s look words tweet data nrc lexicon codes fear-related words.total 1,174 words fear valence tweet data according nrc classification. Several seem reasonable (e.g., “death,” “pandemic”); others seems less (e.g., “mum,” “fight”).","code":"\nget_sentiments(\"afinn\")## # A tibble: 2,477 × 2\n##    word       value\n##    <chr>      <dbl>\n##  1 abandon       -2\n##  2 abandoned     -2\n##  3 abandons      -2\n##  4 abducted      -2\n##  5 abduction     -2\n##  6 abductions    -2\n##  7 abhor         -3\n##  8 abhorred      -3\n##  9 abhorrent     -3\n## 10 abhors        -3\n## # … with 2,467 more rows\nget_sentiments(\"bing\")## # A tibble: 6,786 × 2\n##    word        sentiment\n##    <chr>       <chr>    \n##  1 2-faces     negative \n##  2 abnormal    negative \n##  3 abolish     negative \n##  4 abominable  negative \n##  5 abominably  negative \n##  6 abominate   negative \n##  7 abomination negative \n##  8 abort       negative \n##  9 aborted     negative \n## 10 aborts      negative \n## # … with 6,776 more rows\nget_sentiments(\"nrc\")## # A tibble: 13,901 × 2\n##    word        sentiment\n##    <chr>       <chr>    \n##  1 abacus      trust    \n##  2 abandon     fear     \n##  3 abandon     negative \n##  4 abandon     sadness  \n##  5 abandoned   anger    \n##  6 abandoned   fear     \n##  7 abandoned   negative \n##  8 abandoned   sadness  \n##  9 abandonment anger    \n## 10 abandonment fear     \n## # … with 13,891 more rows\nnrc_fear <- get_sentiments(\"nrc\") %>% \n  filter(sentiment == \"fear\")\n\ntidy_tweets %>%\n  inner_join(nrc_fear) %>%\n  count(word, sort = TRUE)## Joining, by = \"word\"## # A tibble: 1,174 × 2\n##    word           n\n##    <chr>      <int>\n##  1 mum         4509\n##  2 death       4073\n##  3 police      3275\n##  4 hospital    2240\n##  5 government  2179\n##  6 pandemic    1877\n##  7 fight       1309\n##  8 die         1199\n##  9 attack      1099\n## 10 murder      1064\n## # … with 1,164 more rows"},{"path":"exercise-2-sentiment-analysis.html","id":"sentiment-trends-over-time","chapter":"12 Exercise 2: Sentiment analysis","heading":"12.6 Sentiment trends over time","text":"see time trends? First let’s make sure data properly arranged ascending order date. ’ll add column, ’ll call “order,” use become clear sentiment analysis.Remember structure tweet data one token (word) per document (tweet) format. order look sentiment trends time, ’ll need decide many words estimate sentiment., first add sentiment dictionary inner_join(). use count() function, specifying want count dates, words indexed order (.e., row number) every 1000 rows (.e., every 1000 words).means one date many tweets totalling >1000 words, multiple observations given date; one two tweets might just one row associated sentiment score date.calculate sentiment scores sentiment types (positive, negative, anger, anticipation, disgust, fear, joy, sadness, surprise, trust) use spread() function convert separate columns (rather rows). Finally calculate net sentiment score subtracting score negative sentiment positive sentiment.different sentiment dictionaries look compared ? can plot sentiment scores time sentiment dictionaries like :see look pretty similar… interestingly seems overall sentiment positivity increases pandemic breaks.","code":"\n#gen data variable, order and format date\ntidy_tweets$date <- as.Date(tidy_tweets$created_at)\n\ntidy_tweets <- tidy_tweets %>%\n  arrange(date)\n\ntidy_tweets$order <- 1:nrow(tidy_tweets)\n#get tweet sentiment by date\ntweets_nrc_sentiment <- tidy_tweets %>%\n  inner_join(get_sentiments(\"nrc\")) %>%\n  count(date, index = order %/% 1000, sentiment) %>%\n  spread(sentiment, n, fill = 0) %>%\n  mutate(sentiment = positive - negative)## Joining, by = \"word\"\ntweets_nrc_sentiment %>%\n  ggplot(aes(date, sentiment)) +\n  geom_point(alpha=0.5) +\n  geom_smooth(method= loess, alpha=0.25)## `geom_smooth()` using formula 'y ~ x'\ntidy_tweets %>%\n  inner_join(get_sentiments(\"bing\")) %>%\n  count(date, index = order %/% 1000, sentiment) %>%\n  spread(sentiment, n, fill = 0) %>%\n  mutate(sentiment = positive - negative) %>%\n  ggplot(aes(date, sentiment)) +\n  geom_point(alpha=0.5) +\n  geom_smooth(method= loess, alpha=0.25) +\n  ylab(\"bing sentiment\")## Joining, by = \"word\"## `geom_smooth()` using formula 'y ~ x'\ntidy_tweets %>%\n  inner_join(get_sentiments(\"nrc\")) %>%\n  count(date, index = order %/% 1000, sentiment) %>%\n  spread(sentiment, n, fill = 0) %>%\n  mutate(sentiment = positive - negative) %>%\n  ggplot(aes(date, sentiment)) +\n  geom_point(alpha=0.5) +\n  geom_smooth(method= loess, alpha=0.25) +\n  ylab(\"nrc sentiment\")## Joining, by = \"word\"\n## `geom_smooth()` using formula 'y ~ x'\ntidy_tweets %>%\n  inner_join(get_sentiments(\"afinn\")) %>%\n  group_by(date, index = order %/% 1000) %>% \n  summarise(sentiment = sum(value)) %>% \n  ggplot(aes(date, sentiment)) +\n  geom_point(alpha=0.5) +\n  geom_smooth(method= loess, alpha=0.25) +\n  ylab(\"afinn sentiment\")## Joining, by = \"word\"## `summarise()` has grouped output by 'date'. You can override using the `.groups` argument.## `geom_smooth()` using formula 'y ~ x'"},{"path":"exercise-2-sentiment-analysis.html","id":"domain-specific-lexicons","chapter":"12 Exercise 2: Sentiment analysis","heading":"12.7 Domain-specific lexicons","text":"course, list- dictionary-based methods need focus sentiment, even one common uses. essence, ’ll seen sentiment analysis techniques rely given lexicon score words appropriately. nothing stopping us making dictionaries, whether measure sentiment . data , might interested, example, prevalence mortality-related words news. , might choose make dictionary terms. look like?minimal example choose, example, words like “death” synonyms score 1. combine dictionary, ’ve called “mordict” .use technique bind data look incidence words time. Combining sequence scripts following:simply counts number mortality words time. might misleading , example, longer tweets certain points time; .e., length quantity text time-constant.matter? Well, just mortality words later just tweets earlier . just counting words, taking account denominator.alternative, preferable, method simply take character string relevant words. sum total number words across tweets time. filter tweet words whether mortality word , according dictionary words constructed. words, summing number times appear date., join data frame total words date. Note using full_join() want include dates appear “totals” data frame appear filter mortality words; .e., days mortality words equal 0. go plotting .","code":"\nword <- c('death', 'illness', 'hospital', 'life', 'health',\n             'fatality', 'morbidity', 'deadly', 'dead', 'victim')\nvalue <- c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1)\nmordict <- data.frame(word, value)\nmordict##         word value\n## 1      death     1\n## 2    illness     1\n## 3   hospital     1\n## 4       life     1\n## 5     health     1\n## 6   fatality     1\n## 7  morbidity     1\n## 8     deadly     1\n## 9       dead     1\n## 10    victim     1\ntidy_tweets %>%\n  inner_join(mordict) %>%\n  group_by(date, index = order %/% 1000) %>% \n  summarise(morwords = sum(value)) %>% \n  ggplot(aes(date, morwords)) +\n  geom_bar(stat= \"identity\") +\n  ylab(\"mortality words\")## Joining, by = \"word\"## `summarise()` has grouped output by 'date'. You can override using the `.groups` argument.\nmordict <- c('death', 'illness', 'hospital', 'life', 'health',\n             'fatality', 'morbidity', 'deadly', 'dead', 'victim')\n\n#get total tweets per day (no missing dates so no date completion required)\ntotals <- tidy_tweets %>%\n  mutate(obs=1) %>%\n  group_by(date) %>%\n  summarise(sum_words = sum(obs))\n\n#plot\ntidy_tweets %>%\n  mutate(obs=1) %>%\n  filter(grepl(paste0(mordict, collapse = \"|\"),word, ignore.case = T)) %>%\n  group_by(date) %>%\n  summarise(sum_mwords = sum(obs)) %>%\n  full_join(totals, word, by=\"date\") %>%\n  mutate(sum_mwords= ifelse(is.na(sum_mwords), 0, sum_mwords),\n         pctmwords = sum_mwords/sum_words) %>%\n  ggplot(aes(date, pctmwords)) +\n  geom_point(alpha=0.5) +\n  geom_smooth(method= loess, alpha=0.25) +\n  xlab(\"Date\") + ylab(\"% mortality words\")## `geom_smooth()` using formula 'y ~ x'"},{"path":"exercise-2-sentiment-analysis.html","id":"using-lexicoder","chapter":"12 Exercise 2: Sentiment analysis","heading":"12.8 Using Lexicoder","text":"approaches use general dictionary-based techniques designed domain-specific text news text. Lexicoder Sentiment Dictionary, Young Soroka (2012) designed specifically examining affective content news text. follows, see implement analysis using dictionary.conduct analysis using quanteda package. see can tokenize text similar way using functions included quanteda package.quanteda package first need create “corpus” object, declaring tweets corpus object. , make sure date column correctly stored create corpus object corpus() function. Note specifying text_field “tweet” text data interest , including information date tweet published. information specified docvars argument. ’ll see tthen corpus consists text -called “docvars,” just variables (columns) original dataset. , included date column.tokenize text using tokens() function quanteda, removing punctuation along way:take data_dictionary_LSD2015 comes bundled quanteda select positive negative categories, excluding words deemed “neutral.” , ready “look ” dictionary tokens corpus scored tokens_lookup() function.creates long list texts (tweets) annotated series ‘positive’ ‘negative’ annotations depending valence words text. creators quanteda recommend generate document feature matric . Grouping date, get dfm object, quite convoluted list object can plot using base graphics functions plotting matrices.Alternativelym can recreate tidy format follows:plot accordingly:","code":"\ntweets$date <- as.Date(tweets$created_at)\n\ntweet_corpus <- corpus(tweets, text_field = \"tweet\", docvars = \"date\")## Warning: docvars argument is not used.\ntoks_news <- tokens(tweet_corpus, remove_punct = TRUE)\n# select only the \"negative\" and \"positive\" categories\ndata_dictionary_LSD2015_pos_neg <- data_dictionary_LSD2015[1:2]\n\ntoks_news_lsd <- tokens_lookup(toks_news, dictionary = data_dictionary_LSD2015_pos_neg)\n# create a document document-feature matrix and group it by date\ndfmat_news_lsd <- dfm(toks_news_lsd) %>% \n  dfm_group(groups = date)\n\n# plot positive and negative valence over time\nmatplot(dfmat_news_lsd$date, dfmat_news_lsd, type = \"l\", lty = 1, col = 1:2,\n        ylab = \"Frequency\", xlab = \"\")\ngrid()\nlegend(\"topleft\", col = 1:2, legend = colnames(dfmat_news_lsd), lty = 1, bg = \"white\")\n# plot overall sentiment (positive  - negative) over time\n\nplot(dfmat_news_lsd$date, dfmat_news_lsd[,\"positive\"] - dfmat_news_lsd[,\"negative\"], \n     type = \"l\", ylab = \"Sentiment\", xlab = \"\")\ngrid()\nabline(h = 0, lty = 2)\nnegative <- dfmat_news_lsd@x[1:121]\npositive <- dfmat_news_lsd@x[122:242]\ndate <- dfmat_news_lsd@Dimnames$docs\n\n\ntidy_sent <- as.data.frame(cbind(negative, positive, date))\n\ntidy_sent$negative <- as.numeric(tidy_sent$negative)\ntidy_sent$positive <- as.numeric(tidy_sent$positive)\ntidy_sent$sentiment <- tidy_sent$positive - tidy_sent$negative\ntidy_sent$date <- as.Date(tidy_sent$date)\ntidy_sent %>%\n  ggplot() +\n  geom_line(aes(date, sentiment))"},{"path":"exercise-2-sentiment-analysis.html","id":"exercises-1","chapter":"12 Exercise 2: Sentiment analysis","heading":"12.9 Exercises","text":"Take subset tweets data “user_name” names describe name newspaper source Twitter account. see different sentiment dynamics look different newspaper sources?Build (minimal) dictionary-based filter technique plot resultApply Lexicoder Sentiment Dictionary news tweets, break analysis newspaper","code":""},{"path":"exercise-2-sentiment-analysis.html","id":"references-1","chapter":"12 Exercise 2: Sentiment analysis","heading":"12.10 References","text":"","code":""},{"path":"references-2.html","id":"references-2","chapter":"13 References","heading":"13 References","text":"","code":""}]
